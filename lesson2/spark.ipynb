{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "binding-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "contained-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the entry point for an spark app is the SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"FirstApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handed-alberta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.193.75:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FirstApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f62bdd78be0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phantom-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "retained-biotechnology",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {x for x in range(1,100)}\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "present-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distributed-designation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tamil-error",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_computeFractionForSampleSize', '_defaultReducePartitions', '_is_barrier', '_memory_limit', '_pickled', '_reserialize', '_to_java_object_rdd', 'aggregate', 'aggregateByKey', 'barrier', 'cache', 'cartesian', 'checkpoint', 'coalesce', 'cogroup', 'collect', 'collectAsMap', 'collectWithJobGroup', 'combineByKey', 'context', 'count', 'countApprox', 'countApproxDistinct', 'countByKey', 'countByValue', 'distinct', 'filter', 'first', 'flatMap', 'flatMapValues', 'fold', 'foldByKey', 'foreach', 'foreachPartition', 'fullOuterJoin', 'getCheckpointFile', 'getNumPartitions', 'getStorageLevel', 'glom', 'groupBy', 'groupByKey', 'groupWith', 'histogram', 'id', 'intersection', 'isCheckpointed', 'isEmpty', 'isLocallyCheckpointed', 'join', 'keyBy', 'keys', 'leftOuterJoin', 'localCheckpoint', 'lookup', 'map', 'mapPartitions', 'mapPartitionsWithIndex', 'mapPartitionsWithSplit', 'mapValues', 'max', 'mean', 'meanApprox', 'min', 'name', 'partitionBy', 'persist', 'pipe', 'randomSplit', 'reduce', 'reduceByKey', 'reduceByKeyLocally', 'repartition', 'repartitionAndSortWithinPartitions', 'rightOuterJoin', 'sample', 'sampleByKey', 'sampleStdev', 'sampleVariance', 'saveAsHadoopDataset', 'saveAsHadoopFile', 'saveAsNewAPIHadoopDataset', 'saveAsNewAPIHadoopFile', 'saveAsPickleFile', 'saveAsSequenceFile', 'saveAsTextFile', 'setName', 'sortBy', 'sortByKey', 'stats', 'stdev', 'subtract', 'subtractByKey', 'sum', 'sumApprox', 'take', 'takeOrdered', 'takeSample', 'toDF', 'toDebugString', 'toLocalIterator', 'top', 'treeAggregate', 'treeReduce', 'union', 'unpersist', 'values', 'variance', 'zip', 'zipWithIndex', 'zipWithUniqueId']\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(dir(pyspark.rdd.RDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "possible-bosnia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262\n"
     ]
    }
   ],
   "source": [
    "print(distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "studied-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "distData = distData.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "material-vegetable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[5] at coalesce at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "print(distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "signed-alert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "solid-protection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 12, 25, 26, 27]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "connected-retreat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-telephone",
   "metadata": {},
   "source": [
    "## Brodcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "integrated-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar = sc.broadcast([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fewer-wales",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.broadcast.Broadcast at 0x7f62bd4ccf40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "discrete-alliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-riverside",
   "metadata": {},
   "source": [
    "## Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "alien-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "nonprofit-alexandria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=0, value=0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "possible-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "russian-racing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-spring",
   "metadata": {},
   "source": [
    "## Map Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "delayed-shuttle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ldobre/das_2021'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check the wd\n",
    "os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "representative-cocktail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.TextIOWrapper'>\n",
      "['Liverpool Football Club is a professional football club in Liverpool, England, that competes in the Premier League, the top tier of English football. Domestically, the club has won nineteen League titles, seven FA Cups, a record eight League Cups and fifteen FA Community Shields. In international competitions, the club has won six European Cups, more than any other English club, three UEFA Cups, four UEFA Super Cups (also English records) and one FIFA Club World Cup.\\n', '\\n', \"Founded in 1892, the club joined the Football League the following year and has played at Anfield since its formation. Liverpool established itself as a major force in English and European football in the 1970s and 1980s, when Bill Shankly, Bob Paisley, Joe Fagan and Kenny Dalglish led the club to a combined eleven League titles and four European Cups. Liverpool won two further European Cups in 2005 and 2019 under the management of Rafael Benítez and Jürgen Klopp, respectively, the latter of whom led Liverpool to a nineteenth League title in 2020, the club's first during the Premier League era.\\n\", '\\n', 'One of the most widely supported teams in the world,[3] in 2019, Liverpool was the world\\'s seventh-highest-earning football club, with an annual revenue of €604 million,[4] and the world\\'s eighth-most-valuable football club, valued at $2.183 billion.[5] Liverpool has long-standing rivalries with Manchester United and Everton. The team changed from red shirts and white shorts to an all-red home strip in 1964 which has been used ever since. The club\\'s anthem is \"You\\'ll Never Walk Alone\".\\n']\n"
     ]
    }
   ],
   "source": [
    "# the target file is in <git_root>/lesson2/data/word_count.text\n",
    "file_path = 'lesson2/data/word_count.text'\n",
    "with open(file_path, 'r') as f:\n",
    "    print(type(f))\n",
    "    data =f.readlines()[:5]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-cooperative",
   "metadata": {},
   "source": [
    "### Python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "variable-marathon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 103 lines from <_io.TextIOWrapper name='lesson2/data/word_count.text' mode='r' encoding='UTF-8'>\n",
      "the 407\n",
      "in 173\n",
      "and 149\n",
      "of 135\n",
      "to 109\n",
      "Liverpool 91\n",
      "a 86\n",
      "was 77\n",
      "The 59\n",
      "club 51\n"
     ]
    }
   ],
   "source": [
    "# Python version\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "\n",
    "file_path = 'lesson2/data/word_count.text'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    word_count = defaultdict(int)\n",
    "    lines = f.readlines()\n",
    "    print('read {} lines from {}'.format(len(lines), str(f)))\n",
    "    for line in lines:\n",
    "        for word in line.split():\n",
    "            word_count[word] += 1\n",
    "\n",
    "for word in sorted(word_count, key=word_count.get, reverse=True)[:10]:\n",
    "    print(word, word_count[word])            \n",
    "# for t in sorted(word_count.items(), key=operator.itemgetter(1), reverse=True)[:10]:\n",
    "#     print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "unable-sunday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function sorted in module builtins:\n",
      "\n",
      "sorted(iterable, /, *, key=None, reverse=False)\n",
      "    Return a new list containing all items from the iterable in ascending order.\n",
      "    \n",
      "    A custom key function can be supplied to customize the sort order, and the\n",
      "    reverse flag can be set to request the result in descending order.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-conflict",
   "metadata": {},
   "source": [
    "### Spark Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "descending-conflict",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Liverpool Football Club is a professional football club in Liverpool, England, that competes in the Premier League, the top tier of English football. Domestically, the club has won nineteen League titles, seven FA Cups, a record eight League Cups and fifteen FA Community Shields. In international competitions, the club has won six European Cups, more than any other English club, three UEFA Cups, four UEFA Super Cups (also English records) and one FIFA Club World Cup.',\n",
       " '']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesRDD = sc.textFile(file_path)\n",
    "linesRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "simplified-needle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Football', 9),\n",
       " ('Club', 3),\n",
       " ('is', 15),\n",
       " ('professional', 2),\n",
       " ('football', 9)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "countsRDD = linesRDD.flatMap(lambda x: x.split()) \\\n",
    "            .map(lambda x: (x, 1)) \\\n",
    "            .reduceByKey(add)\n",
    "countsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "linear-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = countsRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "executed-habitat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "terminal-chile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Football', 9),\n",
       " ('Club', 3),\n",
       " ('is', 15),\n",
       " ('professional', 2),\n",
       " ('football', 9)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-shannon",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "latter-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = countsRDD.sortBy(lambda x: x[1], ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "native-reducing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 407\n",
      "in: 173\n",
      "and: 149\n",
      "of: 135\n",
      "to: 109\n",
      "Liverpool: 91\n",
      "a: 86\n",
      "was: 77\n",
      "The: 59\n",
      "club: 51\n"
     ]
    }
   ],
   "source": [
    "for (word, count) in counts[:10]:\n",
    "    print('{}: {}'.format(word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "overall-texas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 407)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countsRDD.sortBy(lambda x: -x[1]).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-university",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "opened-samoa",
   "metadata": {},
   "source": [
    "## Calculate Pi\n",
    "https://www.geeksforgeeks.org/estimating-value-pi-using-monte-carlo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "thousand-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "romance-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = 100000 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "subject-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "federal-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = sc.parallelize(range(1, numbers + 1), partitions).map(f).reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "delayed-element",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.140440\n"
     ]
    }
   ],
   "source": [
    "print(\"Pi is roughly %f\" % (4.0 * count / numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "tutorial-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pi(numbers, partitions):\n",
    "    count = sc.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "    print(\"Pi is roughly %f\" % (4.0 * count / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dominant-indie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.139700\n"
     ]
    }
   ],
   "source": [
    "p = 2 * 4\n",
    "n = 100000 * 2\n",
    "estimate_pi(n,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "durable-royalty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141764\n"
     ]
    }
   ],
   "source": [
    "p = 2\n",
    "n = 100000 * 2 * 10\n",
    "estimate_pi(n,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "surgical-agency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141676\n"
     ]
    }
   ],
   "source": [
    "p = 2 * 4\n",
    "n = 100000 * 2 * 10 * 10\n",
    "estimate_pi(n,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-floating",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "federal-supervisor",
   "metadata": {},
   "source": [
    "## Intersect example\n",
    "find Hosts that were accesed in both July and August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "optical-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "july_path = 'lesson2/data/nasa_19950701.tsv'\n",
    "august_path = 'lesson2/data/nasa_19950801.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "beneficial-garlic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['host\\tlogname\\ttime\\tmethod\\turl\\tresponse\\tbytes\\n', '199.72.81.55\\t-\\t804571201\\tGET\\t/history/apollo/\\t200\\t6245\\t\\t\\n', 'unicomp6.unicomp.net\\t-\\t804571206\\tGET\\t/shuttle/countdown/\\t200\\t3985\\t\\t\\n', '199.120.110.21\\t-\\t804571209\\tGET\\t/shuttle/missions/sts-73/mission-sts-73.html\\t200\\t4085\\t\\t\\n', 'burger.letters.com\\t-\\t804571211\\tGET\\t/shuttle/countdown/liftoff.html\\t304\\t0\\t\\t\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(july_path, 'r') as f:\n",
    "    data =f.readlines()[:5]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "optical-delta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['host\\tlogname\\ttime\\tmethod\\turl\\tresponse\\tbytes\\n', 'in24.inetnebr.com\\t-\\t807249601\\tGET\\t/shuttle/missions/sts-68/news/sts-68-mcc-05.txt\\t200\\t1839\\t\\t\\n', 'uplherc.upl.com\\t-\\t807249607\\tGET\\t/\\t304\\t0\\t\\t\\n', 'uplherc.upl.com\\t-\\t807249608\\tGET\\t/images/ksclogo-medium.gif\\t304\\t0\\t\\t\\n', 'uplherc.upl.com\\t-\\t807249608\\tGET\\t/images/MOSAIC-logosmall.gif\\t304\\t0\\t\\t\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(august_path, 'r') as f:\n",
    "    data =f.readlines()[:5]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "changing-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostsJ = sc.textFile(july_path).map(lambda x: x.split('\\t')[0])\n",
    "hostsA = sc.textFile(august_path).map(lambda x: x.split('\\t')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "known-holmes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host',\n",
       " '199.72.81.55',\n",
       " 'unicomp6.unicomp.net',\n",
       " '199.120.110.21',\n",
       " 'burger.letters.com']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hostsJ.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "phantom-hampton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host',\n",
       " 'in24.inetnebr.com',\n",
       " 'uplherc.upl.com',\n",
       " 'uplherc.upl.com',\n",
       " 'uplherc.upl.com']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hostsA.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "local-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = hostsJ.intersection(hostsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "removed-husband",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www-a1.proxy.aol.com',\n",
       " 'www-d3.proxy.aol.com',\n",
       " 'piweba1y.prodigy.com',\n",
       " 'www-d4.proxy.aol.com',\n",
       " 'piweba2y.prodigy.com',\n",
       " 'bettong.client.uq.oz.au',\n",
       " 'www-d1.proxy.aol.com',\n",
       " 'vagrant.vf.mmc.com',\n",
       " 'hella.stm.it',\n",
       " 'beglinger.dial-up.bdt.com',\n",
       " 'alpha2.csd.uwm.edu',\n",
       " 'disarray.demon.co.uk',\n",
       " 'wwwproxy.info.au',\n",
       " 'magicall.dacom.co.kr',\n",
       " 'palona1.cns.hp.com',\n",
       " 'www-b5.proxy.aol.com',\n",
       " 'netcom3.netcom.com',\n",
       " 'www-a2.proxy.aol.com',\n",
       " 'hitiij.hitachi.co.jp',\n",
       " 'pm206-52.smartlink.net',\n",
       " 'alyssa.prodigy.com',\n",
       " 'www-b2.proxy.aol.com',\n",
       " 'www-b3.proxy.aol.com',\n",
       " 'koala.melbpc.org.au',\n",
       " 'www-d2.proxy.aol.com',\n",
       " 'freenet.edmonton.ab.ca',\n",
       " 'ntigate.nt.com',\n",
       " 'dd08-021.compuserve.com',\n",
       " 'piweba4y.prodigy.com',\n",
       " 'piweba3y.prodigy.com',\n",
       " 'ottgate2.bnr.ca',\n",
       " 'columbia.acc.brad.ac.uk',\n",
       " 'ccn.cs.dal.ca',\n",
       " 'srv1.freenet.calgary.ab.ca',\n",
       " 'spectrum.xerox.com',\n",
       " 'server.elysian.net',\n",
       " 'reggae.iinet.net.au']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = intersection.filter(lambda x: x!='host')\n",
    "clean.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "elegant-queen",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o335.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/ldobre/das_2021/out/inter.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-edd02ac6f92a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out/inter.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1654\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o335.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/ldobre/das_2021/out/inter.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "clean.saveAsTextFile('out/inter.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-adrian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-bishop",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'lesson2/data/RealEstate.csv'\n",
    "df = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(spark.read))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import sys\n",
    "import queue as Queue\n",
    "\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "\n",
    "def delayed(seconds):\n",
    "    def f(x):\n",
    "        time.sleep(seconds)\n",
    "        return x\n",
    "    return f\n",
    "\n",
    "\n",
    "def call_in_background(f, *args):\n",
    "    result = Queue.Queue(1)\n",
    "    t = threading.Thread(target=lambda: result.put(f(*args)))\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    # conf = SparkConf().set(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    # sc = SparkContext(appName=\"PythonStatusAPIDemo\", conf=conf)\n",
    "\n",
    "    def run():\n",
    "        rdd = sc.parallelize(range(10), 10).map(delayed(5))\n",
    "        reduced = rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "        return reduced.map(delayed(2)).collect()\n",
    "\n",
    "    result = call_in_background(run)\n",
    "    status = sc.statusTracker()\n",
    "    while result.empty():\n",
    "        ids = status.getJobIdsForGroup()\n",
    "        for id in ids:\n",
    "            job = status.getJobInfo(id)\n",
    "            print(\"Job\", id, \"status: \", job.status)\n",
    "            for sid in job.stageIds:\n",
    "                info = status.getStageInfo(sid)\n",
    "                if info:\n",
    "                    print(\"Stage %d: %d tasks total (%d active, %d complete)\" %\n",
    "                          (sid, info.numTasks, info.numActiveTasks, info.numCompletedTasks))\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(\"Job results are:\", result.get())\n",
    "    sc.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
