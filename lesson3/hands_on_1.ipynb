{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "favorite-fault",
   "metadata": {},
   "source": [
    "# DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "careful-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python imports are first in a file, after the shebang\n",
    "# https://en.wikipedia.org/wiki/Shebang_(Unix)\n",
    "import os\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "likely-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets start the spark session\n",
    "# the entry point for an spark app is the SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"FirstApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "insured-investing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://EN1310471.endava.net:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FirstApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1891b7b9700>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-nature",
   "metadata": {},
   "source": [
    "## READ\n",
    "Let's dive deeper into the spark.read API"
   ]
  },
  {
   "cell_type": "raw",
   "id": "civilian-pavilion",
   "metadata": {},
   "source": [
    "# open a new shell and run: \n",
    "    \n",
    "aws s3 ls s3://das-2021\n",
    "\n",
    "aws s3 ls s3://das-2021/data/ny-taxi/ --no-sign-request --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-alias",
   "metadata": {},
   "source": [
    "copy the files locally"
   ]
  },
  {
   "cell_type": "raw",
   "id": "knowing-defeat",
   "metadata": {},
   "source": [
    "aws s3 cp s3://das-2021/data/ny-taxi/ ./ --no-sign-request --recursive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spark.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(spark.read))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the wd\n",
    "os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-duncan",
   "metadata": {},
   "source": [
    "### CSV\n",
    "aka delimited text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "automated-dietary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n",
      "\n",
      "csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None, locale=None, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      "    \n",
      "    This function will go through the input once to determine the input schema if\n",
      "    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      "    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      "    \n",
      "    :param path: string, or list of strings, for input path(s),\n",
      "                 or RDD of Strings storing CSV rows.\n",
      "    :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "                   or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "    :param sep: sets a separator (one or more characters) for each field and value. If None is\n",
      "                set, it uses the default value, ``,``.\n",
      "    :param encoding: decodes the CSV files by the given encoding type. If None is set,\n",
      "                     it uses the default value, ``UTF-8``.\n",
      "    :param quote: sets a single character used for escaping quoted values where the\n",
      "                  separator can be part of the value. If None is set, it uses the default\n",
      "                  value, ``\"``. If you would like to turn off quotations, you need to set an\n",
      "                  empty string.\n",
      "    :param escape: sets a single character used for escaping quotes inside an already\n",
      "                   quoted value. If None is set, it uses the default value, ``\\``.\n",
      "    :param comment: sets a single character used for skipping lines beginning with this\n",
      "                    character. By default (None), it is disabled.\n",
      "    :param header: uses the first line as names of columns. If None is set, it uses the\n",
      "                   default value, ``false``.\n",
      "    :param inferSchema: infers the input schema automatically from data. It requires one extra\n",
      "                   pass over the data. If None is set, it uses the default value, ``false``.\n",
      "    :param enforceSchema: If it is set to ``true``, the specified or inferred schema will be\n",
      "                          forcibly applied to datasource files, and headers in CSV files will be\n",
      "                          ignored. If the option is set to ``false``, the schema will be\n",
      "                          validated against all headers in CSV files or the first header in RDD\n",
      "                          if the ``header`` option is set to ``true``. Field names in the schema\n",
      "                          and column names in CSV headers are checked by their positions\n",
      "                          taking into account ``spark.sql.caseSensitive``. If None is set,\n",
      "                          ``true`` is used by default. Though the default value is ``true``,\n",
      "                          it is recommended to disable the ``enforceSchema`` option\n",
      "                          to avoid incorrect results.\n",
      "    :param ignoreLeadingWhiteSpace: A flag indicating whether or not leading whitespaces from\n",
      "                                    values being read should be skipped. If None is set, it\n",
      "                                    uses the default value, ``false``.\n",
      "    :param ignoreTrailingWhiteSpace: A flag indicating whether or not trailing whitespaces from\n",
      "                                     values being read should be skipped. If None is set, it\n",
      "                                     uses the default value, ``false``.\n",
      "    :param nullValue: sets the string representation of a null value. If None is set, it uses\n",
      "                      the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
      "                      applies to all supported types including the string type.\n",
      "    :param nanValue: sets the string representation of a non-number value. If None is set, it\n",
      "                     uses the default value, ``NaN``.\n",
      "    :param positiveInf: sets the string representation of a positive infinity value. If None\n",
      "                        is set, it uses the default value, ``Inf``.\n",
      "    :param negativeInf: sets the string representation of a negative infinity value. If None\n",
      "                        is set, it uses the default value, ``Inf``.\n",
      "    :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      "                       follow the formats at `datetime pattern`_.\n",
      "                       This applies to date type. If None is set, it uses the\n",
      "                       default value, ``yyyy-MM-dd``.\n",
      "    :param timestampFormat: sets the string that indicates a timestamp format.\n",
      "                            Custom date formats follow the formats at `datetime pattern`_.\n",
      "                            This applies to timestamp type. If None is set, it uses the\n",
      "                            default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      "    :param maxColumns: defines a hard limit of how many columns a record can have. If None is\n",
      "                       set, it uses the default value, ``20480``.\n",
      "    :param maxCharsPerColumn: defines the maximum number of characters allowed for any given\n",
      "                              value being read. If None is set, it uses the default value,\n",
      "                              ``-1`` meaning unlimited length.\n",
      "    :param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\n",
      "                                        If specified, it is ignored.\n",
      "    :param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
      "                 set, it uses the default value, ``PERMISSIVE``. Note that Spark tries to\n",
      "                 parse only required columns in CSV under column pruning. Therefore, corrupt\n",
      "                 records can be different based on required set of fields. This behavior can\n",
      "                 be controlled by ``spark.sql.csv.parser.columnPruning.enabled``\n",
      "                 (enabled by default).\n",
      "    \n",
      "            * ``PERMISSIVE``: when it meets a corrupted record, puts the malformed string \\\n",
      "              into a field configured by ``columnNameOfCorruptRecord``, and sets malformed \\\n",
      "              fields to ``null``. To keep corrupt records, an user can set a string type \\\n",
      "              field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\n",
      "              schema does not have the field, it drops corrupt records during parsing. \\\n",
      "              A record with less/more tokens than schema is not a corrupted record to CSV. \\\n",
      "              When it meets a record having fewer tokens than the length of the schema, \\\n",
      "              sets ``null`` to extra fields. When the record has more tokens than the \\\n",
      "              length of the schema, it drops extra tokens.\n",
      "            * ``DROPMALFORMED``: ignores the whole corrupted records.\n",
      "            * ``FAILFAST``: throws an exception when it meets corrupted records.\n",
      "    \n",
      "    :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
      "                                      created by ``PERMISSIVE`` mode. This overrides\n",
      "                                      ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      "                                      it uses the value specified in\n",
      "                                      ``spark.sql.columnNameOfCorruptRecord``.\n",
      "    :param multiLine: parse records, which may span multiple lines. If None is\n",
      "                      set, it uses the default value, ``false``.\n",
      "    :param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
      "                                      the quote character. If None is set, the default value is\n",
      "                                      escape character when escape and quote characters are\n",
      "                                      different, ``\\0`` otherwise.\n",
      "    :param samplingRatio: defines fraction of rows used for schema inferring.\n",
      "                          If None is set, it uses the default value, ``1.0``.\n",
      "    :param emptyValue: sets the string representation of an empty value. If None is set, it uses\n",
      "                       the default value, empty string.\n",
      "    :param locale: sets a locale as language tag in IETF BCP 47 format. If None is set,\n",
      "                   it uses the default value, ``en-US``. For instance, ``locale`` is used while\n",
      "                   parsing dates and timestamps.\n",
      "    :param lineSep: defines the line separator that should be used for parsing. If None is\n",
      "                    set, it covers all ``\\\\r``, ``\\\\r\\\\n`` and ``\\\\n``.\n",
      "                    Maximum length is 1 character.\n",
      "    :param pathGlobFilter: an optional glob pattern to only include files with paths matching\n",
      "                           the pattern. The syntax follows `org.apache.hadoop.fs.GlobFilter`.\n",
      "                           It does not change the behavior of `partition discovery`_.\n",
      "    :param recursiveFileLookup: recursively scan a directory for files. Using this option\n",
      "                                disables `partition discovery`_.\n",
      "    \n",
      "    >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      "    >>> df.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "    >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
      "    >>> df2 = spark.read.csv(rdd)\n",
      "    >>> df2.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "    \n",
      "    .. versionadded:: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "isolated-privilege",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__func__', '__ge__', '__get__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__self__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__']\n"
     ]
    }
   ],
   "source": [
    "print(dir(spark.read.csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "standard-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_path = '../data/nasa_19950701.tsv'\n",
    "df = spark.read.csv(nasa_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "central-techno",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_c0,StringType,true)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "global-intent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='host\\tlogname\\ttime\\tmethod\\turl\\tresponse\\tbytes'),\n",
       " Row(_c0='199.72.81.55\\t-\\t804571201\\tGET\\t/history/apollo/\\t200\\t6245\\t\\t'),\n",
       " Row(_c0='unicomp6.unicomp.net\\t-\\t804571206\\tGET\\t/shuttle/countdown/\\t200\\t3985\\t\\t'),\n",
       " Row(_c0='199.120.110.21\\t-\\t804571209\\tGET\\t/shuttle/missions/sts-73/mission-sts-73.html\\t200\\t4085\\t\\t'),\n",
       " Row(_c0='burger.letters.com\\t-\\t804571211\\tGET\\t/shuttle/countdown/liftoff.html\\t304\\t0\\t\\t'),\n",
       " Row(_c0='199.120.110.21\\t-\\t804571211\\tGET\\t/shuttle/missions/sts-73/sts-73-patch-small.gif\\t200\\t4179\\t\\t'),\n",
       " Row(_c0='burger.letters.com\\t-\\t804571212\\tGET\\t/images/NASA-logosmall.gif\\t304\\t0\\t\\t'),\n",
       " Row(_c0='burger.letters.com\\t-\\t804571212\\tGET\\t/shuttle/countdown/video/livevideo.gif\\t200\\t0\\t\\t'),\n",
       " Row(_c0='205.212.115.106\\t-\\t804571212\\tGET\\t/shuttle/countdown/countdown.html\\t200\\t3985\\t\\t'),\n",
       " Row(_c0='d104.aa.net\\t-\\t804571213\\tGET\\t/shuttle/countdown/\\t200\\t3985\\t\\t'),\n",
       " Row(_c0='129.94.144.152\\t-\\t804571213\\tGET\\t/\\t200\\t7074\\t\\t'),\n",
       " Row(_c0='unicomp6.unicomp.net\\t-\\t804571214\\tGET\\t/shuttle/countdown/count.gif\\t200\\t40310\\t\\t'),\n",
       " Row(_c0='unicomp6.unicomp.net\\t-\\t804571214\\tGET\\t/images/NASA-logosmall.gif\\t200\\t786\\t\\t'),\n",
       " Row(_c0='unicomp6.unicomp.net\\t-\\t804571214\\tGET\\t/images/KSC-logosmall.gif\\t200\\t1204\\t\\t'),\n",
       " Row(_c0='d104.aa.net\\t-\\t804571215\\tGET\\t/shuttle/countdown/count.gif\\t200\\t40310\\t\\t'),\n",
       " Row(_c0='d104.aa.net\\t-\\t804571215\\tGET\\t/images/NASA-logosmall.gif\\t200\\t786\\t\\t'),\n",
       " Row(_c0='d104.aa.net\\t-\\t804571215\\tGET\\t/images/KSC-logosmall.gif\\t200\\t1204\\t\\t'),\n",
       " Row(_c0='129.94.144.152\\t-\\t804571217\\tGET\\t/images/ksclogo-medium.gif\\t304\\t0\\t\\t'),\n",
       " Row(_c0='199.120.110.21\\t-\\t804571217\\tGET\\t/images/launch-logo.gif\\t200\\t1713\\t\\t'),\n",
       " Row(_c0='ppptky391.asahi-net.or.jp\\t-\\t804571218\\tGET\\t/facts/about_ksc.html\\t200\\t3977\\t\\t')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "operational-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(nasa_path, sep='\\t', header=True, inferSchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vietnamese-religion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(host='199.72.81.55', logname='-', time='804571201', method='GET', url='/history/apollo/', response='200', bytes='6245'),\n",
       " Row(host='unicomp6.unicomp.net', logname='-', time='804571206', method='GET', url='/shuttle/countdown/', response='200', bytes='3985'),\n",
       " Row(host='199.120.110.21', logname='-', time='804571209', method='GET', url='/shuttle/missions/sts-73/mission-sts-73.html', response='200', bytes='4085'),\n",
       " Row(host='burger.letters.com', logname='-', time='804571211', method='GET', url='/shuttle/countdown/liftoff.html', response='304', bytes='0'),\n",
       " Row(host='199.120.110.21', logname='-', time='804571211', method='GET', url='/shuttle/missions/sts-73/sts-73-patch-small.gif', response='200', bytes='4179'),\n",
       " Row(host='burger.letters.com', logname='-', time='804571212', method='GET', url='/images/NASA-logosmall.gif', response='304', bytes='0'),\n",
       " Row(host='burger.letters.com', logname='-', time='804571212', method='GET', url='/shuttle/countdown/video/livevideo.gif', response='200', bytes='0'),\n",
       " Row(host='205.212.115.106', logname='-', time='804571212', method='GET', url='/shuttle/countdown/countdown.html', response='200', bytes='3985'),\n",
       " Row(host='d104.aa.net', logname='-', time='804571213', method='GET', url='/shuttle/countdown/', response='200', bytes='3985'),\n",
       " Row(host='129.94.144.152', logname='-', time='804571213', method='GET', url='/', response='200', bytes='7074'),\n",
       " Row(host='unicomp6.unicomp.net', logname='-', time='804571214', method='GET', url='/shuttle/countdown/count.gif', response='200', bytes='40310'),\n",
       " Row(host='unicomp6.unicomp.net', logname='-', time='804571214', method='GET', url='/images/NASA-logosmall.gif', response='200', bytes='786'),\n",
       " Row(host='unicomp6.unicomp.net', logname='-', time='804571214', method='GET', url='/images/KSC-logosmall.gif', response='200', bytes='1204'),\n",
       " Row(host='d104.aa.net', logname='-', time='804571215', method='GET', url='/shuttle/countdown/count.gif', response='200', bytes='40310'),\n",
       " Row(host='d104.aa.net', logname='-', time='804571215', method='GET', url='/images/NASA-logosmall.gif', response='200', bytes='786'),\n",
       " Row(host='d104.aa.net', logname='-', time='804571215', method='GET', url='/images/KSC-logosmall.gif', response='200', bytes='1204'),\n",
       " Row(host='129.94.144.152', logname='-', time='804571217', method='GET', url='/images/ksclogo-medium.gif', response='304', bytes='0'),\n",
       " Row(host='199.120.110.21', logname='-', time='804571217', method='GET', url='/images/launch-logo.gif', response='200', bytes='1713'),\n",
       " Row(host='ppptky391.asahi-net.or.jp', logname='-', time='804571218', method='GET', url='/facts/about_ksc.html', response='200', bytes='3977'),\n",
       " Row(host='net-1-141.eden.com', logname='-', time='804571219', method='GET', url='/shuttle/missions/sts-71/images/KSC-95EC-0916.jpg', response='200', bytes='34029')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adjacent-letters",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(host,StringType,true),StructField(logname,StringType,true),StructField(time,StringType,true),StructField(method,StringType,true),StructField(url,StringType,true),StructField(response,StringType,true),StructField(bytes,StringType,true)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "august-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(nasa_path, sep='\\t', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "expensive-transparency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(host,StringType,true),StructField(logname,StringType,true),StructField(time,IntegerType,true),StructField(method,StringType,true),StructField(url,StringType,true),StructField(response,IntegerType,true),StructField(bytes,IntegerType,true)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "respiratory-concentrate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'host'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "native-hierarchy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'host'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is considered the safe way of referincing a column\n",
    "df['host']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "behavioral-toner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(host='199.72.81.55'),\n",
       " Row(host='unicomp6.unicomp.net'),\n",
       " Row(host='199.120.110.21'),\n",
       " Row(host='burger.letters.com'),\n",
       " Row(host='199.120.110.21')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df.host).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "statewide-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(host='199.72.81.55'),\n",
       " Row(host='unicomp6.unicomp.net'),\n",
       " Row(host='199.120.110.21'),\n",
       " Row(host='burger.letters.com'),\n",
       " Row(host='199.120.110.21')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('host').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "basic-world",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---------+------+--------------------+--------+-----+\n",
      "|                host|logname|     time|method|                 url|response|bytes|\n",
      "+--------------------+-------+---------+------+--------------------+--------+-----+\n",
      "|        199.72.81.55|      -|804571201|   GET|    /history/apollo/|     200| 6245|\n",
      "|unicomp6.unicomp.net|      -|804571206|   GET| /shuttle/countdown/|     200| 3985|\n",
      "|      199.120.110.21|      -|804571209|   GET|/shuttle/missions...|     200| 4085|\n",
      "|  burger.letters.com|      -|804571211|   GET|/shuttle/countdow...|     304|    0|\n",
      "|      199.120.110.21|      -|804571211|   GET|/shuttle/missions...|     200| 4179|\n",
      "|  burger.letters.com|      -|804571212|   GET|/images/NASA-logo...|     304|    0|\n",
      "|  burger.letters.com|      -|804571212|   GET|/shuttle/countdow...|     200|    0|\n",
      "|     205.212.115.106|      -|804571212|   GET|/shuttle/countdow...|     200| 3985|\n",
      "|         d104.aa.net|      -|804571213|   GET| /shuttle/countdown/|     200| 3985|\n",
      "|      129.94.144.152|      -|804571213|   GET|                   /|     200| 7074|\n",
      "|unicomp6.unicomp.net|      -|804571214|   GET|/shuttle/countdow...|     200|40310|\n",
      "|unicomp6.unicomp.net|      -|804571214|   GET|/images/NASA-logo...|     200|  786|\n",
      "|unicomp6.unicomp.net|      -|804571214|   GET|/images/KSC-logos...|     200| 1204|\n",
      "|         d104.aa.net|      -|804571215|   GET|/shuttle/countdow...|     200|40310|\n",
      "|         d104.aa.net|      -|804571215|   GET|/images/NASA-logo...|     200|  786|\n",
      "|         d104.aa.net|      -|804571215|   GET|/images/KSC-logos...|     200| 1204|\n",
      "|      129.94.144.152|      -|804571217|   GET|/images/ksclogo-m...|     304|    0|\n",
      "|      199.120.110.21|      -|804571217|   GET|/images/launch-lo...|     200| 1713|\n",
      "|ppptky391.asahi-n...|      -|804571218|   GET|/facts/about_ksc....|     200| 3977|\n",
      "|  net-1-141.eden.com|      -|804571219|   GET|/shuttle/missions...|     200|34029|\n",
      "+--------------------+-------+---------+------+--------------------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "boring-warren",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---------+------+--------------------+--------+------+\n",
      "|                host|logname|     time|method|                 url|response| bytes|\n",
      "+--------------------+-------+---------+------+--------------------+--------+------+\n",
      "|        199.72.81.55|      -|804571201|   GET|    /history/apollo/|     200|  6245|\n",
      "|unicomp6.unicomp.net|      -|804571206|   GET| /shuttle/countdown/|     200|  3985|\n",
      "|      199.120.110.21|      -|804571209|   GET|/shuttle/missions...|     200|  4085|\n",
      "|  burger.letters.com|      -|804571211|   GET|/shuttle/countdow...|     304|     0|\n",
      "|      199.120.110.21|      -|804571211|   GET|/shuttle/missions...|     200|  4179|\n",
      "|  burger.letters.com|      -|804571212|   GET|/images/NASA-logo...|     304|     0|\n",
      "|  burger.letters.com|      -|804571212|   GET|/shuttle/countdow...|     200|     0|\n",
      "|     205.212.115.106|      -|804571212|   GET|/shuttle/countdow...|     200|  3985|\n",
      "|         d104.aa.net|      -|804571213|   GET| /shuttle/countdown/|     200|  3985|\n",
      "|      129.94.144.152|      -|804571213|   GET|                   /|     200|  7074|\n",
      "|unicomp6.unicomp.net|      -|804571214|   GET|/shuttle/countdow...|     200| 40310|\n",
      "|unicomp6.unicomp.net|      -|804571214|   GET|/images/NASA-logo...|     200|   786|\n",
      "|unicomp6.unicomp.net|      -|804571214|   GET|/images/KSC-logos...|     200|  1204|\n",
      "|         d104.aa.net|      -|804571215|   GET|/shuttle/countdow...|     200| 40310|\n",
      "|         d104.aa.net|      -|804571215|   GET|/images/NASA-logo...|     200|   786|\n",
      "|         d104.aa.net|      -|804571215|   GET|/images/KSC-logos...|     200|  1204|\n",
      "|      129.94.144.152|      -|804571217|   GET|/images/ksclogo-m...|     304|     0|\n",
      "|      199.120.110.21|      -|804571217|   GET|/images/launch-lo...|     200|  1713|\n",
      "|ppptky391.asahi-n...|      -|804571218|   GET|/facts/about_ksc....|     200|  3977|\n",
      "|  net-1-141.eden.com|      -|804571219|   GET|/shuttle/missions...|     200| 34029|\n",
      "|ppptky391.asahi-n...|      -|804571219|   GET|/images/launchpal...|     200| 11473|\n",
      "|      205.189.154.54|      -|804571224|   GET| /shuttle/countdown/|     200|  3985|\n",
      "|waters-gw.starway...|      -|804571225|   GET|/shuttle/missions...|     200|  6723|\n",
      "|ppp-mia-30.shadow...|      -|804571227|   GET|                   /|     200|  7074|\n",
      "|      205.189.154.54|      -|804571229|   GET|/shuttle/countdow...|     200| 40310|\n",
      "|  alyssa.prodigy.com|      -|804571233|   GET|/shuttle/missions...|     200| 12054|\n",
      "|ppp-mia-30.shadow...|      -|804571235|   GET|/images/ksclogo-m...|     200|  5866|\n",
      "|    dial22.lloyd.com|      -|804571237|   GET|/shuttle/missions...|     200| 61716|\n",
      "|smyth-pc.moorecap...|      -|804571238|   GET|/history/apollo/a...|     200|101267|\n",
      "|      205.189.154.54|      -|804571240|   GET|/images/NASA-logo...|     200|   786|\n",
      "|ix-orl2-01.ix.net...|      -|804571241|   GET| /shuttle/countdown/|     200|  3985|\n",
      "|ppp-mia-30.shadow...|      -|804571241|   GET|/images/NASA-logo...|     200|   786|\n",
      "|ppp-mia-30.shadow...|      -|804571241|   GET|/images/MOSAIC-lo...|     200|   363|\n",
      "|      205.189.154.54|      -|804571241|   GET|/images/KSC-logos...|     200|  1204|\n",
      "|ppp-mia-30.shadow...|      -|804571241|   GET|/images/USA-logos...|     200|   234|\n",
      "|ppp-mia-30.shadow...|      -|804571243|   GET|/images/WORLD-log...|     200|   669|\n",
      "|ix-orl2-01.ix.net...|      -|804571244|   GET|/shuttle/countdow...|     200| 40310|\n",
      "|gayle-gaston.tene...|      -|804571250|   GET|/shuttle/missions...|     200| 12040|\n",
      "|piweba3y.prodigy.com|      -|804571254|   GET|/shuttle/missions...|     200| 12054|\n",
      "|   scheyer.clark.net|      -|804571258|   GET|/shuttle/missions...|     200| 49152|\n",
      "| ppp-nyc-3-1.ios.com|      -|804571259|   GET|/shuttle/missions...|     200| 77163|\n",
      "|        199.72.81.55|      -|804571259|   GET|           /history/|     200|  1382|\n",
      "|port26.annex2.nwl...|      -|804571262|   GET|/software/winvn/w...|     200|  9867|\n",
      "|port26.annex2.nwl...|      -|804571264|   GET|/software/winvn/w...|     200| 25218|\n",
      "|port26.annex2.nwl...|      -|804571264|   GET|/images/construct...|     200|  1414|\n",
      "|port26.annex2.nwl...|      -|804571264|   GET|/software/winvn/b...|     200|  4441|\n",
      "|dd14-012.compuser...|      -|804571265|   GET|/shuttle/technolo...|     200| 42732|\n",
      "|      205.189.154.54|      -|804571266|   GET|/cgi-bin/imagemap...|     302|   110|\n",
      "|      205.189.154.54|      -|804571268|   GET|/shuttle/missions...|     200|  7634|\n",
      "|www-a1.proxy.aol.com|      -|804571269|   GET| /shuttle/countdown/|     200|  3985|\n",
      "+--------------------+-------+---------+------+--------------------+--------+------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "capital-terrorist",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------+--------------------+--------+-----+\n",
      "|                host|logname|method|                 url|response|bytes|\n",
      "+--------------------+-------+------+--------------------+--------+-----+\n",
      "|        199.72.81.55|      -|   GET|    /history/apollo/|     200| 6245|\n",
      "|unicomp6.unicomp.net|      -|   GET| /shuttle/countdown/|     200| 3985|\n",
      "|      199.120.110.21|      -|   GET|/shuttle/missions...|     200| 4085|\n",
      "|  burger.letters.com|      -|   GET|/shuttle/countdow...|     304|    0|\n",
      "|      199.120.110.21|      -|   GET|/shuttle/missions...|     200| 4179|\n",
      "|  burger.letters.com|      -|   GET|/images/NASA-logo...|     304|    0|\n",
      "|  burger.letters.com|      -|   GET|/shuttle/countdow...|     200|    0|\n",
      "|     205.212.115.106|      -|   GET|/shuttle/countdow...|     200| 3985|\n",
      "|         d104.aa.net|      -|   GET| /shuttle/countdown/|     200| 3985|\n",
      "|      129.94.144.152|      -|   GET|                   /|     200| 7074|\n",
      "|unicomp6.unicomp.net|      -|   GET|/shuttle/countdow...|     200|40310|\n",
      "|unicomp6.unicomp.net|      -|   GET|/images/NASA-logo...|     200|  786|\n",
      "|unicomp6.unicomp.net|      -|   GET|/images/KSC-logos...|     200| 1204|\n",
      "|         d104.aa.net|      -|   GET|/shuttle/countdow...|     200|40310|\n",
      "|         d104.aa.net|      -|   GET|/images/NASA-logo...|     200|  786|\n",
      "|         d104.aa.net|      -|   GET|/images/KSC-logos...|     200| 1204|\n",
      "|      129.94.144.152|      -|   GET|/images/ksclogo-m...|     304|    0|\n",
      "|      199.120.110.21|      -|   GET|/images/launch-lo...|     200| 1713|\n",
      "|ppptky391.asahi-n...|      -|   GET|/facts/about_ksc....|     200| 3977|\n",
      "|  net-1-141.eden.com|      -|   GET|/shuttle/missions...|     200|34029|\n",
      "+--------------------+-------+------+--------------------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "special-middle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(host,StringType,true),StructField(logname,StringType,true),StructField(time,IntegerType,true),StructField(method,StringType,true),StructField(url,StringType,true),StructField(response,IntegerType,true),StructField(bytes,IntegerType,true)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "demonstrated-reconstruction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(host,StringType,true),StructField(logname,StringType,true),StructField(method,StringType,true),StructField(url,StringType,true),StructField(response,IntegerType,true),StructField(bytes,IntegerType,true)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.drop('time')\n",
    "df2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acquired-wagon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.StructType"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df2.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "collaborative-blackjack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- logname: string (nullable = true)\n",
      " |-- time: integer (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- response: integer (nullable = true)\n",
      " |-- bytes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "weighted-damage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ArrayType', 'AtomicType', 'BinaryType', 'BooleanType', 'ByteType', 'CloudPickleSerializer', 'DataType', 'DataTypeSingleton', 'DateConverter', 'DateType', 'DatetimeConverter', 'DecimalType', 'DoubleType', 'FloatType', 'FractionalType', 'IntegerType', 'IntegralType', 'JavaClass', 'LongType', 'MapType', 'NullType', 'NumericType', 'Row', 'ShortType', 'SparkContext', 'StringType', 'StructField', 'StructType', 'TimestampType', 'UserDefinedType', '_FIXED_DECIMAL', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_acceptable_types', '_all_atomic_types', '_all_complex_types', '_array_signed_int_typecode_ctype_mappings', '_array_type_mappings', '_array_unsigned_int_typecode_ctype_mappings', '_atomic_types', '_create_converter', '_create_row', '_create_row_inbound_converter', '_has_nulltype', '_infer_schema', '_infer_type', '_int_size_to_type', '_make_type_verifier', '_merge_type', '_need_converter', '_parse_datatype_json_string', '_parse_datatype_json_value', '_parse_datatype_string', '_test', '_type_mappings', '_typecode', 'array', 'base64', 'basestring', 'calendar', 'ctypes', 'datetime', 'decimal', 'dt', 'json', 'long', 'os', 'platform', 're', 'register_input_converter', 'size', 'sys', 'time', 'unicode', 'warnings']\n"
     ]
    }
   ],
   "source": [
    "print(dir(pyspark.sql.types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "changed-directory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host', 'logname', 'time', 'method', 'url', 'response', 'bytes']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema.fieldNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "nasty-municipality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host', 'logname', 'time', 'method', 'url', 'response', 'bytes']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adapted-romania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df.schema.names), type(df.schema.fieldNames()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "banned-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, DateConverter, DateType, IntegerType\n",
    "\n",
    "# alternatively, you can use\n",
    "# from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "choice-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_schema = StructType([\n",
    "    # StructField(\"column_name\", columnType(), Nullable),\n",
    "    StructField(\"Host\", StringType(), False),\n",
    "    StructField(\"Logname\", StringType(), True),\n",
    "    StructField(\"Time\", IntegerType(), False),\n",
    "    StructField(\"Method\", StringType(), True),\n",
    "    StructField(\"URL\", StringType(), True),\n",
    "    StructField(\"Response\", StringType(), True),\n",
    "    # StructField(\"Bytes\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "leading-begin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema == our_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "broke-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(nasa_path, sep='\\t', header=True, schema=our_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "found-louis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Host: string (nullable = true)\n",
      " |-- Logname: string (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Method: string (nullable = true)\n",
      " |-- URL: string (nullable = true)\n",
      " |-- Response: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "developing-lecture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Host='199.72.81.55', Logname='-', Time=804571201, Method='GET', URL='/history/apollo/', Response='200'),\n",
       " Row(Host='unicomp6.unicomp.net', Logname='-', Time=804571206, Method='GET', URL='/shuttle/countdown/', Response='200'),\n",
       " Row(Host='199.120.110.21', Logname='-', Time=804571209, Method='GET', URL='/shuttle/missions/sts-73/mission-sts-73.html', Response='200'),\n",
       " Row(Host='burger.letters.com', Logname='-', Time=804571211, Method='GET', URL='/shuttle/countdown/liftoff.html', Response='304'),\n",
       " Row(Host='199.120.110.21', Logname='-', Time=804571211, Method='GET', URL='/shuttle/missions/sts-73/sts-73-patch-small.gif', Response='200')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "corporate-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_schema_with_extra_column = StructType([\n",
    "    # StructField(\"column_name\", columnType(), Nullable),\n",
    "    StructField(\"Host\", StringType(), False),\n",
    "    StructField(\"Logname\", StringType(), True),\n",
    "    StructField(\"Time\", IntegerType(), False),\n",
    "    StructField(\"Method\", StringType(), True),\n",
    "    StructField(\"URL\", StringType(), True),\n",
    "    StructField(\"Response\", StringType(), True),\n",
    "    StructField(\"Bytes\", StringType(), True),\n",
    "    StructField(\"Missing\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "civic-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(nasa_path, sep='\\t', header=True, schema=our_schema_with_extra_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "rental-range",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Host: string (nullable = true)\n",
      " |-- Logname: string (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Method: string (nullable = true)\n",
      " |-- URL: string (nullable = true)\n",
      " |-- Response: string (nullable = true)\n",
      " |-- Bytes: string (nullable = true)\n",
      " |-- Missing: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dimensional-blond",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---------+------+--------------------+--------+-----+-------+\n",
      "|                Host|Logname|     Time|Method|                 URL|Response|Bytes|Missing|\n",
      "+--------------------+-------+---------+------+--------------------+--------+-----+-------+\n",
      "|        199.72.81.55|      -|804571201|   GET|    /history/apollo/|     200| 6245|   null|\n",
      "|unicomp6.unicomp.net|      -|804571206|   GET| /shuttle/countdown/|     200| 3985|   null|\n",
      "|      199.120.110.21|      -|804571209|   GET|/shuttle/missions...|     200| 4085|   null|\n",
      "|  burger.letters.com|      -|804571211|   GET|/shuttle/countdow...|     304|    0|   null|\n",
      "|      199.120.110.21|      -|804571211|   GET|/shuttle/missions...|     200| 4179|   null|\n",
      "|  burger.letters.com|      -|804571212|   GET|/images/NASA-logo...|     304|    0|   null|\n",
      "|  burger.letters.com|      -|804571212|   GET|/shuttle/countdow...|     200|    0|   null|\n",
      "|     205.212.115.106|      -|804571212|   GET|/shuttle/countdow...|     200| 3985|   null|\n",
      "|         d104.aa.net|      -|804571213|   GET| /shuttle/countdown/|     200| 3985|   null|\n",
      "|      129.94.144.152|      -|804571213|   GET|                   /|     200| 7074|   null|\n",
      "|unicomp6.unicomp.net|      -|804571214|   GET|/shuttle/countdow...|     200|40310|   null|\n",
      "|unicomp6.unicomp.net|      -|804571214|   GET|/images/NASA-logo...|     200|  786|   null|\n",
      "|unicomp6.unicomp.net|      -|804571214|   GET|/images/KSC-logos...|     200| 1204|   null|\n",
      "|         d104.aa.net|      -|804571215|   GET|/shuttle/countdow...|     200|40310|   null|\n",
      "|         d104.aa.net|      -|804571215|   GET|/images/NASA-logo...|     200|  786|   null|\n",
      "|         d104.aa.net|      -|804571215|   GET|/images/KSC-logos...|     200| 1204|   null|\n",
      "|      129.94.144.152|      -|804571217|   GET|/images/ksclogo-m...|     304|    0|   null|\n",
      "|      199.120.110.21|      -|804571217|   GET|/images/launch-lo...|     200| 1713|   null|\n",
      "|ppptky391.asahi-n...|      -|804571218|   GET|/facts/about_ksc....|     200| 3977|   null|\n",
      "|  net-1-141.eden.com|      -|804571219|   GET|/shuttle/missions...|     200|34029|   null|\n",
      "+--------------------+-------+---------+------+--------------------+--------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "expensive-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+-------------------+------+---------------+------------------+------------------+-------+\n",
      "|summary|                Host|Logname|               Time|Method|            URL|          Response|             Bytes|Missing|\n",
      "+-------+--------------------+-------+-------------------+------+---------------+------------------+------------------+-------+\n",
      "|  count|                9999|   9999|               9999|  9999|           9999|              9999|              9999|      0|\n",
      "|   mean|                null|   null|8.045768773637364E8|  null|           null|210.66426642664266|24649.759175917592|   null|\n",
      "| stddev|                null|   null|  3636.201300821906|  null|           null|  32.9404221059951| 83860.87366060312|   null|\n",
      "|    min|     128.187.140.171|      -|          804571201|   GET|              /|               200|                 0|   null|\n",
      "|    max|zzsbtafe.slip.cc....|      -|          804584384|  POST|/whats-new.html|               404|             99942|   null|\n",
      "+-------+--------------------+-------+-------------------+------+---------------+------------------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "technological-increase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|                host|\n",
      "+-------+--------------------+\n",
      "|  count|                9999|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|     128.187.140.171|\n",
      "|    max|zzsbtafe.slip.cc....|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('host').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-bosnia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "industrial-funds",
   "metadata": {},
   "source": [
    "lets read another file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "standard-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_path = '../data/prime_nums.text'\n",
    "df = spark.read.csv(nasa_path) # , sep='\\t', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "attractive-index",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|  2\t  3\t  5\t  7\t ...|\n",
      "| 31\t 37\t 41\t 43\t ...|\n",
      "| 73\t 79\t 83\t 89\t ...|\n",
      "|127\t131\t137\t139\t1...|\n",
      "|179\t181\t191\t193\t1...|\n",
      "|233\t239\t241\t251\t2...|\n",
      "|283\t293\t307\t311\t3...|\n",
      "|353\t359\t367\t373\t3...|\n",
      "|419\t421\t431\t433\t4...|\n",
      "|467\t479\t487\t491\t4...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-massachusetts",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "intended-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "jp = '../data/resource_hvrh-b6nb.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "increasing-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(jp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "smaller-parking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- ratecodeid: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      " |-- vendorid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "narrow-integer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "binary-intervention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "nearby-opportunity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|tolls_amount|\n",
      "+------------+\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('tolls_amount').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ignored-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag =df.groupby('vendorid','ratecodeid').agg({'fare_amount': 'sum'}).alias('fare_sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "developing-inclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------------+\n",
      "|vendorid|ratecodeid|sum(fare_amount)|\n",
      "+--------+----------+----------------+\n",
      "|       2|         2|            52.0|\n",
      "|       2|         4|            16.0|\n",
      "|       2|         1|         12123.5|\n",
      "|       2|         5|           262.0|\n",
      "+--------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "trying-drama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendorid: string (nullable = true)\n",
      " |-- ratecodeid: string (nullable = true)\n",
      " |-- fare_sum: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ag.withColumnRenamed('sum(fare_amount)', 'fare_sum').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "manufactured-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "improved-daniel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Column', 'DataFrame', 'DataType', 'PandasUDFType', 'PythonEvalType', 'SparkContext', 'StringType', 'UserDefinedFunction', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_binary_mathfunctions', '_collect_list_doc', '_collect_set_doc', '_create_binary_mathfunction', '_create_column_from_literal', '_create_column_from_name', '_create_function', '_create_function_over_column', '_create_udf', '_create_window_function', '_functions', '_functions_1_4_over_column', '_functions_1_6_over_column', '_functions_2_1_over_column', '_functions_2_4', '_functions_deprecated', '_functions_over_column', '_lit_doc', '_message', '_options_to_str', '_string_functions', '_test', '_to_java_column', '_to_seq', '_window_functions', '_wrap_deprecated_function', 'abs', 'acos', 'add_months', 'approxCountDistinct', 'approx_count_distinct', 'array', 'array_contains', 'array_distinct', 'array_except', 'array_intersect', 'array_join', 'array_max', 'array_min', 'array_position', 'array_remove', 'array_repeat', 'array_sort', 'array_union', 'arrays_overlap', 'arrays_zip', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'ascii', 'asin', 'atan', 'atan2', 'avg', 'base64', 'basestring', 'bin', 'bitwiseNOT', 'blacklist', 'broadcast', 'bround', 'cbrt', 'ceil', 'coalesce', 'col', 'collect_list', 'collect_set', 'column', 'concat', 'concat_ws', 'conv', 'corr', 'cos', 'cosh', 'count', 'countDistinct', 'covar_pop', 'covar_samp', 'crc32', 'create_map', 'cume_dist', 'current_date', 'current_timestamp', 'date_add', 'date_format', 'date_sub', 'date_trunc', 'datediff', 'dayofmonth', 'dayofweek', 'dayofyear', 'decode', 'degrees', 'dense_rank', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'element_at', 'encode', 'exp', 'explode', 'explode_outer', 'expm1', 'expr', 'factorial', 'first', 'flatten', 'floor', 'format_number', 'format_string', 'from_csv', 'from_json', 'from_unixtime', 'from_utc_timestamp', 'functools', 'get_json_object', 'greatest', 'grouping', 'grouping_id', 'hash', 'hex', 'hour', 'hypot', 'ignore_unicode_prefix', 'initcap', 'input_file_name', 'instr', 'isnan', 'isnull', 'json_tuple', 'kurtosis', 'lag', 'last', 'last_day', 'lead', 'least', 'length', 'levenshtein', 'lit', 'locate', 'log', 'log10', 'log1p', 'log2', 'lower', 'lpad', 'ltrim', 'map_concat', 'map_entries', 'map_from_arrays', 'map_from_entries', 'map_keys', 'map_values', 'max', 'md5', 'mean', 'min', 'minute', 'monotonically_increasing_id', 'month', 'months_between', 'nanvl', 'next_day', 'ntile', 'overlay', 'pandas_udf', 'percent_rank', 'posexplode', 'posexplode_outer', 'pow', 'quarter', 'radians', 'rand', 'randn', 'rank', 'regexp_extract', 'regexp_replace', 'repeat', 'reverse', 'rint', 'round', 'row_number', 'rpad', 'rtrim', 'schema_of_csv', 'schema_of_json', 'second', 'sequence', 'sha1', 'sha2', 'shiftLeft', 'shiftRight', 'shiftRightUnsigned', 'shuffle', 'signum', 'sin', 'since', 'sinh', 'size', 'skewness', 'slice', 'sort_array', 'soundex', 'spark_partition_id', 'split', 'sqrt', 'stddev', 'stddev_pop', 'stddev_samp', 'struct', 'substring', 'substring_index', 'sum', 'sumDistinct', 'sys', 'tan', 'tanh', 'toDegrees', 'toRadians', 'to_csv', 'to_date', 'to_json', 'to_str', 'to_timestamp', 'to_utc_timestamp', 'translate', 'trim', 'trunc', 'udf', 'unbase64', 'unhex', 'unix_timestamp', 'upper', 'var_pop', 'var_samp', 'variance', 'warnings', 'weekofyear', 'when', 'window', 'xxhash64', 'year']\n"
     ]
    }
   ],
   "source": [
    "print(dir(psql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "recovered-cancer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------------+---+\n",
      "|vendorid|ratecodeid|sum(fare_amount)| rn|\n",
      "+--------+----------+----------------+---+\n",
      "|       2|         1|         12123.5|  1|\n",
      "|       2|         2|            52.0|  2|\n",
      "|       2|         4|            16.0|  3|\n",
      "|       2|         5|           262.0|  4|\n",
      "+--------+----------+----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "window = Window.orderBy(\"ratecodeid\").partitionBy(\"vendorid\")\n",
    "ag.withColumn('rn', psql.row_number().over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "contained-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cordless-inclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-----+-----------+---------------------+---------------------+--------------------+-------+---------------+------------+------------------+-------------------+----------+------------------+----------+------------+------------+-------------+---------+--------+\n",
      "|  dropoff_latitude|  dropoff_longitude|extra|fare_amount|improvement_surcharge|lpep_dropoff_datetime|lpep_pickup_datetime|mta_tax|passenger_count|payment_type|   pickup_latitude|   pickup_longitude|ratecodeid|store_and_fwd_flag|tip_amount|tolls_amount|total_amount|trip_distance|trip_type|vendorid|\n",
      "+------------------+-------------------+-----+-----------+---------------------+---------------------+--------------------+-------+---------------+------------+------------------+-------------------+----------+------------------+----------+------------+------------+-------------+---------+--------+\n",
      "|40.698043823242188|-73.924278259277344|  0.5|          8|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:29:...|    0.5|              1|           1|40.680610656738281|-73.928642272949219|         1|                 N|      1.86|           0|       11.16|         1.46|        1|       2|\n",
      "|40.761379241943359|-73.923919677734375|  0.5|       15.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:19:...|    0.5|              1|           2|40.723175048828125|-73.952674865722656|         1|                 N|         0|           0|        16.8|         3.56|        1|       2|\n",
      "|40.646072387695313|-74.013160705566406|  0.5|       16.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:19:...|    0.5|              1|           1|40.676105499267578|-73.971611022949219|         1|                 N|      4.45|           0|       22.25|         3.79|        1|       2|\n",
      "|40.689033508300781|-74.000648498535156|  0.5|       13.5|                  0.3| 2016-01-01T00:38:...|2016-01-01T00:22:...|    0.5|              1|           2|40.669578552246094|   -73.989501953125|         1|                 N|         0|           0|        14.8|         3.01|        1|       2|\n",
      "|40.663013458251953|-73.940719604492188|  0.5|         12|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:24:...|    0.5|              1|           2|40.682853698730469|-73.964729309082031|         1|                 N|         0|           0|        13.3|         2.55|        1|       2|\n",
      "|40.742111206054688|-73.867744445800781|  0.5|          7|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:32:...|    0.5|              1|           2|40.746456146240234|-73.891143798828125|         1|                 N|         0|           0|         8.3|         1.37|        1|       2|\n",
      "|40.745689392089844|-73.886192321777344|  0.5|          5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:34:...|    0.5|              1|           2|40.746196746826172|-73.896675109863281|         1|                 N|         0|           0|         6.3|         0.57|        1|       2|\n",
      "|40.794120788574219|-73.949150085449219|  0.5|          7|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:31:...|    0.5|              1|           2|40.803558349609375|-73.953353881835937|         1|                 N|         0|           0|         8.3|         1.01|        1|       2|\n",
      "|40.679725646972656|-73.971572875976562|  0.5|         12|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:24:...|    0.5|              1|           1|40.702816009521484|-73.994064331054688|         1|                 N|         2|           0|        15.3|         2.46|        1|       2|\n",
      "|40.739658355712891|-73.917549133300781|  0.5|          9|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:28:...|    0.5|              1|           1|40.756641387939453|-73.914131164550781|         1|                 N|       1.6|           0|        11.9|         1.61|        1|       2|\n",
      "|40.763126373291016|-73.921028137207031|  0.5|          6|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:32:...|    0.5|              1|           1|40.761829376220703|-73.911178588867188|         1|                 N|      1.82|           0|        9.12|         0.72|        1|       2|\n",
      "|40.718177795410156|-73.962753295898438|  0.5|        3.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:37:...|    0.5|              1|           1|40.715328216552734|-73.958168029785156|         1|                 N|      0.96|           0|        5.76|         0.32|        1|       2|\n",
      "|40.842765808105469|-73.924903869628906|  0.5|       14.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:21:...|    0.5|              1|           2|40.800785064697266|-73.946678161621094|         1|                 N|         0|           0|        15.8|         3.54|        1|       2|\n",
      "|40.775833129882812| -73.90240478515625|  0.5|          6|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:34:...|    0.5|              1|           2|40.763439178466797|-73.914291381835938|         1|                 N|         0|           0|         7.3|         1.10|        1|       2|\n",
      "|40.850196838378906|-73.932029724121094|  0.5|         11|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:26:...|    0.5|              1|           1|40.824314117431641|-73.943374633789063|         1|                 N|         2|           0|        14.3|         2.28|        1|       2|\n",
      "|40.640159606933594|-73.966880798339844|  0.5|        4.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:35:...|    0.5|              1|           1|40.632152557373047| -73.96697998046875|         1|                 N|      1.16|           0|        6.96|         0.68|        1|       2|\n",
      "|40.811866760253906|-73.951583862304688|  0.5|         10|                  0.3| 2016-01-01T00:38:...|2016-01-01T00:25:...|    0.5|              1|           2|40.814445495605469|-73.937843322753906|         1|                 N|         0|           0|        11.3|         1.36|        1|       2|\n",
      "|40.693325042724609|-73.947746276855469|  0.5|       15.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:17:...|    0.5|              1|           1|40.690513610839844|-73.990364074707031|         1|                 N|         3|           0|        19.8|         3.07|        1|       2|\n",
      "|40.751564025878906|-73.855522155761719|  0.5|        7.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:31:...|    0.5|              1|           2|    40.74755859375|-73.883827209472656|         1|                 N|         0|           0|         8.8|         1.52|        1|       2|\n",
      "|40.659637451171875|-73.976943969726563|  0.5|       11.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:25:...|    0.5|              1|           2|40.684413909912109|-73.980354309082031|         1|                 N|         0|           0|        12.8|         2.55|        1|       2|\n",
      "+------------------+-------------------+-----+-----------+---------------------+---------------------+--------------------+-------+---------------+------------+------------------+-------------------+----------+------------------+----------+------------+------------+-------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from data').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "mathematical-boating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|vendorid|fare_sum|\n",
      "+--------+--------+\n",
      "|       2| 12453.5|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select vendorid, sum(fare_amount) as fare_sum from data group by vendorid').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "hidden-physics",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o240.showString.\n: java.lang.UnsupportedOperationException: Cannot generate code for expression: row_number()\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode(Expression.scala:304)\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode$(Expression.scala:303)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.DeclarativeAggregate.doGenCode(interfaces.scala:369)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.doGenCode(Cast.scala:828)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.genCode(Cast.scala:823)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:159)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$generateResultFunction$5(HashAggregateExec.scala:575)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultFunction(HashAggregateExec.scala:575)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:762)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:169)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:48)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:632)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:692)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:316)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:434)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat jdk.internal.reflect.GeneratedMethodAccessor70.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-1ac90ee02cca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'select vendorid, sum(fare_amount) as fare_sum, row_number() as rn from data group by vendorid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o240.showString.\n: java.lang.UnsupportedOperationException: Cannot generate code for expression: row_number()\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode(Expression.scala:304)\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode$(Expression.scala:303)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.DeclarativeAggregate.doGenCode(interfaces.scala:369)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.doGenCode(Cast.scala:828)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.genCode(Cast.scala:823)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:159)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$generateResultFunction$5(HashAggregateExec.scala:575)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultFunction(HashAggregateExec.scala:575)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:762)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:169)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:48)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:632)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:692)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:316)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:434)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat jdk.internal.reflect.GeneratedMethodAccessor70.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select vendorid, sum(fare_amount) as fare_sum, row_number() as rn from data group by vendorid').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "improving-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('out/df.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "duplicate-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv('out/df.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fifth-proceeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan json [dropoff_latitude#902,dropoff_longitude#903,extra#904,fare_amount#905,improvement_surcharge#906,lpep_dropoff_datetime#907,lpep_pickup_datetime#908,mta_tax#909,passenger_count#910,payment_type#911,pickup_latitude#912,pickup_longitude#913,ratecodeid#914,store_and_fwd_flag#915,tip_amount#916,tolls_amount#917,total_amount#918,trip_distance#919,trip_type#920,vendorid#921] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex[file:/D:/das_2021/data/resource_hvrh-b6nb.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dropoff_latitude:string,dropoff_longitude:string,extra:string,fare_amount:string,improveme...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "strong-portsmouth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "linear-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
