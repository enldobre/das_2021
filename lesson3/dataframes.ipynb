{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "desperate-royal",
   "metadata": {},
   "source": [
    "#### always add the following cell to the start of a notebook when using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "polished-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets start the spark session\n",
    "# the entry point for an spark app is the SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"FirstApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "compatible-drunk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.212.197:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FirstApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcd7c469d90>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you don't get an output here it means that jupyter isn't connected to pyspark\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-hurricane",
   "metadata": {},
   "source": [
    "#### use this to debug any errors related to wrong path/file not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "transparent-roommate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ldobre/das_2021/lesson3'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "# os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-voluntary",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-asthma",
   "metadata": {},
   "source": [
    "we can create a dataframe from a list that we parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "sustainable-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('1', 'JS', 179),\n",
    "    ('2', 'CL', 175),\n",
    "    ('3', 'AS', 140),\n",
    "    ('4', 'LF', 170)\n",
    "]\n",
    "df = spark.createDataFrame(\n",
    "        data, \n",
    "        ['Id', 'Name', 'Height']  # column list\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "hollow-empty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "retained-drawing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| Id|Name|Height|\n",
      "+---+----+------+\n",
      "|  1|  JS|   179|\n",
      "|  2|  CL|   175|\n",
      "|  3|  AS|   140|\n",
      "|  4|  LF|   170|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)  # default 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "weekly-wednesday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id='1', Name='JS', Height=179), Row(Id='2', Name='CL', Height=175)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can retrieve a subset of the df using head\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "reflected-noise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "discrete-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "nervous-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also pass the schema \n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "mature-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    # StructField(\"column_name\", columnType(), Nullable),\n",
    "    StructField(\"id\", StringType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "recent-isaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = false)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-connectivity",
   "metadata": {},
   "source": [
    "## SPARK.READ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-michael",
   "metadata": {},
   "source": [
    "usually we want to create a df from a data source.\n",
    "Spark can read from the following sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-headline",
   "metadata": {},
   "source": [
    "## CSV\n",
    "spark.read.csv\n",
    "\n",
    "usefull when reading from delimited files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "outdoor-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '../data/airports.text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "proved-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    csv_path,\n",
    "    # header=True,\n",
    "    inferSchema=True  # affects performance as data as parsed a second time to inferSchema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fantastic-employer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: double (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: double (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "latter-alabama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------+--------------+-----------+----+----+-----------------+-------------------+------------------+------------------+----+--------------+\n",
      "|summary|               _c0|           _c1|           _c2|        _c3| _c4| _c5|              _c6|                _c7|               _c8|               _c9|_c10|          _c11|\n",
      "+-------+------------------+--------------+--------------+-----------+----+----+-----------------+-------------------+------------------+------------------+----+--------------+\n",
      "|  count|              8107|          8107|          8107|       8107|5880|8044|             8107|               8107|              8107|              8107|8107|          8107|\n",
      "|   mean|4766.3610460096215|           NaN|           NaN|       null| NaN|null|26.81772048414372|-3.9219686495380888| 933.4493647465154|0.1692364623165166|null|          null|\n",
      "| stddev| 2943.205192743097|           NaN|           NaN|       null| NaN|null|27.86695318132571|  85.90087275710746|1624.7408989269568| 5.737325603398365|null|          null|\n",
      "|    min|                 1|    7 Novembre|108 Mile Ranch|Afghanistan| %u0|%u04|       -89.999997|           -179.877|             -1266|             -12.0|   A|Africa/Abidjan|\n",
      "|    max|              9541|Žilina Airport|        Žilina|   Zimbabwe| ИКУ|  \\N|        82.517778|            179.951|             14472|              13.0|   Z|            \\N|\n",
      "+-------+------------------+--------------+--------------+-----------+----+----+-----------------+-------------------+------------------+------------------+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe() can be used to glance over the data statics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "inappropriate-exchange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------+----------------+---+----+---------+----------+----+----+----+--------------------+\n",
      "|_c0|                 _c1|           _c2|             _c3|_c4| _c5|      _c6|       _c7| _c8| _c9|_c10|                _c11|\n",
      "+---+--------------------+--------------+----------------+---+----+---------+----------+----+----+----+--------------------+\n",
      "|  1|              Goroka|        Goroka|Papua New Guinea|GKA|AYGA|-6.081689|145.391881|5282|10.0|   U|Pacific/Port_Moresby|\n",
      "|  2|              Madang|        Madang|Papua New Guinea|MAG|AYMD|-5.207083|  145.7887|  20|10.0|   U|Pacific/Port_Moresby|\n",
      "|  3|         Mount Hagen|   Mount Hagen|Papua New Guinea|HGU|AYMH|-5.826789|144.295861|5388|10.0|   U|Pacific/Port_Moresby|\n",
      "|  4|              Nadzab|        Nadzab|Papua New Guinea|LAE|AYNZ|-6.569828|146.726242| 239|10.0|   U|Pacific/Port_Moresby|\n",
      "|  5|Port Moresby Jack...|  Port Moresby|Papua New Guinea|POM|AYPY|-9.443383| 147.22005| 146|10.0|   U|Pacific/Port_Moresby|\n",
      "|  6|          Wewak Intl|         Wewak|Papua New Guinea|WWK|AYWK|-3.583828|143.669186|  19|10.0|   U|Pacific/Port_Moresby|\n",
      "|  7|          Narsarsuaq|  Narssarssuaq|       Greenland|UAK|BGBW|61.160517|-45.425978| 112|-3.0|   E|     America/Godthab|\n",
      "|  8|                Nuuk|      Godthaab|       Greenland|GOH|BGGH|64.190922|-51.678064| 283|-3.0|   E|     America/Godthab|\n",
      "|  9|   Sondre Stromfjord|   Sondrestrom|       Greenland|SFJ|BGSF|67.016969|-50.689325| 165|-3.0|   E|     America/Godthab|\n",
      "| 10|      Thule Air Base|         Thule|       Greenland|THU|BGTL|76.531203|-68.703161| 251|-4.0|   E|       America/Thule|\n",
      "| 11|            Akureyri|      Akureyri|         Iceland|AEY|BIAR|65.659994|-18.072703|   6| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 12|         Egilsstadir|   Egilsstadir|         Iceland|EGS|BIEG|65.283333|-14.401389|  76| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 13|        Hornafjordur|          Hofn|         Iceland|HFN|BIHN|64.295556|-15.227222|  24| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 14|             Husavik|       Husavik|         Iceland|HZK|BIHU|65.952328|-17.425978|  48| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 15|          Isafjordur|    Isafjordur|         Iceland|IFJ|BIIS|66.058056|-23.135278|   8| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 16|Keflavik Internat...|      Keflavik|         Iceland|KEF|BIKF|   63.985|-22.605556| 171| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 17|      Patreksfjordur|Patreksfjordur|         Iceland|PFJ|BIPA|65.555833|   -23.965|  11| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 18|           Reykjavik|     Reykjavik|         Iceland|RKV|BIRK|    64.13|-21.940556|  48| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 19|        Siglufjordur|  Siglufjordur|         Iceland|SIJ|BISI|66.133333|-18.916667|  10| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 20|      Vestmannaeyjar|Vestmannaeyjar|         Iceland|VEY|BIVM|63.424303|-20.278875| 326| 0.0|   N|  Atlantic/Reykjavik|\n",
      "+---+--------------------+--------------+----------------+---+----+---------+----------+----+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-extent",
   "metadata": {},
   "source": [
    "using the output from the previous 2 cells, build a schema and pass it at read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "civilian-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_schema = StructType([\n",
    "    # StructField(\"column_name\", columnType(), Nullable),\n",
    "    # edit this and add the columns\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "confident-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(csv_path, schema=csv_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "concrete-daily",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "++\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-charleston",
   "metadata": {},
   "source": [
    "## TEXT\n",
    "spark.read.text\n",
    "\n",
    "similar to spark.Context.textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "sought-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = '../data/word_count.text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dramatic-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "built-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|Liverpool Footbal...|\n",
      "|                    |\n",
      "|Founded in 1892, ...|\n",
      "|                    |\n",
      "|One of the most w...|\n",
      "|                    |\n",
      "|The club's suppor...|\n",
      "|                    |\n",
      "|Liverpool F.C. wa...|\n",
      "|                    |\n",
      "|Liverpool played ...|\n",
      "|                    |\n",
      "|Liverpool reached...|\n",
      "|Statue of a man w...|\n",
      "|Statue of Bill Sh...|\n",
      "|                    |\n",
      "|The club was prom...|\n",
      "|                    |\n",
      "|Paisley retired i...|\n",
      "|3 burgundy tablet...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "incomplete-federal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method text in module pyspark.sql.readwriter:\n",
      "\n",
      "text(paths, wholetext=False, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      "    string column named \"value\", and followed by partitioned columns if there\n",
      "    are any.\n",
      "    The text files must be encoded as UTF-8.\n",
      "    \n",
      "    By default, each line in the text file is a new row in the resulting DataFrame.\n",
      "    \n",
      "    :param paths: string, or list of strings, for input path(s).\n",
      "    :param wholetext: if true, read each file from input path(s) as a single row.\n",
      "    :param lineSep: defines the line separator that should be used for parsing. If None is\n",
      "                    set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.\n",
      "    :param pathGlobFilter: an optional glob pattern to only include files with paths matching\n",
      "                           the pattern. The syntax follows `org.apache.hadoop.fs.GlobFilter`.\n",
      "                           It does not change the behavior of `partition discovery`_.\n",
      "    :param recursiveFileLookup: recursively scan a directory for files. Using this option\n",
      "                                disables `partition discovery`_.\n",
      "    \n",
      "    >>> df = spark.read.text('python/test_support/sql/text-test.txt')\n",
      "    >>> df.collect()\n",
      "    [Row(value='hello'), Row(value='this')]\n",
      "    >>> df = spark.read.text('python/test_support/sql/text-test.txt', wholetext=True)\n",
      "    >>> df.collect()\n",
      "    [Row(value='hello\\nthis')]\n",
      "    \n",
      "    .. versionadded:: 1.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-dictionary",
   "metadata": {},
   "source": [
    "## JSON\n",
    "spark.read.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "piano-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '../data/resource_hvrh-b6nb.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "irish-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "incomplete-citizenship",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- ratecodeid: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      " |-- vendorid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-lancaster",
   "metadata": {},
   "source": [
    "as long as they have a valid schema the json can be different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "pending-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonStrings  = ['{\"uploadTimeStamp\":\"1500618037189\",\"ID\":\"123ID\",\"data\":[{\"Data\":{\"unit\":\"rpm\",\"value\":\"0\"},\"EventID\":\"E1\",\"Timestamp\":1500618037189,\"pii\":{}},{\"Data\":{\"heading\":\"N\",\"loc1\":\"false\",\"loc2\":\"13.022425\",\"loc3\":\"77.760587\",\"loc4\":\"false\",\"speed\":\"10\"},\"EventID\":\"E2\",\"Timestamp\":1500618037189,\"pii\":{}},{\"Data\":{\"x\":\"1.1\",\"y\":\"1.2\",\"z\":\"2.2\"},\"EventID\":\"E3\",\"Timestamp\":1500618037189,\"pii\":{}},{\"EventID\":\"E4\",\"Data\":{\"value\":\"50\",\"unit\":\"percentage\"},\"Timestamp\":1500618037189},{\"Data\":{\"unit\":\"kmph\",\"value\":\"60\"},\"EventID\":\"E5\",\"Timestamp\":1500618037189,\"pii\":{}}]}',\n",
    " '{\"uploadTimeStamp\":\"1500618045735\",\"ID\":\"123ID\",\"data\":[{\"Data\":{\"unit\":\"rpm\",\"value\":\"0\"},\"EventID\":\"E1\",\"Timestamp\":1500618045735,\"pii\":{}},{\"Data\":{\"heading\":\"N\",\"loc1\":\"false\",\"loc2\":\"13.022425\",\"loc3\":\"77.760587\",\"loc4\":\"false\",\"speed\":\"10\"},\"EventID\":\"E2\",\"Timestamp\":1500618045735,\"pii\":{}},{\"Data\":{\"x\":\"1.1\",\"y\":\"1.2\",\"z\":\"2.2\"},\"EventID\":\"E3\",\"Timestamp\":1500618045735,\"pii\":{}},{\"EventID\":\"E4\",\"Data\":{\"value\":\"50\",\"unit\":\"percentage\"},\"Timestamp\":1500618045735},{\"Data\":{\"unit\":\"kmph\",\"value\":\"60\"},\"EventID\":\"E5\",\"Timestamp\":1500618045735,\"pii\":{}}]}',\n",
    " '{\"REGULAR_DUMMY\":\"REGULAR_DUMMY\", \"ID\":\"123ID\", \"uploadTimeStamp\":1500546893837}',\n",
    " '{\"REGULAR_DUMMY\":\"text_of_json_per_item_in_list\"}'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "entire-driver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+---------------+\n",
      "|   ID|       REGULAR_DUMMY|                data|uploadTimeStamp|\n",
      "+-----+--------------------+--------------------+---------------+\n",
      "|123ID|                null|[[[,,,,,, rpm, 0,...|  1500618037189|\n",
      "|123ID|                null|[[[,,,,,, rpm, 0,...|  1500618045735|\n",
      "|123ID|       REGULAR_DUMMY|                null|  1500546893837|\n",
      "| null|text_of_json_per_...|                null|           null|\n",
      "+-----+--------------------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonRDD = spark.sparkContext.parallelize(jsonStrings)\n",
    "df = spark.read.json(jsonRDD)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "defined-clone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- REGULAR_DUMMY: string (nullable = true)\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Data: struct (nullable = true)\n",
      " |    |    |    |-- heading: string (nullable = true)\n",
      " |    |    |    |-- loc1: string (nullable = true)\n",
      " |    |    |    |-- loc2: string (nullable = true)\n",
      " |    |    |    |-- loc3: string (nullable = true)\n",
      " |    |    |    |-- loc4: string (nullable = true)\n",
      " |    |    |    |-- speed: string (nullable = true)\n",
      " |    |    |    |-- unit: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- x: string (nullable = true)\n",
      " |    |    |    |-- y: string (nullable = true)\n",
      " |    |    |    |-- z: string (nullable = true)\n",
      " |    |    |-- EventID: string (nullable = true)\n",
      " |    |    |-- Timestamp: long (nullable = true)\n",
      " |-- uploadTimeStamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the schema of the json is merged\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with Spark 2.2 you can read a multiline json\n",
    "# ideally you want to receive the json on a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "found-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_json = '../data/multiline.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "elegant-endorsement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json(m_json).printSchema(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "opponent-northwest",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).json(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).json(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-37dc55711b32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).json(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).json(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().;"
     ]
    }
   ],
   "source": [
    "spark.read.json(m_json).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "institutional-verification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json(m_json, multiLine=True).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "attractive-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df['data'].isNotNull()).drop('REGULAR_DUMMY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "monthly-demonstration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|data                                                                                                                                                                                                                                                       |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[[[,,,,,, rpm, 0,,,], E1, 1500618037189], [[N, false, 13.022425, 77.760587, false, 10,,,,,], E2, 1500618037189], [[,,,,,,,, 1.1, 1.2, 2.2], E3, 1500618037189], [[,,,,,, percentage, 50,,,], E4, 1500618037189], [[,,,,,, kmph, 60,,,], E5, 1500618037189]]|\n",
      "|[[[,,,,,, rpm, 0,,,], E1, 1500618045735], [[N, false, 13.022425, 77.760587, false, 10,,,,,], E2, 1500618045735], [[,,,,,,,, 1.1, 1.2, 2.2], E3, 1500618045735], [[,,,,,, percentage, 50,,,], E4, 1500618045735], [[,,,,,, kmph, 60,,,], E5, 1500618045735]]|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('data').show(20, False)  # why did I used False here?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "affecting-colleague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Data: struct (nullable = true)\n",
      " |    |    |    |-- heading: string (nullable = true)\n",
      " |    |    |    |-- loc1: string (nullable = true)\n",
      " |    |    |    |-- loc2: string (nullable = true)\n",
      " |    |    |    |-- loc3: string (nullable = true)\n",
      " |    |    |    |-- loc4: string (nullable = true)\n",
      " |    |    |    |-- speed: string (nullable = true)\n",
      " |    |    |    |-- unit: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- x: string (nullable = true)\n",
      " |    |    |    |-- y: string (nullable = true)\n",
      " |    |    |    |-- z: string (nullable = true)\n",
      " |    |    |-- EventID: string (nullable = true)\n",
      " |    |    |-- Timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('data').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "imposed-printer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|speed    |\n",
      "+---------+\n",
      "|[, 10,,,]|\n",
      "|[, 10,,,]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('data.Data.speed').show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "protecting-attribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- speed: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('data.Data.speed').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "quality-measurement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+\n",
      "|col                                                                    |\n",
      "+-----------------------------------------------------------------------+\n",
      "|[[[,,,,,, rpm, 0,,,], E1, 1500618037189]]                              |\n",
      "|[[[N, false, 13.022425, 77.760587, false, 10,,,,,], E2, 1500618037189]]|\n",
      "|[[[,,,,,,,, 1.1, 1.2, 2.2], E3, 1500618037189]]                        |\n",
      "|[[[,,,,,, percentage, 50,,,], E4, 1500618037189]]                      |\n",
      "|[[[,,,,,, kmph, 60,,,], E5, 1500618037189]]                            |\n",
      "|[[[,,,,,, rpm, 0,,,], E1, 1500618045735]]                              |\n",
      "|[[[N, false, 13.022425, 77.760587, false, 10,,,,,], E2, 1500618045735]]|\n",
      "|[[[,,,,,,,, 1.1, 1.2, 2.2], E3, 1500618045735]]                        |\n",
      "|[[[,,,,,, percentage, 50,,,], E4, 1500618045735]]                      |\n",
      "|[[[,,,,,, kmph, 60,,,], E5, 1500618045735]]                            |\n",
      "+-----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exploding nested jsons fields is a \"hard\" problem in spark\n",
    "from pyspark.sql.functions import explode, arrays_zip\n",
    "df.select(explode(arrays_zip('data'))).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "identical-profile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col: struct (nullable = false)\n",
      " |    |-- data: struct (nullable = true)\n",
      " |    |    |-- Data: struct (nullable = true)\n",
      " |    |    |    |-- heading: string (nullable = true)\n",
      " |    |    |    |-- loc1: string (nullable = true)\n",
      " |    |    |    |-- loc2: string (nullable = true)\n",
      " |    |    |    |-- loc3: string (nullable = true)\n",
      " |    |    |    |-- loc4: string (nullable = true)\n",
      " |    |    |    |-- speed: string (nullable = true)\n",
      " |    |    |    |-- unit: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- x: string (nullable = true)\n",
      " |    |    |    |-- y: string (nullable = true)\n",
      " |    |    |    |-- z: string (nullable = true)\n",
      " |    |    |-- EventID: string (nullable = true)\n",
      " |    |    |-- Timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(explode(arrays_zip('data'))).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-pressing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "appointed-devices",
   "metadata": {},
   "source": [
    "## JDBC\n",
    "spark.read.jdbc\n",
    "\n",
    "depending on the number of partitions, the db will receive multiple connections. This might make the db unresponsive.\n",
    "\n",
    "used less in big projects\n",
    "\n",
    "the code below is just an example. read the following article for more details about jdbc reads\n",
    "https://github.com/awesome-spark/spark-gotchas/blob/master/05_spark_sql_and_dataset_api.md#reading-data-using-jdbc-source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "alternative-visiting",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o541.load.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:105)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:105)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:344)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:221)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-382e9acff900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjdbcDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jdbc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jdbc:postgresql:dbserver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dbtable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"schema.tablename\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"username\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o541.load.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:105)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:105)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:344)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:221)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "\n",
    "jdbcDF2 = spark.read \\\n",
    "    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "          properties={\"user\": \"username\", \"password\": \"password\"})\n",
    "\n",
    "# Specifying dataframe column data types on read\n",
    "jdbcDF3 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"customSchema\", \"id DECIMAL(38, 0), name STRING\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-living",
   "metadata": {},
   "source": [
    "## Parquet\n",
    "spark.read.parquet\n",
    "\n",
    "https://databricks.com/glossary/what-is-parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "criminal-customs",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parquet_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-1130567cb16c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parquet_path' is not defined"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-shield",
   "metadata": {},
   "source": [
    "read more about partition discovery\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-tunnel",
   "metadata": {},
   "source": [
    "## FORMAT & LOAD\n",
    "generic way of reading data from the above data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "spatial-factory",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parquet_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-2d085ba0daad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parquet_path' is not defined"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('jdbc').option().load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option().load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-riding",
   "metadata": {},
   "source": [
    "usefull when developing frameworks (reading metadata and using generic ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-watson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "organic-kinase",
   "metadata": {},
   "source": [
    "## Write\n",
    "same as read, with additional options related to number of partitions.\n",
    "\n",
    "assuming df is the final dataframe, you can do something like in the cells below\n",
    "\n",
    "read the entire list of options at\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(output_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.jdbc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.format('parquet|jdbc|json').option().save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-honolulu",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "educational-pulse",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "offensive-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '../data/uk-postcode.csv'\n",
    "df = spark.read.csv(csv_file, header = True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "plastic-force",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "|Postcode|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|       Region|Postcodes|Active postcodes|Population|Households|\n",
      "+--------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "|     AB1| 57.1269| -2.13644| 391839|  804005|NJ918040|            Aberdeen|     Aberdeen|     2655|               0|      null|      null|\n",
      "|     AB2| 57.1713| -2.14152| 391541|  808948|NJ915089|            Aberdeen|     Aberdeen|     3070|               0|      null|      null|\n",
      "|     AB3| 57.0876| -2.59624| 363963|  799780|NO639997|            Aberdeen|     Aberdeen|     2168|               0|      null|      null|\n",
      "|     AB4| 57.5343| -2.12713| 392487|  849358|NJ924493|Fraserburgh, Pete...|     Aberdeen|     2956|               0|      null|      null|\n",
      "|     AB5| 57.4652| -2.64764| 361248|  841843|NJ612418|Buckie, Huntly, I...|     Aberdeen|     3002|               0|      null|      null|\n",
      "|     AB9| 57.1466|  -2.1142| 393189|  806196|NJ931061|            Aberdeen|     Aberdeen|     1066|               0|      null|      null|\n",
      "|    AB10| 57.1348| -2.11748| 392988|  804882|NJ929048|Aberdeen city cen...|     Aberdeen|      888|             675|     21964|     11517|\n",
      "|    AB11| 57.1371| -2.09341| 394445|  805136|NJ944051|Aberdeen city cen...|     Aberdeen|      889|             644|     21237|     10926|\n",
      "|    AB12| 57.1033| -2.11034| 393414|  801375|NJ934013|Aberdeen, Altens,...|     Aberdeen|      991|             782|     25414|     10688|\n",
      "|    AB13| 57.1127| -2.24469| 385279|  802443|NJ852024|          Milltimber|     Aberdeen|      100|              80|      2725|       947|\n",
      "|    AB14| 57.1033| -2.27251| 383590|  801402|NJ835014|Peterculter, Uppe...|     Aberdeen|      164|             140|      4881|      2162|\n",
      "|    AB15| 57.1388| -2.16551| 390082|  805334|NJ900053|Aberdeen, Bieldsi...|     Aberdeen|     1205|            1019|     35543|     15330|\n",
      "|    AB16| 57.1596| -2.15654| 390630|  807648|NJ906076|Aberdeen, Mastric...|     Aberdeen|      893|             776|     29238|     12874|\n",
      "|    AB21| 57.2091| -2.20174| 387912|  813165|NJ879131|Aberdeen, Blackbu...|     Aberdeen|      879|             732|     22181|      9650|\n",
      "|    AB22| 57.1864| -2.11913| 392898|  810627|NJ928106|Aberdeen, Bridge ...|     Aberdeen|      361|             300|     16311|      6978|\n",
      "|    AB23| 57.2088| -2.08971| 394680|  813118|NJ946131|Aberdeen, Balmedi...|     Aberdeen|      399|             311|     11143|      4517|\n",
      "|    AB24| 57.1634| -2.10828| 393550|  808065|NJ935080|Aberdeen, Old Abe...|     Aberdeen|      968|             808|     36343|     16935|\n",
      "|    AB25| 57.1534| -2.11422| 393189|  806953|NJ931069|Aberdeen city cen...|     Aberdeen|      610|             476|     18407|      9634|\n",
      "|    AB30|  56.846| -2.47726| 370986|  772829|NO709728|        Laurencekirk|Aberdeenshire|      349|             308|      7229|      2935|\n",
      "|    AB31| 57.0672| -2.50552| 369444|  797465|NO694974|            Banchory|Aberdeenshire|     1033|             629|     15319|      6096|\n",
      "+--------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "after-strategy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------------+------------------+------------------+-----------------+--------+-----------------+--------+-----------------+-----------------+------------------+------------------+\n",
      "|summary|Post Code|Latitude          |Longitude         |Easting           |Northing         |GridRef |Town/Area        |Region  |Postcodes        |Active postcodes |Population        |Households        |\n",
      "+-------+---------+------------------+------------------+------------------+-----------------+--------+-----------------+--------+-----------------+-----------------+------------------+------------------+\n",
      "|count  |3107     |3094              |3094              |3082              |3082             |3082    |3107             |3106    |3086             |3086             |2814              |2814              |\n",
      "|mean   |null     |53.034849482870136|-2.051575161550915|399520.80012978584|351774.8997404283|null    |null             |null    |832.2216461438755|564.9565780946209|22437.184434968018|9390.271144278608 |\n",
      "|stddev |null     |1.8865014315147148|1.8334605907478179|121798.85778550198|209187.830957896 |null    |null             |null    |600.2495165657779|397.5467297411277|16578.512623860708|6814.9887522729805|\n",
      "|min    |AB1      |49.1995           |-7.82593          |22681             |8307             |HU390111|Abbey Hey, Gorton|Aberdeen|1                |0                |2                 |1                 |\n",
      "|max    |ZE3      |60.3156           |1.73337           |653560            |1159304          |TV604994|York City Centre |York    |3621             |2644             |153812            |61886             |\n",
      "+-------+---------+------------------+------------------+------------------+-----------------+--------+-----------------+--------+-----------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "+-------+---------+------------------+------------------+------------------+-----------------+--------+-----------------+--------+-----------------+-----------------+------------------+------------------+\n",
    "|summary|Post Code|Latitude          |Longitude         |Easting           |Northing         |GridRef |Town/Area        |Region  |Postcodes        |Active postcodes |Population        |Households        |\n",
    "+-------+---------+------------------+------------------+------------------+-----------------+--------+-----------------+--------+-----------------+-----------------+------------------+------------------+\n",
    "|count  |3107     |3094              |3094              |3082              |3082             |3082    |3107             |3106    |3086             |3086             |2814              |2814              |\n",
    "|mean   |null     |53.034849482870136|-2.051575161550915|399520.80012978584|351774.8997404283|null    |null             |null    |832.2216461438755|564.9565780946209|22437.184434968018|9390.271144278608 |\n",
    "|stddev |null     |1.8865014315147148|1.8334605907478179|121798.85778550198|209187.830957896 |null    |null             |null    |600.2495165657779|397.5467297411277|16578.512623860708|6814.9887522729805|\n",
    "|min    |AB1      |49.1995           |-7.82593          |22681             |8307             |HU390111|Abbey Hey, Gorton|Aberdeen|1                |0                |2                 |1                 |\n",
    "|max    |ZE3      |60.3156           |1.73337           |653560            |1159304          |TV604994|York City Centre |York    |3621             |2644             |153812            |61886             |\n",
    "+-------+---------+------------------+------------------+------------------+-----------------+--------+-----------------+--------+-----------------+-----------------+------------------+------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "speaking-greeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|Postcode|Latitude|\n",
      "+--------+--------+\n",
      "|     AB1| 57.1269|\n",
      "|     AB2| 57.1713|\n",
      "|     AB3| 57.0876|\n",
      "|     AB4| 57.5343|\n",
      "|     AB5| 57.4652|\n",
      "|     AB9| 57.1466|\n",
      "|    AB10| 57.1348|\n",
      "|    AB11| 57.1371|\n",
      "|    AB12| 57.1033|\n",
      "|    AB13| 57.1127|\n",
      "|    AB14| 57.1033|\n",
      "|    AB15| 57.1388|\n",
      "|    AB16| 57.1596|\n",
      "|    AB21| 57.2091|\n",
      "|    AB22| 57.1864|\n",
      "|    AB23| 57.2088|\n",
      "|    AB24| 57.1634|\n",
      "|    AB25| 57.1534|\n",
      "|    AB30|  56.846|\n",
      "|    AB31| 57.0672|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select fields\n",
    "df.select('Postcode', 'Latitude').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "absolute-recovery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "|Post Code|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|       Region|Postcodes|Active postcodes|Population|Households|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "|      AB1| 57.1269| -2.13644| 391839|  804005|NJ918040|            Aberdeen|     Aberdeen|     2655|               0|      null|      null|\n",
      "|      AB2| 57.1713| -2.14152| 391541|  808948|NJ915089|            Aberdeen|     Aberdeen|     3070|               0|      null|      null|\n",
      "|      AB3| 57.0876| -2.59624| 363963|  799780|NO639997|            Aberdeen|     Aberdeen|     2168|               0|      null|      null|\n",
      "|      AB4| 57.5343| -2.12713| 392487|  849358|NJ924493|Fraserburgh, Pete...|     Aberdeen|     2956|               0|      null|      null|\n",
      "|      AB5| 57.4652| -2.64764| 361248|  841843|NJ612418|Buckie, Huntly, I...|     Aberdeen|     3002|               0|      null|      null|\n",
      "|      AB9| 57.1466|  -2.1142| 393189|  806196|NJ931061|            Aberdeen|     Aberdeen|     1066|               0|      null|      null|\n",
      "|     AB10| 57.1348| -2.11748| 392988|  804882|NJ929048|Aberdeen city cen...|     Aberdeen|      888|             675|     21964|     11517|\n",
      "|     AB11| 57.1371| -2.09341| 394445|  805136|NJ944051|Aberdeen city cen...|     Aberdeen|      889|             644|     21237|     10926|\n",
      "|     AB12| 57.1033| -2.11034| 393414|  801375|NJ934013|Aberdeen, Altens,...|     Aberdeen|      991|             782|     25414|     10688|\n",
      "|     AB13| 57.1127| -2.24469| 385279|  802443|NJ852024|          Milltimber|     Aberdeen|      100|              80|      2725|       947|\n",
      "|     AB14| 57.1033| -2.27251| 383590|  801402|NJ835014|Peterculter, Uppe...|     Aberdeen|      164|             140|      4881|      2162|\n",
      "|     AB15| 57.1388| -2.16551| 390082|  805334|NJ900053|Aberdeen, Bieldsi...|     Aberdeen|     1205|            1019|     35543|     15330|\n",
      "|     AB16| 57.1596| -2.15654| 390630|  807648|NJ906076|Aberdeen, Mastric...|     Aberdeen|      893|             776|     29238|     12874|\n",
      "|     AB21| 57.2091| -2.20174| 387912|  813165|NJ879131|Aberdeen, Blackbu...|     Aberdeen|      879|             732|     22181|      9650|\n",
      "|     AB22| 57.1864| -2.11913| 392898|  810627|NJ928106|Aberdeen, Bridge ...|     Aberdeen|      361|             300|     16311|      6978|\n",
      "|     AB23| 57.2088| -2.08971| 394680|  813118|NJ946131|Aberdeen, Balmedi...|     Aberdeen|      399|             311|     11143|      4517|\n",
      "|     AB24| 57.1634| -2.10828| 393550|  808065|NJ935080|Aberdeen, Old Abe...|     Aberdeen|      968|             808|     36343|     16935|\n",
      "|     AB25| 57.1534| -2.11422| 393189|  806953|NJ931069|Aberdeen city cen...|     Aberdeen|      610|             476|     18407|      9634|\n",
      "|     AB30|  56.846| -2.47726| 370986|  772829|NO709728|        Laurencekirk|Aberdeenshire|      349|             308|      7229|      2935|\n",
      "|     AB31| 57.0672| -2.50552| 369444|  797465|NO694974|            Banchory|Aberdeenshire|     1033|             629|     15319|      6096|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename column\n",
    "df = df.withColumnRenamed('Postcode', 'Post Code')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "smooth-alloy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Post Code',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Easting',\n",
       " 'Northing',\n",
       " 'GridRef',\n",
       " 'Town/Area',\n",
       " 'Region',\n",
       " 'Postcodes',\n",
       " 'Active postcodes',\n",
       " 'Population',\n",
       " 'Households']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema.fieldNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "certified-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "df = df.withColumn('type', when(col('Population') < 10000, 'village').when(df['Population'] < 20000, 'town').otherwise('city').alias('type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "instant-chase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   type|\n",
      "+-------+\n",
      "|   city|\n",
      "|   town|\n",
      "|village|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('type').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "incoming-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you write a condition like this, is easier to read it\n",
    "df = df.withColumn('schema',\n",
    "                   when(col('Population').isNull(), None)\\\n",
    "                   .when(col('Population') < 10000, 'village')\\\n",
    "                   .when(df['Population'] < 20000, 'town')\\\n",
    "                   .otherwise('city'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "metropolitan-marina",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StructType' object has no attribute 'isNull'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-239-2ed4c05aeec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'StructType' object has no attribute 'isNull'"
     ]
    }
   ],
   "source": [
    "df.filter(df.schema.isNull()).show(5)\n",
    "# why do we have an error here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "reflected-roulette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------+--------+--------+--------------------+-----------+---------+----------------+----------+----------+----+------+\n",
      "|Post Code|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|     Region|Postcodes|Active postcodes|Population|Households|type|schema|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+-----------+---------+----------------+----------+----------+----+------+\n",
      "|     CH28| 53.4005| -3.11196| 326165|  389873|SJ261898|Non-geographic, M...|     Wirral|       37|               4|      null|      null|null|  null|\n",
      "|     SN38| 51.5599| -1.78933| 414701|  184578|SU147845|             Swindon|    Swindon|      292|             151|      null|      null|null|  null|\n",
      "|      SO9| 50.9129|   -1.407| 441786|  112771|SU417127|Southampton city ...|Southampton|     1120|               0|      null|      null|null|  null|\n",
      "|     BN99| 50.8215|-0.356139| 515888|  103727|TQ158037|   Worthing, Lancing|   Worthing|      384|             347|      null|      null|null|  null|\n",
      "|     NE88| 54.9473| -1.61193| 424956|  561474|NZ249614|      Non-geographic|  Newcastle|       11|               8|      null|      null|null|  null|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+-----------+---------+----------------+----------+----------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['schema'].isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "+---------+--------+---------+-------+--------+--------+--------------------+--------+---------+----------------+----------+----------+----+\n",
    "|Post Code|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|  Region|Postcodes|Active postcodes|Population|Households|type|\n",
    "+---------+--------+---------+-------+--------+--------+--------------------+--------+---------+----------------+----------+----------+----+\n",
    "|      AB1| 57.1269| -2.13644| 391839|  804005|NJ918040|            Aberdeen|Aberdeen|     2655|               0|      null|      null|city|\n",
    "|      AB2| 57.1713| -2.14152| 391541|  808948|NJ915089|            Aberdeen|Aberdeen|     3070|               0|      null|      null|city|\n",
    "|      AB3| 57.0876| -2.59624| 363963|  799780|NO639997|            Aberdeen|Aberdeen|     2168|               0|      null|      null|city|\n",
    "|      AB4| 57.5343| -2.12713| 392487|  849358|NJ924493|Fraserburgh, Pete...|Aberdeen|     2956|               0|      null|      null|city|\n",
    "|      AB5| 57.4652| -2.64764| 361248|  841843|NJ612418|Buckie, Huntly, I...|Aberdeen|     3002|               0|      null|      null|city|\n",
    "+---------+--------+---------+-------+--------+--------+--------------------+--------+---------+----------------+----------+----------+----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "recreational-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------+--------+--------+--------------------+------+---------+----------------+----------+----------+----+------+\n",
      "|Post Code|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|Region|Postcodes|Active postcodes|Population|Households|type|schema|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+------+---------+----------------+----------+----------+----+------+\n",
      "|     CH28| 53.4005| -3.11196| 326165|  389873|SJ261898|Non-geographic, M...|Wirral|       37|               4|      null|      null|    |      |\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+------+---------+----------------+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace null\n",
    "df.na.fill('').filter(df['GridRef'] == 'SJ261898').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "congressional-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------+--------+--------+--------------------+------+---------+----------------+----------+----------+-----------+-----------+\n",
      "|Post Code|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|Region|Postcodes|Active postcodes|Population|Households|       type|     schema|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+------+---------+----------------+----------+----------+-----------+-----------+\n",
      "|     CH28| 53.4005| -3.11196| 326165|  389873|SJ261898|Non-geographic, M...|Wirral|       37|               4|      null|      null|ThisWasNULL|ThisWasNULL|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+------+---------+----------------+----------+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('ThisWasNULL').filter(df['GridRef'] == 'SJ261898').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "scenic-decision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------+--------+-------+---------+------+---------+----------------+----------+----------+----+------+\n",
      "|Post Code|Latitude|Longitude|Easting|Northing|GridRef|Town/Area|Region|Postcodes|Active postcodes|Population|Households|type|schema|\n",
      "+---------+--------+---------+-------+--------+-------+---------+------+---------+----------------+----------+----------+----+------+\n",
      "+---------+--------+---------+-------+--------+-------+---------+------+---------+----------------+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().filter(df['GridRef'] == 'SJ261898').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "boolean-coalition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(schema='city'), Row(schema='village'), Row(schema=''), Row(schema='Town')]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('schema').na.fill('').replace({'town': 'Town'}).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "ignored-meeting",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "col should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-bf414d5b7e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'extra_column'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'literal_value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m         \"\"\"\n\u001b[0;32m-> 2095\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2096\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: col should be Column"
     ]
    }
   ],
   "source": [
    "df.withColumn('extra_column', 'literal_value').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "impossible-aquatic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Post Code: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Easting: integer (nullable = true)\n",
      " |-- Northing: integer (nullable = true)\n",
      " |-- GridRef: string (nullable = true)\n",
      " |-- Town/Area: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Postcodes: integer (nullable = true)\n",
      " |-- Active postcodes: integer (nullable = true)\n",
      " |-- Population: integer (nullable = true)\n",
      " |-- Households: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- schema: string (nullable = true)\n",
      " |-- extra_column: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.withColumn('extra_column', lit('literal_value')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "impressed-corporation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = df.groupby('Region', 'Town/Area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "possible-morning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Region: string, Town/Area: string, count: bigint] 3107\n"
     ]
    }
   ],
   "source": [
    "print(ag.count(), df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "lyric-meeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              Region|count|\n",
      "+--------------------+-----+\n",
      "|           Worcester|   11|\n",
      "|           Charnwood|    2|\n",
      "|      North Kesteven|    2|\n",
      "|                Arun|    3|\n",
      "|       Epping Forest|    6|\n",
      "|             Waveney|    3|\n",
      "|              Stroud|    5|\n",
      "| Nuneaton & Bedworth|    3|\n",
      "|          New Forest|    8|\n",
      "|           Sedgemoor|    1|\n",
      "|           Newmarket|    1|\n",
      "|              Maldon|    2|\n",
      "|            Worthing|    6|\n",
      "|           Guildford|    8|\n",
      "|            Brighton|    1|\n",
      "|              Bolton|    9|\n",
      "|      North Tyneside|    7|\n",
      "|Central Bedfordshire|    8|\n",
      "|      Wellingborough|    2|\n",
      "|        Surrey Heath|    5|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Region').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "royal-supervision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------------------+-------------------+------------+-------------+--------------+---------------------+---------------+---------------+\n",
      "|              Region|   type|     sum(Latitude)|     sum(Longitude)|sum(Easting)|sum(Northing)|sum(Postcodes)|sum(Active postcodes)|sum(Population)|sum(Households)|\n",
      "+--------------------+-------+------------------+-------------------+------------+-------------+--------------+---------------------+---------------+---------------+\n",
      "|          New Forest|   city|          254.1205|-7.9065900000000005|     2147917|       514103|          6330|                 4719|         157569|          68905|\n",
      "|         Mole Valley|village|           51.2328|          -0.284652|      519858|       149577|           179|                  141|           4313|           1778|\n",
      "|           Hambleton|   town|           54.4691|           -1.16791|      454027|       508511|           612|                  527|          12832|           5821|\n",
      "|            Hereford|village|          156.4724| -8.624130000000001|     1020748|       754231|          1099|                  943|          19355|           8570|\n",
      "|          North Down|   city|163.81220000000002|          -16.83903|      500282|      1587640|          2538|                 2538|          83041|          34521|\n",
      "|   Llandrindod Wells|village|313.31329999999997|-20.214760000000002|     1839260|      1552555|          1033|                  917|          19566|           8772|\n",
      "|           Worcester|   null|104.39920000000001|           -4.43141|      770708|       511458|            43|                    5|           null|           null|\n",
      "|Richmond upon Thames|   town|          154.3924|-0.8182590000000001|     1560248|       525964|          2070|                 1280|          52176|          22252|\n",
      "|       Blaenau Gwent|   city|           155.291|            -9.5967|      952076|       623703|          2234|                 2019|          75512|          32885|\n",
      "|      South Ayrshire|   town|221.73919999999998|          -18.70586|      922980|      2475676|          2214|                 1796|          61814|          28978|\n",
      "|           Rotherham|   town|           53.4663|           -1.34525|      443564|       396817|           728|                  602|          19772|           8539|\n",
      "|            Strabane|village|           54.6961|           -7.61162|       38605|       547918|           305|                  305|           7691|           2836|\n",
      "|            Somerset|village|          715.3105|-40.547019999999996|     4722758|      1863500|          2890|                 2403|          58212|          25113|\n",
      "|               Leeds|village|322.93359999999996|           -9.20168|     2584773|      2617950|          3663|                 1804|          45159|          17165|\n",
      "|  Telford and Wrekin|   city|158.06709999999998|-7.3881499999999996|     1106463|       930750|          4018|                 2763|          99808|          39795|\n",
      "|              Rother|   town|203.67659999999998|           1.915533|     2297339|       464945|          3131|                 2241|          59782|          26656|\n",
      "|            Tendring|   town|103.78049999999999|            2.31362|     1234659|       452026|          1133|                  909|          25258|          11364|\n",
      "|         Bournemouth|   city|202.97639999999998|            -7.4313|     1640513|       375370|          3074|                 2365|         102019|          42792|\n",
      "|      East Hampshire|   town|102.00569999999999|          -1.869828|      949660|       246293|          1288|                  961|          23140|           9739|\n",
      "|              Antrim|   city|           54.7322|            -6.2574|      126023|       545789|           981|                  981|          40205|          15409|\n",
      "+--------------------+-------+------------------+-------------------+------------+-------------+--------------+---------------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Region', 'type').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "level-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = df.groupby('Region', 'type').agg({'Region': 'count'}).withColumnRenamed('count(Region)', 'asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "julian-thursday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---+\n",
      "|              Region|   type|asd|\n",
      "+--------------------+-------+---+\n",
      "|          New Forest|   city|  5|\n",
      "|         Mole Valley|village|  1|\n",
      "|           Hambleton|   town|  1|\n",
      "|            Hereford|village|  3|\n",
      "|          North Down|   city|  3|\n",
      "|   Llandrindod Wells|village|  6|\n",
      "|           Worcester|   null|  2|\n",
      "|Richmond upon Thames|   town|  3|\n",
      "|       Blaenau Gwent|   city|  3|\n",
      "|      South Ayrshire|   town|  4|\n",
      "|           Rotherham|   town|  1|\n",
      "|            Strabane|village|  1|\n",
      "|            Somerset|village| 14|\n",
      "|               Leeds|village|  6|\n",
      "|  Telford and Wrekin|   city|  3|\n",
      "|              Rother|   town|  4|\n",
      "|            Tendring|   town|  2|\n",
      "|         Bournemouth|   city|  4|\n",
      "|      East Hampshire|   town|  2|\n",
      "|              Antrim|   city|  1|\n",
      "+--------------------+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ag.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "assumed-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as pf\n",
    "ag = df.groupby('Region', 'type').agg(pf.sum('Population').alias('sum_population'), pf.count('PostCodes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "legislative-celtic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------+----------------+\n",
      "|              Region|   type|sum_population|count(PostCodes)|\n",
      "+--------------------+-------+--------------+----------------+\n",
      "|          New Forest|   city|        157569|               5|\n",
      "|         Mole Valley|village|          4313|               1|\n",
      "|           Hambleton|   town|         12832|               1|\n",
      "|            Hereford|village|         19355|               3|\n",
      "|          North Down|   city|         83041|               3|\n",
      "|   Llandrindod Wells|village|         19566|               6|\n",
      "|           Worcester|   null|          null|               2|\n",
      "|Richmond upon Thames|   town|         52176|               3|\n",
      "|       Blaenau Gwent|   city|         75512|               3|\n",
      "|      South Ayrshire|   town|         61814|               4|\n",
      "|           Rotherham|   town|         19772|               1|\n",
      "|            Strabane|village|          7691|               1|\n",
      "|            Somerset|village|         58212|              14|\n",
      "|               Leeds|village|         45159|               6|\n",
      "|  Telford and Wrekin|   city|         99808|               3|\n",
      "|              Rother|   town|         59782|               4|\n",
      "|            Tendring|   town|         25258|               2|\n",
      "|         Bournemouth|   city|        102019|               4|\n",
      "|      East Hampshire|   town|         23140|               2|\n",
      "|              Antrim|   city|         40205|               1|\n",
      "+--------------------+-------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ag.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-challenge",
   "metadata": {},
   "source": [
    "## Partitions\n",
    "\n",
    "Choose the right partition column. Think about how the cardinality of that column affects how the data gets distributed.\n",
    "\n",
    "When in doubt hash is better (safer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "charitable-smoke",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "elementary-ozone",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`col1`' given input columns: [type2.Active postcodes, type2.Easting, type2.GridRef, type2.Households, type2.Latitude, type2.Longitude, type2.Northing, type2.Population, type2.Post Code, type2.Postcodes, type2.Region, type2.Town/Area, type2.type];;\n'RepartitionByExpression ['col1, 'col2], 200\n+- Repartition 10, true\n   +- SubqueryAlias type2\n      +- Project [Post Code#2361, Latitude#2252, Longitude#2253, Easting#2254, Northing#2255, GridRef#2256, Town/Area#2257, Region#2258, Postcodes#2259, Active postcodes#2260, Population#2261, Households#2262, CASE WHEN (Population#2261 < 10000) THEN village WHEN (Population#2261 < 20000) THEN town ELSE city END AS type#4009]\n         +- Project [Post Code#2361, Latitude#2252, Longitude#2253, Easting#2254, Northing#2255, GridRef#2256, Town/Area#2257, Region#2258, Postcodes#2259, Active postcodes#2260, Population#2261, Households#2262, CASE WHEN (Population#2261 < 10000) THEN village WHEN (Population#2261 < 20000) THEN town ELSE city END AS type#3923]\n            +- Project [Postcode#2251 AS Post Code#2361, Latitude#2252, Longitude#2253, Easting#2254, Northing#2255, GridRef#2256, Town/Area#2257, Region#2258, Postcodes#2259, Active postcodes#2260, Population#2261, Households#2262]\n               +- Relation[Postcode#2251,Latitude#2252,Longitude#2253,Easting#2254,Northing#2255,GridRef#2256,Town/Area#2257,Region#2258,Postcodes#2259,Active postcodes#2260,Population#2261,Households#2262] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-51f246decc9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# repartition by columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'col1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'col2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# repartition by hash and cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mrepartition\u001b[0;34m(self, numPartitions, *cols)\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbasestring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numPartitions should be an int or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`col1`' given input columns: [type2.Active postcodes, type2.Easting, type2.GridRef, type2.Households, type2.Latitude, type2.Longitude, type2.Northing, type2.Population, type2.Post Code, type2.Postcodes, type2.Region, type2.Town/Area, type2.type];;\n'RepartitionByExpression ['col1, 'col2], 200\n+- Repartition 10, true\n   +- SubqueryAlias type2\n      +- Project [Post Code#2361, Latitude#2252, Longitude#2253, Easting#2254, Northing#2255, GridRef#2256, Town/Area#2257, Region#2258, Postcodes#2259, Active postcodes#2260, Population#2261, Households#2262, CASE WHEN (Population#2261 < 10000) THEN village WHEN (Population#2261 < 20000) THEN town ELSE city END AS type#4009]\n         +- Project [Post Code#2361, Latitude#2252, Longitude#2253, Easting#2254, Northing#2255, GridRef#2256, Town/Area#2257, Region#2258, Postcodes#2259, Active postcodes#2260, Population#2261, Households#2262, CASE WHEN (Population#2261 < 10000) THEN village WHEN (Population#2261 < 20000) THEN town ELSE city END AS type#3923]\n            +- Project [Postcode#2251 AS Post Code#2361, Latitude#2252, Longitude#2253, Easting#2254, Northing#2255, GridRef#2256, Town/Area#2257, Region#2258, Postcodes#2259, Active postcodes#2260, Population#2261, Households#2262]\n               +- Relation[Postcode#2251,Latitude#2252,Longitude#2253,Easting#2254,Northing#2255,GridRef#2256,Town/Area#2257,Region#2258,Postcodes#2259,Active postcodes#2260,Population#2261,Households#2262] csv\n"
     ]
    }
   ],
   "source": [
    "# you can use repartition to redistribuite data\n",
    "# triggers a shuffle\n",
    "\n",
    "# repartition by hash\n",
    "df = df.repartition(10)\n",
    "\n",
    "# repartition by columns\n",
    "df = df.repartition('col1','col2')\n",
    "\n",
    "# repartition by hash and cols\n",
    "df = df.repartition(10, 'col1', 'col2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use coalesce to reduce the number of partitions\n",
    "# assuming 10 partitions and 5 workers\n",
    "df = df.coalesce(5)\n",
    "# will reduce the number of partions without triggering a shuffle\n",
    "# df.coalesce(20) will not do anything because 20 > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "wanted-aerospace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('Region').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "surprising-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_200 = df.repartition('Region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "special-shopping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_200.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "running-perspective",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------+--------+--------+--------------------+--------------+---------+----------------+----------+----------+-------+\n",
      "|Post Code|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|        Region|Postcodes|Active postcodes|Population|Households|   type|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+--------------+---------+----------------+----------+----------+-------+\n",
      "|      WR1| 52.1974| -2.21749| 385231|  255485|SO852554|           Worcester|     Worcester|      835|             431|      9231|      4850|village|\n",
      "|      WR2| 52.1916| -2.23948| 383726|  254845|SO837548|           Worcester|     Worcester|      902|             711|     29799|     12092|   city|\n",
      "|      WR3| 52.2159| -2.21017| 385738|  257541|SO857575|           Worcester|     Worcester|      677|             515|     20397|      8738|   city|\n",
      "|      WR4| 52.2049| -2.18969| 387134|  256314|SO871563|           Worcester|     Worcester|      814|             610|     26127|     10881|   city|\n",
      "|      WR5| 52.1803| -2.20008| 386416|  253580|SO864535|           Worcester|     Worcester|      974|             753|     30933|     12785|   city|\n",
      "|      WR6| 52.2293| -2.36104| 375437|  259072|SO754590|           Worcester|     Worcester|      614|             504|     12216|      5056|   town|\n",
      "|      WR7| 52.1975| -2.06376| 395739|  255475|SO957554|           Worcester|     Worcester|      266|             212|      5348|      2131|village|\n",
      "|      WR8| 52.0907| -2.20143| 386296|  243613|SO862436|           Worcester|     Worcester|      421|             326|      8093|      3602|village|\n",
      "|     WR15| 52.3061| -2.57246| 361065|  267708|SO610677|       Tenbury Wells|     Worcester|      312|             250|      7196|      3110|village|\n",
      "|     WR78| 52.2018| -2.21146| 385645|  255973|SO856559|           Worcester|     Worcester|       15|               0|      null|      null|   city|\n",
      "|     WR99| 52.1974| -2.21995| 385063|  255485|SO850554|           Worcester|     Worcester|       28|               5|      null|      null|   city|\n",
      "|     LE11| 52.7707| -1.21979| 452735|  319519|SK527195|Loughborough, Cha...|     Charnwood|     2027|            1183|     60318|     22336|   city|\n",
      "|     LE12| 52.7681| -1.19231| 454593|  319250|SK545192|East Leake, West ...|     Charnwood|     1689|            1439|     61909|     25869|   city|\n",
      "|      LN4|  53.153|-0.390125| 507754|  362973|TF077629|Branston, Canwick...|North Kesteven|     1305|            1064|     38539|     16126|   city|\n",
      "|     NG34| 52.9876|-0.395382| 507814|  344567|TF078445|            Sleaford|North Kesteven|     1477|            1288|     40829|     17659|   city|\n",
      "|     BN16| 50.8154|-0.497802| 505923|  102836|TQ059028|Angmering, East P...|          Arun|     1267|             949|     28131|     13097|   city|\n",
      "|     BN17| 50.8149| -0.53709| 503157|  102725|TQ031027|Littlehampton, Cl...|          Arun|      891|             713|     28542|     12514|   city|\n",
      "|     BN18| 50.8488|-0.578428| 500172|  106438|TQ001064|Arundel, Amberley...|          Arun|      652|             477|     13232|      5566|   town|\n",
      "|      CM5| 51.7184| 0.247565| 555355|  204576|TL553045|Chipping Ongar, H...| Epping Forest|      584|             439|     10544|      4298|   town|\n",
      "|     CM16| 51.7002| 0.118833| 546522|  202286|TL465022|Epping, Theydon Bois| Epping Forest|      954|             641|     22217|      9398|   city|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+--------------+---------+----------------+----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_200.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "typical-forty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_200.coalesce(427).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "frequent-cathedral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_200.coalesce(100).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you do coalesce(1) only one worker will do the work.\n",
    "# if you have \"unexecuted\" transformations repartition(1) is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "august-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"save\" the dataframe in memory or disk to reusse it\n",
    "df = df.cache()  # <== very important to \"store\" the result in a new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-basics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-violence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
