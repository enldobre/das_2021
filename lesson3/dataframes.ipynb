{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "excessive-allocation",
   "metadata": {},
   "source": [
    "#### always add the following cell to the start of a notebook when using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlled-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets start the spark session\n",
    "# the entry point for an spark app is the SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"FirstApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "clear-booking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.212.197:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FirstApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6752e78e50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you don't get an output here it means that jupyter isn't connected to pyspark\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-biography",
   "metadata": {},
   "source": [
    "#### use this to debug any errors related to wrong path/file not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "identical-booking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ldobre/das_2021/lesson3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "# os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-broadway",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-andorra",
   "metadata": {},
   "source": [
    "we can create a dataframe from a list that we parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rational-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('1', 'JS', 179),\n",
    "    ('2', 'CL', 175),\n",
    "    ('3', 'AS', 140),\n",
    "    ('4', 'LF', 170)\n",
    "]\n",
    "df = spark.createDataFrame(\n",
    "        data, \n",
    "        ['Id', 'Name', 'Height']  # column list\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "based-poland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "forbidden-huntington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| Id|Name|Height|\n",
      "+---+----+------+\n",
      "|  1|  JS|   179|\n",
      "|  2|  CL|   175|\n",
      "|  3|  AS|   140|\n",
      "|  4|  LF|   170|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)  # default 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intended-artist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id='1', Name='JS', Height=179), Row(Id='2', Name='CL', Height=175)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can retrieve a subset of the df using head\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bottom-adelaide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "threaded-driving",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abroad-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also pass the schema \n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "plain-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    # StructField(\"column_name\", columnType(), Nullable),\n",
    "    StructField(\"id\", StringType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "developmental-symphony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = false)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-killing",
   "metadata": {},
   "source": [
    "## SPARK.READ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-resident",
   "metadata": {},
   "source": [
    "usually we want to create a df from a data source.\n",
    "Spark can read from the following sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-poison",
   "metadata": {},
   "source": [
    "## CSV\n",
    "spark.read.csv\n",
    "\n",
    "usefull when reading from delimited files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fallen-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '../data/airports.text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "accepting-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    csv_path,\n",
    "    # header=True,\n",
    "    inferSchema=True  # affects performance as data as parsed a second time to inferSchema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "offensive-carnival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: double (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: double (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "vocal-tenant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------+--------------+-----------+----+----+-----------------+-------------------+------------------+------------------+----+--------------+\n",
      "|summary|               _c0|           _c1|           _c2|        _c3| _c4| _c5|              _c6|                _c7|               _c8|               _c9|_c10|          _c11|\n",
      "+-------+------------------+--------------+--------------+-----------+----+----+-----------------+-------------------+------------------+------------------+----+--------------+\n",
      "|  count|              8107|          8107|          8107|       8107|5880|8044|             8107|               8107|              8107|              8107|8107|          8107|\n",
      "|   mean|4766.3610460096215|           NaN|           NaN|       null| NaN|null|26.81772048414372|-3.9219686495380888| 933.4493647465154|0.1692364623165166|null|          null|\n",
      "| stddev| 2943.205192743097|           NaN|           NaN|       null| NaN|null|27.86695318132571|  85.90087275710746|1624.7408989269568| 5.737325603398365|null|          null|\n",
      "|    min|                 1|    7 Novembre|108 Mile Ranch|Afghanistan| %u0|%u04|       -89.999997|           -179.877|             -1266|             -12.0|   A|Africa/Abidjan|\n",
      "|    max|              9541|Žilina Airport|        Žilina|   Zimbabwe| ИКУ|  \\N|        82.517778|            179.951|             14472|              13.0|   Z|            \\N|\n",
      "+-------+------------------+--------------+--------------+-----------+----+----+-----------------+-------------------+------------------+------------------+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe() can be used to glance over the data statics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "damaged-ready",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------+----------------+---+----+---------+----------+----+----+----+--------------------+\n",
      "|_c0|                 _c1|           _c2|             _c3|_c4| _c5|      _c6|       _c7| _c8| _c9|_c10|                _c11|\n",
      "+---+--------------------+--------------+----------------+---+----+---------+----------+----+----+----+--------------------+\n",
      "|  1|              Goroka|        Goroka|Papua New Guinea|GKA|AYGA|-6.081689|145.391881|5282|10.0|   U|Pacific/Port_Moresby|\n",
      "|  2|              Madang|        Madang|Papua New Guinea|MAG|AYMD|-5.207083|  145.7887|  20|10.0|   U|Pacific/Port_Moresby|\n",
      "|  3|         Mount Hagen|   Mount Hagen|Papua New Guinea|HGU|AYMH|-5.826789|144.295861|5388|10.0|   U|Pacific/Port_Moresby|\n",
      "|  4|              Nadzab|        Nadzab|Papua New Guinea|LAE|AYNZ|-6.569828|146.726242| 239|10.0|   U|Pacific/Port_Moresby|\n",
      "|  5|Port Moresby Jack...|  Port Moresby|Papua New Guinea|POM|AYPY|-9.443383| 147.22005| 146|10.0|   U|Pacific/Port_Moresby|\n",
      "|  6|          Wewak Intl|         Wewak|Papua New Guinea|WWK|AYWK|-3.583828|143.669186|  19|10.0|   U|Pacific/Port_Moresby|\n",
      "|  7|          Narsarsuaq|  Narssarssuaq|       Greenland|UAK|BGBW|61.160517|-45.425978| 112|-3.0|   E|     America/Godthab|\n",
      "|  8|                Nuuk|      Godthaab|       Greenland|GOH|BGGH|64.190922|-51.678064| 283|-3.0|   E|     America/Godthab|\n",
      "|  9|   Sondre Stromfjord|   Sondrestrom|       Greenland|SFJ|BGSF|67.016969|-50.689325| 165|-3.0|   E|     America/Godthab|\n",
      "| 10|      Thule Air Base|         Thule|       Greenland|THU|BGTL|76.531203|-68.703161| 251|-4.0|   E|       America/Thule|\n",
      "| 11|            Akureyri|      Akureyri|         Iceland|AEY|BIAR|65.659994|-18.072703|   6| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 12|         Egilsstadir|   Egilsstadir|         Iceland|EGS|BIEG|65.283333|-14.401389|  76| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 13|        Hornafjordur|          Hofn|         Iceland|HFN|BIHN|64.295556|-15.227222|  24| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 14|             Husavik|       Husavik|         Iceland|HZK|BIHU|65.952328|-17.425978|  48| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 15|          Isafjordur|    Isafjordur|         Iceland|IFJ|BIIS|66.058056|-23.135278|   8| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 16|Keflavik Internat...|      Keflavik|         Iceland|KEF|BIKF|   63.985|-22.605556| 171| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 17|      Patreksfjordur|Patreksfjordur|         Iceland|PFJ|BIPA|65.555833|   -23.965|  11| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 18|           Reykjavik|     Reykjavik|         Iceland|RKV|BIRK|    64.13|-21.940556|  48| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 19|        Siglufjordur|  Siglufjordur|         Iceland|SIJ|BISI|66.133333|-18.916667|  10| 0.0|   N|  Atlantic/Reykjavik|\n",
      "| 20|      Vestmannaeyjar|Vestmannaeyjar|         Iceland|VEY|BIVM|63.424303|-20.278875| 326| 0.0|   N|  Atlantic/Reykjavik|\n",
      "+---+--------------------+--------------+----------------+---+----+---------+----------+----+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-foundation",
   "metadata": {},
   "source": [
    "using the output from the previous 2 cells, build a schema and pass it at read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "regulated-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_schema = StructType([\n",
    "    # StructField(\"column_name\", columnType(), Nullable),\n",
    "    # edit this and add the columns\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "rapid-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(csv_path, schema=csv_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "whole-fever",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "++\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-consumption",
   "metadata": {},
   "source": [
    "## TEXT\n",
    "spark.read.text\n",
    "\n",
    "similar to spark.Context.textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tamil-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = '../data/word_count.text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "vulnerable-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "designed-filling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|Liverpool Footbal...|\n",
      "|                    |\n",
      "|Founded in 1892, ...|\n",
      "|                    |\n",
      "|One of the most w...|\n",
      "|                    |\n",
      "|The club's suppor...|\n",
      "|                    |\n",
      "|Liverpool F.C. wa...|\n",
      "|                    |\n",
      "|Liverpool played ...|\n",
      "|                    |\n",
      "|Liverpool reached...|\n",
      "|Statue of a man w...|\n",
      "|Statue of Bill Sh...|\n",
      "|                    |\n",
      "|The club was prom...|\n",
      "|                    |\n",
      "|Paisley retired i...|\n",
      "|3 burgundy tablet...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "opening-locking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method text in module pyspark.sql.readwriter:\n",
      "\n",
      "text(paths, wholetext=False, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      "    string column named \"value\", and followed by partitioned columns if there\n",
      "    are any.\n",
      "    The text files must be encoded as UTF-8.\n",
      "    \n",
      "    By default, each line in the text file is a new row in the resulting DataFrame.\n",
      "    \n",
      "    :param paths: string, or list of strings, for input path(s).\n",
      "    :param wholetext: if true, read each file from input path(s) as a single row.\n",
      "    :param lineSep: defines the line separator that should be used for parsing. If None is\n",
      "                    set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.\n",
      "    :param pathGlobFilter: an optional glob pattern to only include files with paths matching\n",
      "                           the pattern. The syntax follows `org.apache.hadoop.fs.GlobFilter`.\n",
      "                           It does not change the behavior of `partition discovery`_.\n",
      "    :param recursiveFileLookup: recursively scan a directory for files. Using this option\n",
      "                                disables `partition discovery`_.\n",
      "    \n",
      "    >>> df = spark.read.text('python/test_support/sql/text-test.txt')\n",
      "    >>> df.collect()\n",
      "    [Row(value='hello'), Row(value='this')]\n",
      "    >>> df = spark.read.text('python/test_support/sql/text-test.txt', wholetext=True)\n",
      "    >>> df.collect()\n",
      "    [Row(value='hello\\nthis')]\n",
      "    \n",
      "    .. versionadded:: 1.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-skiing",
   "metadata": {},
   "source": [
    "## JSON\n",
    "spark.read.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "israeli-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '../data/resource_hvrh-b6nb.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "harmful-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "boolean-grave",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- ratecodeid: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      " |-- vendorid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-expense",
   "metadata": {},
   "source": [
    "as long as they have a valid schema the json can be different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "raising-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonStrings  = ['{\"uploadTimeStamp\":\"1500618037189\",\"ID\":\"123ID\",\"data\":[{\"Data\":{\"unit\":\"rpm\",\"value\":\"0\"},\"EventID\":\"E1\",\"Timestamp\":1500618037189,\"pii\":{}},{\"Data\":{\"heading\":\"N\",\"loc1\":\"false\",\"loc2\":\"13.022425\",\"loc3\":\"77.760587\",\"loc4\":\"false\",\"speed\":\"10\"},\"EventID\":\"E2\",\"Timestamp\":1500618037189,\"pii\":{}},{\"Data\":{\"x\":\"1.1\",\"y\":\"1.2\",\"z\":\"2.2\"},\"EventID\":\"E3\",\"Timestamp\":1500618037189,\"pii\":{}},{\"EventID\":\"E4\",\"Data\":{\"value\":\"50\",\"unit\":\"percentage\"},\"Timestamp\":1500618037189},{\"Data\":{\"unit\":\"kmph\",\"value\":\"60\"},\"EventID\":\"E5\",\"Timestamp\":1500618037189,\"pii\":{}}]}',\n",
    " '{\"uploadTimeStamp\":\"1500618045735\",\"ID\":\"123ID\",\"data\":[{\"Data\":{\"unit\":\"rpm\",\"value\":\"0\"},\"EventID\":\"E1\",\"Timestamp\":1500618045735,\"pii\":{}},{\"Data\":{\"heading\":\"N\",\"loc1\":\"false\",\"loc2\":\"13.022425\",\"loc3\":\"77.760587\",\"loc4\":\"false\",\"speed\":\"10\"},\"EventID\":\"E2\",\"Timestamp\":1500618045735,\"pii\":{}},{\"Data\":{\"x\":\"1.1\",\"y\":\"1.2\",\"z\":\"2.2\"},\"EventID\":\"E3\",\"Timestamp\":1500618045735,\"pii\":{}},{\"EventID\":\"E4\",\"Data\":{\"value\":\"50\",\"unit\":\"percentage\"},\"Timestamp\":1500618045735},{\"Data\":{\"unit\":\"kmph\",\"value\":\"60\"},\"EventID\":\"E5\",\"Timestamp\":1500618045735,\"pii\":{}}]}',\n",
    " '{\"REGULAR_DUMMY\":\"REGULAR_DUMMY\", \"ID\":\"123ID\", \"uploadTimeStamp\":1500546893837}',\n",
    " '{\"REGULAR_DUMMY\":\"text_of_json_per_item_in_list\"}'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "given-provider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+---------------+\n",
      "|   ID|       REGULAR_DUMMY|                data|uploadTimeStamp|\n",
      "+-----+--------------------+--------------------+---------------+\n",
      "|123ID|                null|[[[,,,,,, rpm, 0,...|  1500618037189|\n",
      "|123ID|                null|[[[,,,,,, rpm, 0,...|  1500618045735|\n",
      "|123ID|       REGULAR_DUMMY|                null|  1500546893837|\n",
      "| null|text_of_json_per_...|                null|           null|\n",
      "+-----+--------------------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonRDD = spark.sparkContext.parallelize(jsonStrings)\n",
    "df = spark.read.json(jsonRDD)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "opening-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- REGULAR_DUMMY: string (nullable = true)\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Data: struct (nullable = true)\n",
      " |    |    |    |-- heading: string (nullable = true)\n",
      " |    |    |    |-- loc1: string (nullable = true)\n",
      " |    |    |    |-- loc2: string (nullable = true)\n",
      " |    |    |    |-- loc3: string (nullable = true)\n",
      " |    |    |    |-- loc4: string (nullable = true)\n",
      " |    |    |    |-- speed: string (nullable = true)\n",
      " |    |    |    |-- unit: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- x: string (nullable = true)\n",
      " |    |    |    |-- y: string (nullable = true)\n",
      " |    |    |    |-- z: string (nullable = true)\n",
      " |    |    |-- EventID: string (nullable = true)\n",
      " |    |    |-- Timestamp: long (nullable = true)\n",
      " |-- uploadTimeStamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the schema of the json is merged\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fuzzy-railway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with Spark 2.2 you can read a multiline json\n",
    "# ideally you want to receive the json on a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "valuable-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_json = '../data/multiline.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "technological-screw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json(m_json).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bored-spending",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).json(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).json(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-37dc55711b32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).json(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).json(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().;"
     ]
    }
   ],
   "source": [
    "spark.read.json(m_json).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "verbal-revolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json(m_json, multiLine=True).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "serious-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df['data'].isNotNull()).drop('REGULAR_DUMMY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efficient-purse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|data                                                                                                                                                                                                                                                       |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[[[,,,,,, rpm, 0,,,], E1, 1500618037189], [[N, false, 13.022425, 77.760587, false, 10,,,,,], E2, 1500618037189], [[,,,,,,,, 1.1, 1.2, 2.2], E3, 1500618037189], [[,,,,,, percentage, 50,,,], E4, 1500618037189], [[,,,,,, kmph, 60,,,], E5, 1500618037189]]|\n",
      "|[[[,,,,,, rpm, 0,,,], E1, 1500618045735], [[N, false, 13.022425, 77.760587, false, 10,,,,,], E2, 1500618045735], [[,,,,,,,, 1.1, 1.2, 2.2], E3, 1500618045735], [[,,,,,, percentage, 50,,,], E4, 1500618045735], [[,,,,,, kmph, 60,,,], E5, 1500618045735]]|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('data').show(20, False)  # why did I used False here?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "mechanical-photographer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Data: struct (nullable = true)\n",
      " |    |    |    |-- heading: string (nullable = true)\n",
      " |    |    |    |-- loc1: string (nullable = true)\n",
      " |    |    |    |-- loc2: string (nullable = true)\n",
      " |    |    |    |-- loc3: string (nullable = true)\n",
      " |    |    |    |-- loc4: string (nullable = true)\n",
      " |    |    |    |-- speed: string (nullable = true)\n",
      " |    |    |    |-- unit: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- x: string (nullable = true)\n",
      " |    |    |    |-- y: string (nullable = true)\n",
      " |    |    |    |-- z: string (nullable = true)\n",
      " |    |    |-- EventID: string (nullable = true)\n",
      " |    |    |-- Timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('data').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "empty-bargain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|speed    |\n",
      "+---------+\n",
      "|[, 10,,,]|\n",
      "|[, 10,,,]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('data.Data.speed').show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cognitive-dominant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- speed: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('data.Data.speed').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "arabic-panic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+\n",
      "|col                                                                    |\n",
      "+-----------------------------------------------------------------------+\n",
      "|[[[,,,,,, rpm, 0,,,], E1, 1500618037189]]                              |\n",
      "|[[[N, false, 13.022425, 77.760587, false, 10,,,,,], E2, 1500618037189]]|\n",
      "|[[[,,,,,,,, 1.1, 1.2, 2.2], E3, 1500618037189]]                        |\n",
      "|[[[,,,,,, percentage, 50,,,], E4, 1500618037189]]                      |\n",
      "|[[[,,,,,, kmph, 60,,,], E5, 1500618037189]]                            |\n",
      "|[[[,,,,,, rpm, 0,,,], E1, 1500618045735]]                              |\n",
      "|[[[N, false, 13.022425, 77.760587, false, 10,,,,,], E2, 1500618045735]]|\n",
      "|[[[,,,,,,,, 1.1, 1.2, 2.2], E3, 1500618045735]]                        |\n",
      "|[[[,,,,,, percentage, 50,,,], E4, 1500618045735]]                      |\n",
      "|[[[,,,,,, kmph, 60,,,], E5, 1500618045735]]                            |\n",
      "+-----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exploding nested jsons fields is a \"hard\" problem in spark\n",
    "from pyspark.sql.functions import explode, arrays_zip\n",
    "df.select(explode(arrays_zip('data'))).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "surface-coalition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col: struct (nullable = false)\n",
      " |    |-- data: struct (nullable = true)\n",
      " |    |    |-- Data: struct (nullable = true)\n",
      " |    |    |    |-- heading: string (nullable = true)\n",
      " |    |    |    |-- loc1: string (nullable = true)\n",
      " |    |    |    |-- loc2: string (nullable = true)\n",
      " |    |    |    |-- loc3: string (nullable = true)\n",
      " |    |    |    |-- loc4: string (nullable = true)\n",
      " |    |    |    |-- speed: string (nullable = true)\n",
      " |    |    |    |-- unit: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- x: string (nullable = true)\n",
      " |    |    |    |-- y: string (nullable = true)\n",
      " |    |    |    |-- z: string (nullable = true)\n",
      " |    |    |-- EventID: string (nullable = true)\n",
      " |    |    |-- Timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(explode(arrays_zip('data'))).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-forty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "educational-distance",
   "metadata": {},
   "source": [
    "## JDBC\n",
    "spark.read.jdbc\n",
    "\n",
    "depending on the number of partitions, the db will receive multiple connections. This might make the db unresponsive.\n",
    "\n",
    "used less in big projects\n",
    "\n",
    "the code below is just an example. read the following article for more details about jdbc reads\n",
    "https://github.com/awesome-spark/spark-gotchas/blob/master/05_spark_sql_and_dataset_api.md#reading-data-using-jdbc-source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "hairy-password",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o184.load.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:105)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:105)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:344)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:221)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-382e9acff900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjdbcDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jdbc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jdbc:postgresql:dbserver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dbtable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"schema.tablename\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"username\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o184.load.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:105)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:105)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:344)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:221)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "\n",
    "jdbcDF2 = spark.read \\\n",
    "    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "          properties={\"user\": \"username\", \"password\": \"password\"})\n",
    "\n",
    "# Specifying dataframe column data types on read\n",
    "jdbcDF3 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"customSchema\", \"id DECIMAL(38, 0), name STRING\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-sharing",
   "metadata": {},
   "source": [
    "## Parquet\n",
    "spark.read.parquet\n",
    "\n",
    "https://databricks.com/glossary/what-is-parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-pillow",
   "metadata": {},
   "source": [
    "read more about partition discovery\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-bleeding",
   "metadata": {},
   "source": [
    "## FORMAT & LOAD\n",
    "generic way of reading data from the above data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('jdbc').option().load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option().load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-product",
   "metadata": {},
   "source": [
    "usefull when developing frameworks (reading metadata and using generic ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-shepherd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "stunning-regard",
   "metadata": {},
   "source": [
    "## Write\n",
    "same as read, with additional options related to number of partitions.\n",
    "\n",
    "assuming df is the final dataframe, you can do something like in the cells below\n",
    "\n",
    "read the entire list of options at\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(output_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.jdbc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.format('parquet|jdbc|json').option().save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-playing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "lonely-venezuela",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "structural-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '../data/uk-postcode.csv'\n",
    "df = spark.read.csv(csv_file, header = True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "separated-algeria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "|Postcode|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|       Region|Postcodes|Active postcodes|Population|Households|\n",
      "+--------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "|     AB1| 57.1269| -2.13644| 391839|  804005|NJ918040|            Aberdeen|     Aberdeen|     2655|               0|      null|      null|\n",
      "|     AB2| 57.1713| -2.14152| 391541|  808948|NJ915089|            Aberdeen|     Aberdeen|     3070|               0|      null|      null|\n",
      "|     AB3| 57.0876| -2.59624| 363963|  799780|NO639997|            Aberdeen|     Aberdeen|     2168|               0|      null|      null|\n",
      "|     AB4| 57.5343| -2.12713| 392487|  849358|NJ924493|Fraserburgh, Pete...|     Aberdeen|     2956|               0|      null|      null|\n",
      "|     AB5| 57.4652| -2.64764| 361248|  841843|NJ612418|Buckie, Huntly, I...|     Aberdeen|     3002|               0|      null|      null|\n",
      "|     AB9| 57.1466|  -2.1142| 393189|  806196|NJ931061|            Aberdeen|     Aberdeen|     1066|               0|      null|      null|\n",
      "|    AB10| 57.1348| -2.11748| 392988|  804882|NJ929048|Aberdeen city cen...|     Aberdeen|      888|             675|     21964|     11517|\n",
      "|    AB11| 57.1371| -2.09341| 394445|  805136|NJ944051|Aberdeen city cen...|     Aberdeen|      889|             644|     21237|     10926|\n",
      "|    AB12| 57.1033| -2.11034| 393414|  801375|NJ934013|Aberdeen, Altens,...|     Aberdeen|      991|             782|     25414|     10688|\n",
      "|    AB13| 57.1127| -2.24469| 385279|  802443|NJ852024|          Milltimber|     Aberdeen|      100|              80|      2725|       947|\n",
      "|    AB14| 57.1033| -2.27251| 383590|  801402|NJ835014|Peterculter, Uppe...|     Aberdeen|      164|             140|      4881|      2162|\n",
      "|    AB15| 57.1388| -2.16551| 390082|  805334|NJ900053|Aberdeen, Bieldsi...|     Aberdeen|     1205|            1019|     35543|     15330|\n",
      "|    AB16| 57.1596| -2.15654| 390630|  807648|NJ906076|Aberdeen, Mastric...|     Aberdeen|      893|             776|     29238|     12874|\n",
      "|    AB21| 57.2091| -2.20174| 387912|  813165|NJ879131|Aberdeen, Blackbu...|     Aberdeen|      879|             732|     22181|      9650|\n",
      "|    AB22| 57.1864| -2.11913| 392898|  810627|NJ928106|Aberdeen, Bridge ...|     Aberdeen|      361|             300|     16311|      6978|\n",
      "|    AB23| 57.2088| -2.08971| 394680|  813118|NJ946131|Aberdeen, Balmedi...|     Aberdeen|      399|             311|     11143|      4517|\n",
      "|    AB24| 57.1634| -2.10828| 393550|  808065|NJ935080|Aberdeen, Old Abe...|     Aberdeen|      968|             808|     36343|     16935|\n",
      "|    AB25| 57.1534| -2.11422| 393189|  806953|NJ931069|Aberdeen city cen...|     Aberdeen|      610|             476|     18407|      9634|\n",
      "|    AB30|  56.846| -2.47726| 370986|  772829|NO709728|        Laurencekirk|Aberdeenshire|      349|             308|      7229|      2935|\n",
      "|    AB31| 57.0672| -2.50552| 369444|  797465|NO694974|            Banchory|Aberdeenshire|     1033|             629|     15319|      6096|\n",
      "+--------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "little-rapid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------+------------------+------------------+-----------------+--------+-----------------+--------+-----------------+-----------------+------------------+------------------+\n",
      "|summary|Postcode|Latitude          |Longitude         |Easting           |Northing         |GridRef |Town/Area        |Region  |Postcodes        |Active postcodes |Population        |Households        |\n",
      "+-------+--------+------------------+------------------+------------------+-----------------+--------+-----------------+--------+-----------------+-----------------+------------------+------------------+\n",
      "|count  |3107    |3094              |3094              |3082              |3082             |3082    |3107             |3106    |3086             |3086             |2814              |2814              |\n",
      "|mean   |null    |53.034849482870136|-2.051575161550915|399520.80012978584|351774.8997404283|null    |null             |null    |832.2216461438755|564.9565780946209|22437.184434968018|9390.271144278608 |\n",
      "|stddev |null    |1.8865014315147148|1.8334605907478179|121798.85778550198|209187.830957896 |null    |null             |null    |600.2495165657779|397.5467297411277|16578.512623860708|6814.9887522729805|\n",
      "|min    |AB1     |49.1995           |-7.82593          |22681             |8307             |HU390111|Abbey Hey, Gorton|Aberdeen|1                |0                |2                 |1                 |\n",
      "|max    |ZE3     |60.3156           |1.73337           |653560            |1159304          |TV604994|York City Centre |York    |3621             |2644             |153812            |61886             |\n",
      "+-------+--------+------------------+------------------+------------------+-----------------+--------+-----------------+--------+-----------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "entertaining-selling",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-49-8db1ddc8287c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-8db1ddc8287c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    +-------+---------+------------------+------------------+------------------+-----------------+--------+-----------------+--------+-----------------+-----------------+------------------+------------------+\u001b[0m\n\u001b[0m                                                                                                                                                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "+-------+---------+------------------+------------------+------------------+-----------------+--------+-----------------+--------+-----------------+-----------------+------------------+------------------+\n",
    "|summary|Post Code|Latitude          |Longitude         |Easting           |Northing         |GridRef |Town/Area        |Region  |Postcodes        |Active postcodes |Population        |Households        |\n",
    "+-------+---------+------------------+------------------+------------------+-----------------+--------+-----------------+--------+-----------------+-----------------+------------------+------------------+\n",
    "|count  |3107     |3094              |3094              |3082              |3082             |3082    |3107             |3106    |3086             |3086             |2814              |2814              |\n",
    "|mean   |null     |53.034849482870136|-2.051575161550915|399520.80012978584|351774.8997404283|null    |null             |null    |832.2216461438755|564.9565780946209|22437.184434968018|9390.271144278608 |\n",
    "|stddev |null     |1.8865014315147148|1.8334605907478179|121798.85778550198|209187.830957896 |null    |null             |null    |600.2495165657779|397.5467297411277|16578.512623860708|6814.9887522729805|\n",
    "|min    |AB1      |49.1995           |-7.82593          |22681             |8307             |HU390111|Abbey Hey, Gorton|Aberdeen|1                |0                |2                 |1                 |\n",
    "|max    |ZE3      |60.3156           |1.73337           |653560            |1159304          |TV604994|York City Centre |York    |3621             |2644             |153812            |61886             |\n",
    "+-------+---------+------------------+------------------+------------------+-----------------+--------+-----------------+--------+-----------------+-----------------+------------------+------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "filled-violin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|Postcode|Latitude|\n",
      "+--------+--------+\n",
      "|     AB1| 57.1269|\n",
      "|     AB2| 57.1713|\n",
      "|     AB3| 57.0876|\n",
      "|     AB4| 57.5343|\n",
      "|     AB5| 57.4652|\n",
      "|     AB9| 57.1466|\n",
      "|    AB10| 57.1348|\n",
      "|    AB11| 57.1371|\n",
      "|    AB12| 57.1033|\n",
      "|    AB13| 57.1127|\n",
      "|    AB14| 57.1033|\n",
      "|    AB15| 57.1388|\n",
      "|    AB16| 57.1596|\n",
      "|    AB21| 57.2091|\n",
      "|    AB22| 57.1864|\n",
      "|    AB23| 57.2088|\n",
      "|    AB24| 57.1634|\n",
      "|    AB25| 57.1534|\n",
      "|    AB30|  56.846|\n",
      "|    AB31| 57.0672|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select fields\n",
    "df.select('Postcode', 'Latitude').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "searching-filing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "|Post Code|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|       Region|Postcodes|Active postcodes|Population|Households|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "|      AB1| 57.1269| -2.13644| 391839|  804005|NJ918040|            Aberdeen|     Aberdeen|     2655|               0|      null|      null|\n",
      "|      AB2| 57.1713| -2.14152| 391541|  808948|NJ915089|            Aberdeen|     Aberdeen|     3070|               0|      null|      null|\n",
      "|      AB3| 57.0876| -2.59624| 363963|  799780|NO639997|            Aberdeen|     Aberdeen|     2168|               0|      null|      null|\n",
      "|      AB4| 57.5343| -2.12713| 392487|  849358|NJ924493|Fraserburgh, Pete...|     Aberdeen|     2956|               0|      null|      null|\n",
      "|      AB5| 57.4652| -2.64764| 361248|  841843|NJ612418|Buckie, Huntly, I...|     Aberdeen|     3002|               0|      null|      null|\n",
      "|      AB9| 57.1466|  -2.1142| 393189|  806196|NJ931061|            Aberdeen|     Aberdeen|     1066|               0|      null|      null|\n",
      "|     AB10| 57.1348| -2.11748| 392988|  804882|NJ929048|Aberdeen city cen...|     Aberdeen|      888|             675|     21964|     11517|\n",
      "|     AB11| 57.1371| -2.09341| 394445|  805136|NJ944051|Aberdeen city cen...|     Aberdeen|      889|             644|     21237|     10926|\n",
      "|     AB12| 57.1033| -2.11034| 393414|  801375|NJ934013|Aberdeen, Altens,...|     Aberdeen|      991|             782|     25414|     10688|\n",
      "|     AB13| 57.1127| -2.24469| 385279|  802443|NJ852024|          Milltimber|     Aberdeen|      100|              80|      2725|       947|\n",
      "|     AB14| 57.1033| -2.27251| 383590|  801402|NJ835014|Peterculter, Uppe...|     Aberdeen|      164|             140|      4881|      2162|\n",
      "|     AB15| 57.1388| -2.16551| 390082|  805334|NJ900053|Aberdeen, Bieldsi...|     Aberdeen|     1205|            1019|     35543|     15330|\n",
      "|     AB16| 57.1596| -2.15654| 390630|  807648|NJ906076|Aberdeen, Mastric...|     Aberdeen|      893|             776|     29238|     12874|\n",
      "|     AB21| 57.2091| -2.20174| 387912|  813165|NJ879131|Aberdeen, Blackbu...|     Aberdeen|      879|             732|     22181|      9650|\n",
      "|     AB22| 57.1864| -2.11913| 392898|  810627|NJ928106|Aberdeen, Bridge ...|     Aberdeen|      361|             300|     16311|      6978|\n",
      "|     AB23| 57.2088| -2.08971| 394680|  813118|NJ946131|Aberdeen, Balmedi...|     Aberdeen|      399|             311|     11143|      4517|\n",
      "|     AB24| 57.1634| -2.10828| 393550|  808065|NJ935080|Aberdeen, Old Abe...|     Aberdeen|      968|             808|     36343|     16935|\n",
      "|     AB25| 57.1534| -2.11422| 393189|  806953|NJ931069|Aberdeen city cen...|     Aberdeen|      610|             476|     18407|      9634|\n",
      "|     AB30|  56.846| -2.47726| 370986|  772829|NO709728|        Laurencekirk|Aberdeenshire|      349|             308|      7229|      2935|\n",
      "|     AB31| 57.0672| -2.50552| 369444|  797465|NO694974|            Banchory|Aberdeenshire|     1033|             629|     15319|      6096|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+-------------+---------+----------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename column\n",
    "df = df.withColumnRenamed('Postcode', 'Post Code')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eastern-miami",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Post Code',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Easting',\n",
       " 'Northing',\n",
       " 'GridRef',\n",
       " 'Town/Area',\n",
       " 'Region',\n",
       " 'Postcodes',\n",
       " 'Active postcodes',\n",
       " 'Population',\n",
       " 'Households']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema.fieldNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "raising-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "df = df.withColumn('type', when(col('Population') < 10000, 'village').when(df['Population'] < 20000, 'town').otherwise('city').alias('type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "tutorial-nicaragua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   type|\n",
      "+-------+\n",
      "|   city|\n",
      "|   town|\n",
      "|village|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('type').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "studied-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you write a condition like this, is easier to read it\n",
    "df = df.withColumn('schema',\n",
    "                   when(col('Population').isNull(), None)\\\n",
    "                   .when(col('Population') < 10000, 'village')\\\n",
    "                   .when(df['Population'] < 20000, 'town')\\\n",
    "                   .otherwise('city'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "prescription-still",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StructType' object has no attribute 'isNull'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-afa16b0018f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# why do we have an error here?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StructType' object has no attribute 'isNull'"
     ]
    }
   ],
   "source": [
    "df.filter(df.schema.isNull()).show(5)\n",
    "# why do we have an error here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "composed-france",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------+--------+--------+--------------------+--------+---------+----------------+----------+----------+----+------+\n",
      "|Post Code|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|  Region|Postcodes|Active postcodes|Population|Households|type|schema|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+--------+---------+----------------+----------+----------+----+------+\n",
      "|      AB1| 57.1269| -2.13644| 391839|  804005|NJ918040|            Aberdeen|Aberdeen|     2655|               0|      null|      null|city|  null|\n",
      "|      AB2| 57.1713| -2.14152| 391541|  808948|NJ915089|            Aberdeen|Aberdeen|     3070|               0|      null|      null|city|  null|\n",
      "|      AB3| 57.0876| -2.59624| 363963|  799780|NO639997|            Aberdeen|Aberdeen|     2168|               0|      null|      null|city|  null|\n",
      "|      AB4| 57.5343| -2.12713| 392487|  849358|NJ924493|Fraserburgh, Pete...|Aberdeen|     2956|               0|      null|      null|city|  null|\n",
      "|      AB5| 57.4652| -2.64764| 361248|  841843|NJ612418|Buckie, Huntly, I...|Aberdeen|     3002|               0|      null|      null|city|  null|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+--------+---------+----------------+----------+----------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['schema'].isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "+---------+--------+---------+-------+--------+--------+--------------------+--------+---------+----------------+----------+----------+----+\n",
    "|Post Code|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|  Region|Postcodes|Active postcodes|Population|Households|type|\n",
    "+---------+--------+---------+-------+--------+--------+--------------------+--------+---------+----------------+----------+----------+----+\n",
    "|      AB1| 57.1269| -2.13644| 391839|  804005|NJ918040|            Aberdeen|Aberdeen|     2655|               0|      null|      null|city|\n",
    "|      AB2| 57.1713| -2.14152| 391541|  808948|NJ915089|            Aberdeen|Aberdeen|     3070|               0|      null|      null|city|\n",
    "|      AB3| 57.0876| -2.59624| 363963|  799780|NO639997|            Aberdeen|Aberdeen|     2168|               0|      null|      null|city|\n",
    "|      AB4| 57.5343| -2.12713| 392487|  849358|NJ924493|Fraserburgh, Pete...|Aberdeen|     2956|               0|      null|      null|city|\n",
    "|      AB5| 57.4652| -2.64764| 361248|  841843|NJ612418|Buckie, Huntly, I...|Aberdeen|     3002|               0|      null|      null|city|\n",
    "+---------+--------+---------+-------+--------+--------+--------------------+--------+---------+----------------+----------+----------+----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ordinary-masters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------+--------+--------+--------------------+------+---------+----------------+----------+----------+----+------+\n",
      "|Post Code|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|Region|Postcodes|Active postcodes|Population|Households|type|schema|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+------+---------+----------------+----------+----------+----+------+\n",
      "|     CH28| 53.4005| -3.11196| 326165|  389873|SJ261898|Non-geographic, M...|Wirral|       37|               4|      null|      null|city|      |\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+------+---------+----------------+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace null\n",
    "df.na.fill('').filter(df['GridRef'] == 'SJ261898').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "worth-pricing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------+--------+--------+--------------------+------+---------+----------------+----------+----------+----+-----------+\n",
      "|Post Code|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|Region|Postcodes|Active postcodes|Population|Households|type|     schema|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+------+---------+----------------+----------+----------+----+-----------+\n",
      "|     CH28| 53.4005| -3.11196| 326165|  389873|SJ261898|Non-geographic, M...|Wirral|       37|               4|      null|      null|city|ThisWasNULL|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+------+---------+----------------+----------+----------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('ThisWasNULL').filter(df['GridRef'] == 'SJ261898').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "successful-decrease",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------+--------+-------+---------+------+---------+----------------+----------+----------+----+------+\n",
      "|Post Code|Latitude|Longitude|Easting|Northing|GridRef|Town/Area|Region|Postcodes|Active postcodes|Population|Households|type|schema|\n",
      "+---------+--------+---------+-------+--------+-------+---------+------+---------+----------------+----------+----------+----+------+\n",
      "+---------+--------+---------+-------+--------+-------+---------+------+---------+----------------+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().filter(df['GridRef'] == 'SJ261898').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "funky-presence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(schema='city'), Row(schema='village'), Row(schema=''), Row(schema='Town')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('schema').na.fill('').replace({'town': 'Town'}).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "surface-novel",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "col should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-bf414d5b7e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'extra_column'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'literal_value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m         \"\"\"\n\u001b[0;32m-> 2095\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2096\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: col should be Column"
     ]
    }
   ],
   "source": [
    "df.withColumn('extra_column', 'literal_value').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "genetic-finding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Post Code: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Easting: integer (nullable = true)\n",
      " |-- Northing: integer (nullable = true)\n",
      " |-- GridRef: string (nullable = true)\n",
      " |-- Town/Area: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Postcodes: integer (nullable = true)\n",
      " |-- Active postcodes: integer (nullable = true)\n",
      " |-- Population: integer (nullable = true)\n",
      " |-- Households: integer (nullable = true)\n",
      " |-- type: string (nullable = false)\n",
      " |-- schema: string (nullable = true)\n",
      " |-- extra_column: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.withColumn('extra_column', lit('literal_value')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "oriental-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "therapeutic-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = df.groupby('Region', 'Town/Area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "national-pakistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Region: string, Town/Area: string, count: bigint] 3107\n"
     ]
    }
   ],
   "source": [
    "print(ag.count(), df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "integral-seafood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              Region|count|\n",
      "+--------------------+-----+\n",
      "|           Worcester|   11|\n",
      "|           Charnwood|    2|\n",
      "|      North Kesteven|    2|\n",
      "|                Arun|    3|\n",
      "|       Epping Forest|    6|\n",
      "|             Waveney|    3|\n",
      "|              Stroud|    5|\n",
      "| Nuneaton & Bedworth|    3|\n",
      "|          New Forest|    8|\n",
      "|           Newmarket|    1|\n",
      "|              Maldon|    2|\n",
      "|           Sedgemoor|    1|\n",
      "|            Worthing|    6|\n",
      "|            Brighton|    1|\n",
      "|           Guildford|    8|\n",
      "|              Bolton|    9|\n",
      "|Central Bedfordshire|    8|\n",
      "|      North Tyneside|    7|\n",
      "|           Bradford |    1|\n",
      "|        Surrey Heath|    5|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Region').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "later-cartridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------------------+-------------------+------------+-------------+--------------+---------------------+---------------+---------------+\n",
      "|              Region|   type|     sum(Latitude)|     sum(Longitude)|sum(Easting)|sum(Northing)|sum(Postcodes)|sum(Active postcodes)|sum(Population)|sum(Households)|\n",
      "+--------------------+-------+------------------+-------------------+------------+-------------+--------------+---------------------+---------------+---------------+\n",
      "|          New Forest|   city|          254.1205|           -7.90659|     2147917|       514103|          6330|                 4719|         157569|          68905|\n",
      "|          North Down|   city|          163.8122|          -16.83903|      500282|      1587640|          2538|                 2538|          83041|          34521|\n",
      "|            Hereford|village|          156.4724| -8.624130000000001|     1020748|       754231|          1099|                  943|          19355|           8570|\n",
      "|   Llandrindod Wells|village|313.31329999999997|          -20.21476|     1839260|      1552555|          1033|                  917|          19566|           8772|\n",
      "|         Mole Valley|village|           51.2328|          -0.284652|      519858|       149577|           179|                  141|           4313|           1778|\n",
      "|           Hambleton|   town|           54.4691|           -1.16791|      454027|       508511|           612|                  527|          12832|           5821|\n",
      "|      South Ayrshire|   town|          221.7392|          -18.70586|      922980|      2475676|          2214|                 1796|          61814|          28978|\n",
      "|       Blaenau Gwent|   city|           155.291|            -9.5967|      952076|       623703|          2234|                 2019|          75512|          32885|\n",
      "|           Rotherham|   town|           53.4663|           -1.34525|      443564|       396817|           728|                  602|          19772|           8539|\n",
      "|Richmond upon Thames|   town|          154.3924|-0.8182590000000001|     1560248|       525964|          2070|                 1280|          52176|          22252|\n",
      "|            Somerset|village| 715.3104999999999|          -40.54702|     4722758|      1863500|          2890|                 2403|          58212|          25113|\n",
      "|            Strabane|village|           54.6961|           -7.61162|       38605|       547918|           305|                  305|           7691|           2836|\n",
      "|               Leeds|village|          322.9336|           -9.20168|     2584773|      2617950|          3663|                 1804|          45159|          17165|\n",
      "|  Telford and Wrekin|   city|158.06709999999998|-7.3881499999999996|     1106463|       930750|          4018|                 2763|          99808|          39795|\n",
      "|           Blackburn|   city|           53.7535|           -2.46423|      369487|       428670|             1|                    0|           null|           null|\n",
      "|         Bournemouth|   city|202.97639999999998|            -7.4313|     1640513|       375370|          3074|                 2365|         102019|          42792|\n",
      "|              Antrim|   city|           54.7322|            -6.2574|      126023|       545789|           981|                  981|          40205|          15409|\n",
      "|            Tendring|   town|103.78049999999999|            2.31362|     1234659|       452026|          1133|                  909|          25258|          11364|\n",
      "|      East Hampshire|   town|102.00569999999999|          -1.869828|      949660|       246293|          1288|                  961|          23140|           9739|\n",
      "|              Rother|   town|203.67659999999998|           1.915533|     2297339|       464945|          3131|                 2241|          59782|          26656|\n",
      "+--------------------+-------+------------------+-------------------+------------+-------------+--------------+---------------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Region', 'type').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "indonesian-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = df.groupby('Region', 'type').agg({'Region': 'count'}).withColumnRenamed('count(Region)', 'asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "continuous-syntax",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---+\n",
      "|              Region|   type|asd|\n",
      "+--------------------+-------+---+\n",
      "|          New Forest|   city|  5|\n",
      "|          North Down|   city|  3|\n",
      "|            Hereford|village|  3|\n",
      "|   Llandrindod Wells|village|  6|\n",
      "|         Mole Valley|village|  1|\n",
      "|           Hambleton|   town|  1|\n",
      "|      South Ayrshire|   town|  4|\n",
      "|       Blaenau Gwent|   city|  3|\n",
      "|           Rotherham|   town|  1|\n",
      "|Richmond upon Thames|   town|  3|\n",
      "|            Somerset|village| 14|\n",
      "|            Strabane|village|  1|\n",
      "|               Leeds|village|  6|\n",
      "|  Telford and Wrekin|   city|  3|\n",
      "|           Blackburn|   city|  1|\n",
      "|         Bournemouth|   city|  4|\n",
      "|              Antrim|   city|  1|\n",
      "|            Tendring|   town|  2|\n",
      "|      East Hampshire|   town|  2|\n",
      "|              Rother|   town|  4|\n",
      "+--------------------+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ag.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "quarterly-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as pf\n",
    "ag = df.groupby('Region', 'type').agg(pf.sum('Population').alias('sum_population'), pf.count('PostCodes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "christian-milton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------+----------------+\n",
      "|              Region|   type|sum_population|count(PostCodes)|\n",
      "+--------------------+-------+--------------+----------------+\n",
      "|          New Forest|   city|        157569|               5|\n",
      "|          North Down|   city|         83041|               3|\n",
      "|            Hereford|village|         19355|               3|\n",
      "|   Llandrindod Wells|village|         19566|               6|\n",
      "|         Mole Valley|village|          4313|               1|\n",
      "|           Hambleton|   town|         12832|               1|\n",
      "|      South Ayrshire|   town|         61814|               4|\n",
      "|       Blaenau Gwent|   city|         75512|               3|\n",
      "|           Rotherham|   town|         19772|               1|\n",
      "|Richmond upon Thames|   town|         52176|               3|\n",
      "|            Somerset|village|         58212|              14|\n",
      "|            Strabane|village|          7691|               1|\n",
      "|               Leeds|village|         45159|               6|\n",
      "|  Telford and Wrekin|   city|         99808|               3|\n",
      "|           Blackburn|   city|          null|               1|\n",
      "|         Bournemouth|   city|        102019|               4|\n",
      "|              Antrim|   city|         40205|               1|\n",
      "|            Tendring|   town|         25258|               2|\n",
      "|      East Hampshire|   town|         23140|               2|\n",
      "|              Rother|   town|         59782|               4|\n",
      "+--------------------+-------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ag.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-helicopter",
   "metadata": {},
   "source": [
    "## Partitions\n",
    "\n",
    "Choose the right partition column. Think about how the cardinality of that column affects how the data gets distributed.\n",
    "\n",
    "When in doubt hash is better (safer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "confirmed-civilization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "frequent-currency",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`col1`' given input columns: [Active postcodes, Easting, GridRef, Households, Latitude, Longitude, Northing, Population, Post Code, Postcodes, Region, Town/Area, schema, type];;\n'RepartitionByExpression ['col1, 'col2], 200\n+- Repartition 10, true\n   +- Project [Post Code#1866, Latitude#1026, Longitude#1027, Easting#1028, Northing#1029, GridRef#1030, Town/Area#1031, Region#1032, Postcodes#1033, Active postcodes#1034, Population#1035, Households#1036, type#1941, CASE WHEN isnull(Population#1035) THEN cast(null as string) WHEN (Population#1035 < 10000) THEN village WHEN (Population#1035 < 20000) THEN town ELSE city END AS schema#1962]\n      +- Project [Post Code#1866, Latitude#1026, Longitude#1027, Easting#1028, Northing#1029, GridRef#1030, Town/Area#1031, Region#1032, Postcodes#1033, Active postcodes#1034, Population#1035, Households#1036, CASE WHEN (Population#1035 < 10000) THEN village WHEN (Population#1035 < 20000) THEN town ELSE city END AS type#1941]\n         +- Project [Postcode#1025 AS Post Code#1866, Latitude#1026, Longitude#1027, Easting#1028, Northing#1029, GridRef#1030, Town/Area#1031, Region#1032, Postcodes#1033, Active postcodes#1034, Population#1035, Households#1036]\n            +- Relation[Postcode#1025,Latitude#1026,Longitude#1027,Easting#1028,Northing#1029,GridRef#1030,Town/Area#1031,Region#1032,Postcodes#1033,Active postcodes#1034,Population#1035,Households#1036] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-51f246decc9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# repartition by columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'col1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'col2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# repartition by hash and cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mrepartition\u001b[0;34m(self, numPartitions, *cols)\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbasestring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numPartitions should be an int or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/spark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`col1`' given input columns: [Active postcodes, Easting, GridRef, Households, Latitude, Longitude, Northing, Population, Post Code, Postcodes, Region, Town/Area, schema, type];;\n'RepartitionByExpression ['col1, 'col2], 200\n+- Repartition 10, true\n   +- Project [Post Code#1866, Latitude#1026, Longitude#1027, Easting#1028, Northing#1029, GridRef#1030, Town/Area#1031, Region#1032, Postcodes#1033, Active postcodes#1034, Population#1035, Households#1036, type#1941, CASE WHEN isnull(Population#1035) THEN cast(null as string) WHEN (Population#1035 < 10000) THEN village WHEN (Population#1035 < 20000) THEN town ELSE city END AS schema#1962]\n      +- Project [Post Code#1866, Latitude#1026, Longitude#1027, Easting#1028, Northing#1029, GridRef#1030, Town/Area#1031, Region#1032, Postcodes#1033, Active postcodes#1034, Population#1035, Households#1036, CASE WHEN (Population#1035 < 10000) THEN village WHEN (Population#1035 < 20000) THEN town ELSE city END AS type#1941]\n         +- Project [Postcode#1025 AS Post Code#1866, Latitude#1026, Longitude#1027, Easting#1028, Northing#1029, GridRef#1030, Town/Area#1031, Region#1032, Postcodes#1033, Active postcodes#1034, Population#1035, Households#1036]\n            +- Relation[Postcode#1025,Latitude#1026,Longitude#1027,Easting#1028,Northing#1029,GridRef#1030,Town/Area#1031,Region#1032,Postcodes#1033,Active postcodes#1034,Population#1035,Households#1036] csv\n"
     ]
    }
   ],
   "source": [
    "# you can use repartition to redistribuite data\n",
    "# triggers a shuffle\n",
    "\n",
    "# repartition by hash\n",
    "df = df.repartition(10)\n",
    "\n",
    "# repartition by columns\n",
    "df = df.repartition('col1','col2')\n",
    "\n",
    "# repartition by hash and cols\n",
    "df = df.repartition(10, 'col1', 'col2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "royal-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use coalesce to reduce the number of partitions\n",
    "# assuming 10 partitions and 5 workers\n",
    "df = df.coalesce(5)\n",
    "# will reduce the number of partions without triggering a shuffle\n",
    "# df.coalesce(20) will not do anything because 20 > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ultimate-mailman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('Region').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dried-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_200 = df.repartition('Region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "virtual-messaging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_200.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "altered-walnut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------+--------+--------+--------------------+--------------+---------+----------------+----------+----------+-------+-------+\n",
      "|Post Code|Latitude|Longitude|Easting|Northing| GridRef|           Town/Area|        Region|Postcodes|Active postcodes|Population|Households|   type| schema|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+--------------+---------+----------------+----------+----------+-------+-------+\n",
      "|      WR1| 52.1974| -2.21749| 385231|  255485|SO852554|           Worcester|     Worcester|      835|             431|      9231|      4850|village|village|\n",
      "|      WR2| 52.1916| -2.23948| 383726|  254845|SO837548|           Worcester|     Worcester|      902|             711|     29799|     12092|   city|   city|\n",
      "|      WR3| 52.2159| -2.21017| 385738|  257541|SO857575|           Worcester|     Worcester|      677|             515|     20397|      8738|   city|   city|\n",
      "|      WR4| 52.2049| -2.18969| 387134|  256314|SO871563|           Worcester|     Worcester|      814|             610|     26127|     10881|   city|   city|\n",
      "|      WR5| 52.1803| -2.20008| 386416|  253580|SO864535|           Worcester|     Worcester|      974|             753|     30933|     12785|   city|   city|\n",
      "|      WR6| 52.2293| -2.36104| 375437|  259072|SO754590|           Worcester|     Worcester|      614|             504|     12216|      5056|   town|   town|\n",
      "|      WR7| 52.1975| -2.06376| 395739|  255475|SO957554|           Worcester|     Worcester|      266|             212|      5348|      2131|village|village|\n",
      "|      WR8| 52.0907| -2.20143| 386296|  243613|SO862436|           Worcester|     Worcester|      421|             326|      8093|      3602|village|village|\n",
      "|     WR15| 52.3061| -2.57246| 361065|  267708|SO610677|       Tenbury Wells|     Worcester|      312|             250|      7196|      3110|village|village|\n",
      "|     WR78| 52.2018| -2.21146| 385645|  255973|SO856559|           Worcester|     Worcester|       15|               0|      null|      null|   city|   null|\n",
      "|     WR99| 52.1974| -2.21995| 385063|  255485|SO850554|           Worcester|     Worcester|       28|               5|      null|      null|   city|   null|\n",
      "|     LE11| 52.7707| -1.21979| 452735|  319519|SK527195|Loughborough, Cha...|     Charnwood|     2027|            1183|     60318|     22336|   city|   city|\n",
      "|     LE12| 52.7681| -1.19231| 454593|  319250|SK545192|East Leake, West ...|     Charnwood|     1689|            1439|     61909|     25869|   city|   city|\n",
      "|      LN4|  53.153|-0.390125| 507754|  362973|TF077629|Branston, Canwick...|North Kesteven|     1305|            1064|     38539|     16126|   city|   city|\n",
      "|     NG34| 52.9876|-0.395382| 507814|  344567|TF078445|            Sleaford|North Kesteven|     1477|            1288|     40829|     17659|   city|   city|\n",
      "|     BN16| 50.8154|-0.497802| 505923|  102836|TQ059028|Angmering, East P...|          Arun|     1267|             949|     28131|     13097|   city|   city|\n",
      "|     BN17| 50.8149| -0.53709| 503157|  102725|TQ031027|Littlehampton, Cl...|          Arun|      891|             713|     28542|     12514|   city|   city|\n",
      "|     BN18| 50.8488|-0.578428| 500172|  106438|TQ001064|Arundel, Amberley...|          Arun|      652|             477|     13232|      5566|   town|   town|\n",
      "|      CM5| 51.7184| 0.247565| 555355|  204576|TL553045|Chipping Ongar, H...| Epping Forest|      584|             439|     10544|      4298|   town|   town|\n",
      "|     CM16| 51.7002| 0.118833| 546522|  202286|TL465022|Epping, Theydon Bois| Epping Forest|      954|             641|     22217|      9398|   city|   city|\n",
      "+---------+--------+---------+-------+--------+--------+--------------------+--------------+---------+----------------+----------+----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_200.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "auburn-hometown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_200.coalesce(427).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "allied-devon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_200.coalesce(100).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "posted-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you do coalesce(1) only one worker will do the work.\n",
    "# if you have \"unexecuted\" transformations repartition(1) is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "original-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"save\" the dataframe in memory or disk to reusse it\n",
    "df = df.cache()  # <== very important to \"store\" the result in a new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-austria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "tender-uzbekistan",
   "metadata": {},
   "source": [
    "## UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-romantic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "graphic-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "adapted-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_custom_fct(x, y):\n",
    "    if condition:\n",
    "        return int\n",
    "    else\n",
    "        return str\n",
    "    return x+y\n",
    "\n",
    "udf_my_custom_fct = udf(my_custom_fct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "willing-architecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+-------+--------+--------+--------------------+--------------------+---------+----------------+----------+----------+-------+-------+\n",
      "|Post Code|Latitude| Longitude|Easting|Northing| GridRef|           Town/Area|              Region|Postcodes|Active postcodes|Population|Households|   type| schema|\n",
      "+---------+--------+----------+-------+--------+--------+--------------------+--------------------+---------+----------------+----------+----------+-------+-------+\n",
      "|     TA18| 50.8829|  -2.78813| 344653|  109563|ST446095|           Crewkerne|            Somerset|      505|             399|     10136|      4565|   town|   town|\n",
      "|     IV17| 57.6988|  -4.25616| 265632|  869902|NH656699|              Alness|            Highland|      182|             170|      6444|      2710|village|village|\n",
      "|      EN8| 51.6964|-0.0332062| 536027|  201570|TL360015|Waltham Cross, Ch...|             Enfield|     1414|             911|     35807|     14936|   city|   city|\n",
      "|     CA28| 54.5437|  -3.57948| 297919|  517641|NX979176|Whitehaven, Sandwith|            Copeland|     1142|             943|     28849|     12447|   city|   city|\n",
      "|     SE22| 51.4546|-0.0725509| 534017|  174608|TQ340746|East Dulwich, Pec...|           Southwark|      916|             516|     29687|     12633|   city|   city|\n",
      "|      EN7| 51.7082|-0.0644782| 533832|  202824|TL338028|            Cheshunt|          Broxbourne|      603|             480|     21884|      8256|   city|   city|\n",
      "|     IP18| 52.3304|   1.66881| 650069|  276595|TM500765|Southwold, Easton...|             Waveney|      321|             287|      4055|      2047|village|village|\n",
      "|     LE21| 52.6315|  -1.12809| 459109|  304106|SK591041|           Leicester|           Leicester|      224|              27|      null|      null|   city|   null|\n",
      "|     EH26| 55.8329|  -3.22401| 323430|  660640|NT234606|            Penicuik|          Midlothian|      593|             525|     17022|      7074|   town|   town|\n",
      "|     PH26| 57.3299|  -3.60628| 303388|  827737|NJ033277|Grantown-on-Spey,...|            Highland|      154|             134|      3764|      1639|village|village|\n",
      "|     GU28| 50.9912| -0.628665| 496342|  122206|SU963222|Petworth, Graffha...|          Chichester|      454|             354|      6781|      2862|village|village|\n",
      "|     SO50| 50.9708|  -1.34524| 446071|  119246|SU460192|Eastleigh Town Ce...|           Eastleigh|     1635|            1245|     46444|     18995|   city|   city|\n",
      "|     CO12| 51.9322|   1.25555| 623907|  230973|TM239309|             Harwich|            Tendring|      769|             613|     22251|      9751|   city|   city|\n",
      "|      DT5| 50.5491|  -2.44273| 368729|   72242|SY687722|            Portland|Weymouth and Port...|      385|             302|     12844|      5175|   town|   town|\n",
      "|     BS38|    null|      null|   null|    null|    null|      Non-geographic|             Bristol|        5|               0|      null|      null|   city|   null|\n",
      "|     BT78| 54.5726|  -7.37309|  52902|  532991|NV529329|Omagh, Dromore, D...|               Omagh|     1208|            1208|     28598|     10407|   city|   city|\n",
      "|     BD15| 53.8081|  -1.83514| 410954|  434658|SE109346|Allerton, Norr, W...|            Bradford|      554|             422|     15713|      6272|   town|   town|\n",
      "|     NE11| 54.9401|  -1.63172| 423692|  560666|NZ236606|Dunston, Metro Ce...|           Gateshead|      705|             482|     16457|      7162|   town|   town|\n",
      "|      TA7| 51.1311|  -2.91524| 336054|  137267|ST360372|Puriton, Polden H...|            Somerset|      657|             583|     15166|      6318|   town|   town|\n",
      "|      DN8| 53.6144| -0.956363| 469139|  413600|SE691136|              Thorne|           Doncaster|      556|             476|     17578|      7387|   town|   town|\n",
      "+---------+--------+----------+-------+--------+--------+--------------------+--------------------+---------+----------------+----------+----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('calculated_value', udf_my_custom_fct(df['Population']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
