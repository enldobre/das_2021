{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrames\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "valuable-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "combined-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as psf\n",
    "from pyspark.sql.functions import *\n",
    "from collections import defaultdict\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-blank",
   "metadata": {},
   "source": [
    "# Graph like logic in spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "emotional-whale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  u|  p|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  1|\n",
      "|  4|  4|\n",
      "|  5|  1|\n",
      "|  6|  6|\n",
      "|  7|  7|\n",
      "|  8|  8|\n",
      "|  9|  3|\n",
      "|  9|  6|\n",
      "| 10|  1|\n",
      "| 11| 11|\n",
      "| 12|  9|\n",
      "| 13| 13|\n",
      "| 14| 14|\n",
      "| 15|  9|\n",
      "| 16| 16|\n",
      "| 17| 17|\n",
      "| 18| 15|\n",
      "| 18| 16|\n",
      "| 19| 15|\n",
      "| 19| 13|\n",
      "| 20| 20|\n",
      "| 51| 51|\n",
      "| 55| 51|\n",
      "| 57| 57|\n",
      "| 59| 57|\n",
      "| 61| 55|\n",
      "| 61| 59|\n",
      "+---+---+\n",
      "\n",
      "root\n",
      " |-- u2: long (nullable = true)\n",
      " |-- p2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#validate\n",
    "l = [(1,1),\n",
    "    (2,2),\n",
    "    (3,1),\n",
    "    (4,4),\n",
    "    (5,1),\n",
    "    (6,6),\n",
    "    (7,7),\n",
    "    (8,8),\n",
    "    (9,3),\n",
    "    (9,6),\n",
    "    (10,1),\n",
    "    (11,11),\n",
    "    (12,9),\n",
    "    (13,13),\n",
    "    (14,14),\n",
    "    (15,9),\n",
    "    (16,16),\n",
    "    (17,17),\n",
    "    (18,15),\n",
    "    (18,16),\n",
    "    (19,15),\n",
    "    (19,13),\n",
    "    (20,20),\n",
    "    (51,51),\n",
    "    (55,51),\n",
    "    (57,57),\n",
    "    (59,57),\n",
    "    (61,55),\n",
    "    (61,59)]\n",
    "rdd = sc.parallelize(l)\n",
    "ucid_mpp_acc = rdd.map(lambda x: Row(u=x[0],p=x[1]))\n",
    "np = spark.createDataFrame(ucid_mpp_acc)\n",
    "\n",
    "np.show(50)\n",
    "\n",
    "np2 = np.withColumnRenamed('p','p2').withColumnRenamed('u','u2')\n",
    "np2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "loop = 1\n",
    "debug = 0\n",
    "happy = 0\n",
    "happy_mp = 1\n",
    "happy_gp = 1\n",
    "infinit_loop_exit = 100\n",
    "df_dict = defaultdict(list)\n",
    "df_dict[1].append(np)\n",
    "df_dict[1].append(np2)\n",
    "##print('Start time:', str(datetime.datetime.now()))\n",
    "while not happy: \n",
    "    print('doing the {0} check'.format(loop))\n",
    "    print('checking for multiple parents.')\n",
    "    #mp = df_dict[loop][0].join(df_dict[loop][0].groupBy(\"u\").agg(countDistinct(\"p\").alias('count_{0}'.format(loop))), 'u')\\\n",
    "    #            .filter('count_{0} > 1'.format(loop))        \n",
    "    mp = df_dict[loop][0].join(df_dict[loop][0].groupBy(\"u\").agg(countDistinct(\"p\").alias('count')), 'u')\\\n",
    "               .filter('count > 1')\n",
    "    #mp.show()    \n",
    "    print('rows with multiple parents issues:',mp.count(), 'check_{0}'.format(loop))\n",
    "    if mp.count() > 0:\n",
    "        print('adding the parents rel.')\n",
    "        happy_mp = 0\n",
    "        wd = Window.partitionBy('w').orderBy('u')\n",
    "        mp_rel = mp.withColumnRenamed('u','w').withColumnRenamed('p','u').withColumn('p', psf.min('u').over(wd))\n",
    "        new_rel = mp_rel.filter('u != p').select('u', 'p').distinct()\n",
    "        if debug == 1:\n",
    "            new_rel.show()\n",
    "        np_a = df_dict[loop][0].select('u', 'p').union(new_rel).distinct()\n",
    "        if debug == 1:\n",
    "            #np.sort('u').show(50)\n",
    "            np_a.show(50)\n",
    "        np = np_a.join(np_a.groupBy(\"u\").agg(countDistinct(\"p\").alias('count_{0}'.format(loop))), 'u')\\\n",
    "                               .filter('(count_{0} > 1 and u!=p) or count_{0} = 1'.format(loop))\n",
    "        if debug == 1:\n",
    "            # np.sort('u').show()\n",
    "            np.show(50)\n",
    "        df_dict[loop+1].append(np)\n",
    "        #generate np2 in case we down't go into grandpa issues\n",
    "        df_dict[loop+1].append(np.withColumnRenamed('p','p2').withColumnRenamed('u','u2'))\n",
    "        #overwrite np and np2 for loop in case we go into grandpa issues\n",
    "        df_dict[loop][0] = np\n",
    "        df_dict[loop][1] = np.withColumnRenamed('p','p2').withColumnRenamed('u','u2')\n",
    "        \n",
    "        \n",
    "        # cleanup\n",
    "        mp_rel.unpersist()\n",
    "        new_rel.unpersist()\n",
    "        np_a.unpersist()\n",
    "        \n",
    "    else:\n",
    "        happy_mp = 1\n",
    "\n",
    "\n",
    "   \n",
    "    print('checking for grandparents.')\n",
    "    chk = df_dict[loop][0].join(df_dict[loop][1], df_dict[loop][0].p==df_dict[loop][1].u2)\\\n",
    "            .filter(df_dict[loop][0].p!=df_dict[loop][1].p2)\n",
    "    print('rows with grandpa issues:',chk.count())\n",
    "    \n",
    "    # new parent df\n",
    "    if chk.count() > 0 :\n",
    "        print('adding the grandparents.')\n",
    "        happy_gp = 0\n",
    "        np_temp = df_dict[loop][0].join(df_dict[loop][1], df_dict[loop][0].p==df_dict[loop][1].u2)\\\n",
    "                    .select(df_dict[loop][0].u,df_dict[loop][1].p2)\\\n",
    "                    .withColumnRenamed('u','u').withColumnRenamed('p2','p').distinct()\n",
    "        np = np_temp\n",
    "        if debug:\n",
    "            print('gp resulting np:')\n",
    "            np.sort('u', 'p').show(60)\n",
    "        np2 = np.withColumnRenamed('p','p2').withColumnRenamed('u','u2')\n",
    "        if df_dict[loop+1][0]:\n",
    "            df_dict[loop+1][0] = np\n",
    "            df_dict[loop+1][1] = np2\n",
    "        else:\n",
    "            df_dict[loop+1].append(np)\n",
    "            df_dict[loop+1].append(np2)\n",
    "            \n",
    "        # cleanup\n",
    "        np_temp.unpersist()\n",
    "        \n",
    "    else:\n",
    "        happy_gp = 1\n",
    "    \n",
    "    print('Loop {} happy_gp:'.format(loop+1),happy_gp)\n",
    "    print('Loop {} happy_mp:'.format(loop+1), happy_mp)\n",
    "    \n",
    "    if happy_gp and happy_mp:\n",
    "        happy = 1\n",
    "    \n",
    "    if not happy:\n",
    "        df_dict[loop+1][0] = df_dict[loop+1][0].cache()\n",
    "        df_dict[loop+1][1] = df_dict[loop+1][1].cache()\n",
    "        df_dict[loop][0].unpersist()\n",
    "        df_dict[loop][1].unpersist()\n",
    "    \n",
    "    \n",
    "    if infinit_loop_exit < loop:\n",
    "        happy = 1\n",
    "    \n",
    "    loop = loop + 1\n",
    "    \n",
    "print('End time:', str(datetime.datetime.now()))\n",
    "\n",
    "print('The dataframe with the right Parent - - Child relationship.')\n",
    "df_dict[loop-1][0].sort('u').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort('u').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-baghdad",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-combine",
   "metadata": {},
   "source": [
    "#### the next cell needs to be executed with a fresh kernel in order for the library to be loaded in spark\n",
    "if we were to use spark-skell we would add --packages graphframes:graphframes:0.8.1-spark3.0-s_2.12\n",
    "\n",
    "multiple spark packages can be found at https://spark-packages.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "solved-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Graph\")\\\n",
    "    .config('spark.jars.packages', 'graphframes:graphframes:0.8.1-spark3.0-s_2.12')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "empty-blogger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.17.94.87:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Graph</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc0f8864c70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unavailable-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "capable-swedish",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|inDegree|\n",
      "+---+--------+\n",
      "|  c|       1|\n",
      "|  b|       2|\n",
      "+---+--------+\n",
      "\n",
      "+---+------------------+\n",
      "| id|          pagerank|\n",
      "+---+------------------+\n",
      "|  b|1.0905890109440908|\n",
      "|  a|              0.01|\n",
      "|  c|1.8994109890559092|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a Vertex DataFrame with unique ID column \"id\"\n",
    "v = sqlContext.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 30),\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "# Create an Edge DataFrame with \"src\" and \"dst\" columns\n",
    "e = sqlContext.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "# Create a GraphFrame\n",
    "from graphframes import *\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# Query: Get in-degree of each vertex.\n",
    "g.inDegrees.show()\n",
    "\n",
    "# Query: Count the number of \"follow\" connections in the graph.\n",
    "g.edges.filter(\"relationship = 'follow'\").count()\n",
    "\n",
    "# Run PageRank algorithm, and show results.\n",
    "results = g.pageRank(resetProbability=0.01, maxIter=20)\n",
    "results.vertices.select(\"id\", \"pagerank\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-journalist",
   "metadata": {},
   "source": [
    "#### lets try to implement the same logic as in the unique id example from the powerpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "colored-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of nodes/vertexes\n",
    "v = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"A\"),\n",
    "        (2, \"B\"),\n",
    "        (3, \"C\"),\n",
    "        (4, \"D\"),\n",
    "        (5, \"E\"),\n",
    "        (6, \"F\"),\n",
    "        (7, \"G\"),\n",
    "        (8, \"H\"),\n",
    "        (9, \"I\"),\n",
    "        (10,\"J\"),\n",
    "        (11,\"K\")\n",
    "        \n",
    "    ],\n",
    "        ['id', 'name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dramatic-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and a list of relationships/edges\n",
    "e = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 1, \"A\"),\n",
    "        (2, 2, \"B\"),\n",
    "        (3, 3, \"C\"),\n",
    "        (4, 4, \"D\"),\n",
    "        (5, 5, \"E\"),\n",
    "        (6, 2, \"F\"),\n",
    "        (6, 3, \"F\"),\n",
    "        (7, 4, \"G\"),\n",
    "        (7, 5, \"G\"),\n",
    "        (8, 6, \"H\"),\n",
    "        (9, 7, \"I\"),\n",
    "        (9, 8, \"I\"),\n",
    "        (10,10, \"J\"),\n",
    "        (11,10, \"K\")\n",
    "        \n",
    "    ],\n",
    "        ['src', 'dst', \"source\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "structural-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GraphFrame(v, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baking-savannah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|    src|       edge|    dst|\n",
      "+-------+-----------+-------+\n",
      "| [1, A]|  [1, 1, A]| [1, A]|\n",
      "| [2, B]|  [2, 2, B]| [2, B]|\n",
      "| [3, C]|  [3, 3, C]| [3, C]|\n",
      "| [4, D]|  [4, 4, D]| [4, D]|\n",
      "| [5, E]|  [5, 5, E]| [5, E]|\n",
      "| [6, F]|  [6, 3, F]| [3, C]|\n",
      "| [6, F]|  [6, 2, F]| [2, B]|\n",
      "| [7, G]|  [7, 5, G]| [5, E]|\n",
      "| [7, G]|  [7, 4, G]| [4, D]|\n",
      "| [8, H]|  [8, 6, H]| [6, F]|\n",
      "| [9, I]|  [9, 7, I]| [7, G]|\n",
      "| [9, I]|  [9, 8, I]| [8, H]|\n",
      "|[10, J]|[10, 10, J]|[10, J]|\n",
      "|[11, K]|[11, 10, K]|[10, J]|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can use triplets to show the graph as a DF\n",
    "# sorting by src we can see that 6,7,9 have multiple parents\n",
    "g.triplets.sort('src').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "middle-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph require a checkpoint directory\n",
    "# a tmp folder which is used for dumping temporary data\n",
    "sc = spark.sparkContext\n",
    "sc.setCheckpointDir('graphframes_cps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "conventional-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get the unique groups\n",
    "unique_groups = g.connectedComponents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "refined-symbol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unique_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "expensive-biography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+\n",
      "| id|name|component|\n",
      "+---+----+---------+\n",
      "|  1|   A|        1|\n",
      "|  2|   B|        2|\n",
      "|  3|   C|        2|\n",
      "|  4|   D|        2|\n",
      "|  5|   E|        2|\n",
      "|  6|   F|        2|\n",
      "|  7|   G|        2|\n",
      "|  8|   H|        2|\n",
      "|  9|   I|        2|\n",
      "| 10|   J|       10|\n",
      "| 11|   K|       10|\n",
      "+---+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_groups.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-exhibit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "returning-august",
   "metadata": {},
   "source": [
    "just for fun lets see how long each version works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "special-vision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+\n",
      "| id|name|component|\n",
      "+---+----+---------+\n",
      "|  1|   A|        1|\n",
      "|  2|   B|        2|\n",
      "|  3|   C|        2|\n",
      "|  4|   D|        2|\n",
      "|  5|   E|        2|\n",
      "|  6|   F|        2|\n",
      "|  7|   G|        2|\n",
      "|  8|   H|        2|\n",
      "|  9|   I|        2|\n",
      "+---+----+---------+\n",
      "\n",
      "CPU times: user 233 ms, sys: 117 ms, total: 351 ms\n",
      "Wall time: 27.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unique_groups2 = g.connectedComponents()\n",
    "unique_groups2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "horizontal-vaccine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  u|  p|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "|  5|  5|\n",
      "|  6|  2|\n",
      "|  6|  3|\n",
      "|  7|  4|\n",
      "|  7|  5|\n",
      "|  8|  6|\n",
      "|  9|  7|\n",
      "|  9|  8|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#validate\n",
    "l = [\n",
    "    (1, 1),\n",
    "    (2, 2),\n",
    "    (3, 3),\n",
    "    (4, 4),\n",
    "    (5, 5),\n",
    "    (6, 2),\n",
    "    (6, 3),\n",
    "    (7, 4),\n",
    "    (7, 5),\n",
    "    (8, 6),\n",
    "    (9, 7),\n",
    "    (9, 8)]\n",
    "rdd = sc.parallelize(l)\n",
    "ucid_mpp_acc = rdd.map(lambda x: Row(u=x[0],p=x[1]))\n",
    "np = spark.createDataFrame(ucid_mpp_acc)\n",
    "\n",
    "np.show(50)\n",
    "\n",
    "np2 = np.withColumnRenamed('p','p2').withColumnRenamed('u','u2')\n",
    "# np2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "horizontal-annual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing the 1 check\n",
      "checking for multiple parents.\n",
      "rows with multiple parents issues: 6 check_1\n",
      "adding the parents rel.\n",
      "checking for grandparents.\n",
      "rows with grandpa issues: 10\n",
      "adding the grandparents.\n",
      "Loop 2 happy_gp: 0\n",
      "Loop 2 happy_mp: 0\n",
      "doing the 2 check\n",
      "checking for multiple parents.\n",
      "rows with multiple parents issues: 8 check_2\n",
      "adding the parents rel.\n",
      "checking for grandparents.\n",
      "rows with grandpa issues: 13\n",
      "adding the grandparents.\n",
      "Loop 3 happy_gp: 0\n",
      "Loop 3 happy_mp: 0\n",
      "doing the 3 check\n",
      "checking for multiple parents.\n",
      "rows with multiple parents issues: 4 check_3\n",
      "adding the parents rel.\n",
      "checking for grandparents.\n",
      "rows with grandpa issues: 2\n",
      "adding the grandparents.\n",
      "Loop 4 happy_gp: 0\n",
      "Loop 4 happy_mp: 0\n",
      "doing the 4 check\n",
      "checking for multiple parents.\n",
      "rows with multiple parents issues: 0 check_4\n",
      "checking for grandparents.\n",
      "rows with grandpa issues: 0\n",
      "Loop 5 happy_gp: 1\n",
      "Loop 5 happy_mp: 1\n",
      "The dataframe with the right Parent - - Child relationship.\n",
      "+---+---+\n",
      "|  u|  p|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  2|\n",
      "|  4|  2|\n",
      "|  5|  2|\n",
      "|  6|  2|\n",
      "|  7|  2|\n",
      "|  8|  2|\n",
      "|  9|  2|\n",
      "+---+---+\n",
      "\n",
      "CPU times: user 4.54 s, sys: 2.33 s, total: 6.87 s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "\n",
    "loop = 1\n",
    "debug = 0\n",
    "happy = 0\n",
    "happy_mp = 1\n",
    "happy_gp = 1\n",
    "infinit_loop_exit = 100\n",
    "df_dict = defaultdict(list)\n",
    "df_dict[1].append(np)\n",
    "df_dict[1].append(np2)\n",
    "##print('Start time:', str(datetime.datetime.now()))\n",
    "while not happy: \n",
    "    print('doing the {0} check'.format(loop))\n",
    "    print('checking for multiple parents.')\n",
    "    #mp = df_dict[loop][0].join(df_dict[loop][0].groupBy(\"u\").agg(countDistinct(\"p\").alias('count_{0}'.format(loop))), 'u')\\\n",
    "    #            .filter('count_{0} > 1'.format(loop))        \n",
    "    mp = df_dict[loop][0].join(df_dict[loop][0].groupBy(\"u\").agg(countDistinct(\"p\").alias('count')), 'u')\\\n",
    "               .filter('count > 1')\n",
    "    #mp.show()    \n",
    "    print('rows with multiple parents issues:',mp.count(), 'check_{0}'.format(loop))\n",
    "    if mp.count() > 0:\n",
    "        print('adding the parents rel.')\n",
    "        happy_mp = 0\n",
    "        wd = Window.partitionBy('w').orderBy('u')\n",
    "        mp_rel = mp.withColumnRenamed('u','w').withColumnRenamed('p','u').withColumn('p', psf.min('u').over(wd))\n",
    "        new_rel = mp_rel.filter('u != p').select('u', 'p').distinct()\n",
    "        if debug == 1:\n",
    "            new_rel.show()\n",
    "        np_a = df_dict[loop][0].select('u', 'p').union(new_rel).distinct()\n",
    "        if debug == 1:\n",
    "            #np.sort('u').show(50)\n",
    "            np_a.show(50)\n",
    "        np = np_a.join(np_a.groupBy(\"u\").agg(countDistinct(\"p\").alias('count_{0}'.format(loop))), 'u')\\\n",
    "                               .filter('(count_{0} > 1 and u!=p) or count_{0} = 1'.format(loop))\n",
    "        if debug == 1:\n",
    "            # np.sort('u').show()\n",
    "            np.show(50)\n",
    "        df_dict[loop+1].append(np)\n",
    "        #generate np2 in case we down't go into grandpa issues\n",
    "        df_dict[loop+1].append(np.withColumnRenamed('p','p2').withColumnRenamed('u','u2'))\n",
    "        #overwrite np and np2 for loop in case we go into grandpa issues\n",
    "        df_dict[loop][0] = np\n",
    "        df_dict[loop][1] = np.withColumnRenamed('p','p2').withColumnRenamed('u','u2')\n",
    "        \n",
    "        \n",
    "        # cleanup\n",
    "        mp_rel.unpersist()\n",
    "        new_rel.unpersist()\n",
    "        np_a.unpersist()\n",
    "        \n",
    "    else:\n",
    "        happy_mp = 1\n",
    "\n",
    "\n",
    "   \n",
    "    print('checking for grandparents.')\n",
    "    chk = df_dict[loop][0].join(df_dict[loop][1], df_dict[loop][0].p==df_dict[loop][1].u2)\\\n",
    "            .filter(df_dict[loop][0].p!=df_dict[loop][1].p2)\n",
    "    print('rows with grandpa issues:',chk.count())\n",
    "    \n",
    "    # new parent df\n",
    "    if chk.count() > 0 :\n",
    "        print('adding the grandparents.')\n",
    "        happy_gp = 0\n",
    "        np_temp = df_dict[loop][0].join(df_dict[loop][1], df_dict[loop][0].p==df_dict[loop][1].u2)\\\n",
    "                    .select(df_dict[loop][0].u,df_dict[loop][1].p2)\\\n",
    "                    .withColumnRenamed('u','u').withColumnRenamed('p2','p').distinct()\n",
    "        np = np_temp\n",
    "        if debug:\n",
    "            print('gp resulting np:')\n",
    "            np.sort('u', 'p').show(60)\n",
    "        np2 = np.withColumnRenamed('p','p2').withColumnRenamed('u','u2')\n",
    "        if df_dict[loop+1][0]:\n",
    "            df_dict[loop+1][0] = np\n",
    "            df_dict[loop+1][1] = np2\n",
    "        else:\n",
    "            df_dict[loop+1].append(np)\n",
    "            df_dict[loop+1].append(np2)\n",
    "            \n",
    "        # cleanup\n",
    "        np_temp.unpersist()\n",
    "        \n",
    "    else:\n",
    "        happy_gp = 1\n",
    "    \n",
    "    print('Loop {} happy_gp:'.format(loop+1),happy_gp)\n",
    "    print('Loop {} happy_mp:'.format(loop+1), happy_mp)\n",
    "    \n",
    "    if happy_gp and happy_mp:\n",
    "        happy = 1\n",
    "    \n",
    "    if not happy:\n",
    "        df_dict[loop+1][0] = df_dict[loop+1][0].cache()\n",
    "        df_dict[loop+1][1] = df_dict[loop+1][1].cache()\n",
    "        df_dict[loop][0].unpersist()\n",
    "        df_dict[loop][1].unpersist()\n",
    "    \n",
    "    \n",
    "    if infinit_loop_exit < loop:\n",
    "        happy = 1\n",
    "    \n",
    "    loop = loop + 1\n",
    "    \n",
    "print('The dataframe with the right Parent - - Child relationship.')\n",
    "df_dict[loop-1][0].sort('u').show(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
