{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Execution Job\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_path = 'data_examples/products_parquet/'\n",
    "sales_path = 'data_examples/sales_parquet/'\n",
    "sellers_path = 'data_examples/sellers_parquet/'\n",
    "paths = [products_path,sales_path,sellers_path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet File format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "for p in paths:\n",
    "  dataframes.append(spark.read.parquet(p)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = dataframes[0]\n",
    "sales = dataframes[1]\n",
    "sellers = dataframes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|       1|         0|        0|2020-07-10|             62|esctmitgntlqljxnC...|\n",
      "|       2|         0|        0|2020-07-07|             38|cufiduzyskiviokju...|\n",
      "|       3|         0|        0|2020-07-05|             13|hscngebsortzolelf...|\n",
      "|       4|         0|        0|2020-07-06|             94|jxhvzoobncxwzkpdl...|\n",
      "|       5|         0|        0|2020-07-02|             41|nqazvvrqffccuwzpr...|\n",
      "|       6|         0|        0|2020-07-07|             72|auesyqwlzglbecnmn...|\n",
      "|       7|         0|        0|2020-07-04|             58|wymwvtmlsrirflpne...|\n",
      "|       8|         0|        0|2020-07-08|             97|xzsadhvwyzhiboqIu...|\n",
      "|       9|         0|        0|2020-07-08|             53|gvbzspbwezmfjwmuz...|\n",
      "|      10|         0|        0|2020-07-09|             98|rxosjlrysukzmkvjv...|\n",
      "|      11|         0|        0|2020-07-01|              1|cuvkjjcdhwaofsogj...|\n",
      "|      12|         0|        0|2020-07-04|             14|wgvshpzutcqtzlbct...|\n",
      "|      13|         0|        0|2020-07-09|             22|pvdahkwfuwtbakwuh...|\n",
      "|      14|         0|        0|2020-07-08|             32|zayqtkhtjnhhvppyk...|\n",
      "|      15|         0|        0|2020-07-04|              6|qziqvljlpkegnhrsm...|\n",
      "|      16|         0|        0|2020-07-07|             79|caxetagbnacoovqoz...|\n",
      "|      17|         0|        0|2020-07-10|             79|irhvnfobpftjxttxa...|\n",
      "|      18|         0|        0|2020-07-01|             95|khyfqjhvdjbnwsylf...|\n",
      "|      19|         0|        0|2020-07-04|             89|yotcaxeexheeoyfyl...|\n",
      "|      20|         0|        0|2020-07-01|             86|rjpcdurxltgqjlbwz...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df = sales \\\n",
    ".filter(col('product_id') == 0).groupBy('date').agg(sum('num_pieces_sold').alias('total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|      date|   total|\n",
      "+----------+--------+\n",
      "|2020-07-03|966312.0|\n",
      "|2020-07-07|959833.0|\n",
      "|2020-07-01|966041.0|\n",
      "|2020-07-08|972484.0|\n",
      "|2020-07-04|959798.0|\n",
      "|2020-07-10|946825.0|\n",
      "|2020-07-09|950340.0|\n",
      "|2020-07-06|967997.0|\n",
      "|2020-07-02|958383.0|\n",
      "|2020-07-05|960707.0|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[date#33], functions=[sum(cast(num_pieces_sold#34 as double))])\n",
      "+- Exchange hashpartitioning(date#33, 200), true, [id=#288]\n",
      "   +- *(1) HashAggregate(keys=[date#33], functions=[partial_sum(cast(num_pieces_sold#34 as double))])\n",
      "      +- *(1) Project [date#33, num_pieces_sold#34]\n",
      "         +- *(1) Filter (isnotnull(product_id#31) AND (cast(product_id#31 as int) = 0))\n",
      "            +- *(1) ColumnarToRow\n",
      "               +- FileScan parquet [product_id#31,date#33,num_pieces_sold#34] Batched: true, DataFilters: [isnotnull(product_id#31), (cast(product_id#31 as int) = 0)], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data_examples/sales_parquet], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:string,date:string,num_pieces_sold:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.write.mode(\"overwrite\").partitionBy('date').parquet('data_examples/sales_parquet_partitioned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales1 = spark.read.parquet('data_examples/sales_parquet_partitioned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped  = sales1.filter(col('date') == '2020-07-07').groupBy('product_id').agg(sum('num_pieces_sold').alias('total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|total|\n",
      "+----------+-----+\n",
      "|     17677| 30.0|\n",
      "|     42638| 48.0|\n",
      "|     26730| 34.0|\n",
      "|     34842| 63.0|\n",
      "|     59799|  3.0|\n",
      "|     46162| 65.0|\n",
      "|     65991| 27.0|\n",
      "|      6548| 73.0|\n",
      "|     21638| 46.0|\n",
      "|     25467| 48.0|\n",
      "|     67162| 23.0|\n",
      "|     16378| 29.0|\n",
      "|     67896| 37.0|\n",
      "|     57556| 98.0|\n",
      "|      8520| 38.0|\n",
      "|      4117| 62.0|\n",
      "|      7235| 10.0|\n",
      "|     37483| 29.0|\n",
      "|     15891| 90.0|\n",
      "|     12852| 55.0|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[product_id#610], functions=[sum(cast(num_pieces_sold#612 as double))])\n",
      "   +- Exchange hashpartitioning(product_id#610, 4), true, [id=#594]\n",
      "      +- HashAggregate(keys=[product_id#610], functions=[partial_sum(cast(num_pieces_sold#612 as double))])\n",
      "         +- Project [product_id#610, num_pieces_sold#612]\n",
      "            +- FileScan parquet [product_id#610,num_pieces_sold#612,date#614] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data_examples/sales_parquet_partitioned], PartitionFilters: [isnotnull(date#614), (date#614 = 18450)], PushedFilters: [], ReadSchema: struct<product_id:string,num_pieces_sold:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.shuffle.partitions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions','4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies1 = spark.read.option('Header','true').csv('movies1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies2 = spark.read.option('Header','true').csv('movies2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               title|count|\n",
      "+--------------------+-----+\n",
      "|      Jumanji (1995)|    1|\n",
      "|Father of the Bri...|    1|\n",
      "|      Copycat (1995)|    1|\n",
      "|Waiting to Exhale...|    1|\n",
      "|  Money Train (1995)|    1|\n",
      "|       Casino (1995)|    1|\n",
      "|Sense and Sensibi...|    1|\n",
      "|    GoldenEye (1995)|    1|\n",
      "|Ace Ventura: When...|    1|\n",
      "|Grumpier Old Men ...|    1|\n",
      "| Tom and Huck (1995)|    1|\n",
      "|      Sabrina (1995)|    1|\n",
      "|   Get Shorty (1995)|    1|\n",
      "|American Presiden...|    1|\n",
      "|         Heat (1995)|    1|\n",
      "|    Toy Story (1995)|    1|\n",
      "|Cutthroat Island ...|    1|\n",
      "| Sudden Death (1995)|    1|\n",
      "|        Balto (1995)|    1|\n",
      "|   Four Rooms (1995)|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[title#41], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(title#41, 200), true, [id=#147]\n",
      "   +- *(2) HashAggregate(keys=[title#41], functions=[partial_count(1)])\n",
      "      +- *(2) Project [title#41]\n",
      "         +- *(2) BroadcastHashJoin [title#41], [title#63], Inner, BuildLeft\n",
      "            :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])), [id=#138]\n",
      "            :  +- *(1) Project [title#41]\n",
      "            :     +- *(1) Filter isnotnull(title#41)\n",
      "            :        +- FileScan csv [title#41] Batched: false, DataFilters: [isnotnull(title#41)], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/movies1.csv], PartitionFilters: [], PushedFilters: [IsNotNull(title)], ReadSchema: struct<title:string>\n",
      "            +- *(2) Project [title#63]\n",
      "               +- *(2) Filter isnotnull(title#63)\n",
      "                  +- FileScan csv [title#63] Batched: false, DataFilters: [isnotnull(title#63)], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/movies2.csv], PartitionFilters: [], PushedFilters: [IsNotNull(title)], ReadSchema: struct<title:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies = movies1.join(movies2,'title')\n",
    "\n",
    "data =movies.groupBy('title').count()\n",
    "\n",
    "data.show()\n",
    "\n",
    "data.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies1.write\\\n",
    "    .bucketBy(16, 'title') \\\n",
    "    .sortBy('title') \\\n",
    "    .saveAsTable('movies1_bucket', format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies2.write\\\n",
    "    .bucketBy(16, 'title') \\\n",
    "    .sortBy('title') \\\n",
    "    .saveAsTable('movies2_bucket', format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = spark.table('movies1_bucket')\n",
    "m2 = spark.table('movies2_bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+-------+--------------------+--------------------+\n",
      "|movieId|             title|              genres|movieId|               title|              genres|\n",
      "+-------+------------------+--------------------+-------+--------------------+--------------------+\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     23|    Assassins (1995)|Action|Crime|Thri...|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     40|Cry, the Beloved ...|               Drama|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     31|Dangerous Minds (...|               Drama|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     63|Don't Be a Menace...|        Comedy|Crime|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     71|    Fair Game (1995)|              Action|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|    118| If Lucy Fell (1996)|      Comedy|Romance|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     25|Leaving Las Vegas...|       Drama|Romance|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     89| Nick of Time (1995)|     Action|Thriller|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     50|Usual Suspects, T...|Crime|Mystery|Thr...|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     80|White Balloon, Th...|      Children|Drama|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     82|Antonia's Line (A...|        Comedy|Drama|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     95| Broken Arrow (1996)|Action|Adventure|...|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     61|Eye for an Eye (1...|      Drama|Thriller|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     72|Kicking and Screa...|        Comedy|Drama|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     66|Lawnmower Man 2: ...|Action|Sci-Fi|Thr...|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|    112|Rumble in the Bro...|Action|Adventure|...|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|    116|Anne Frank Rememb...|         Documentary|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|    102|    Mr. Wrong (1996)|              Comedy|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|     77|    Nico Icon (1995)|         Documentary|\n",
      "|     20|Money Train (1995)|Action|Comedy|Cri...|    106|Nobody Loves Me (...|        Comedy|Drama|\n",
      "+-------+------------------+--------------------+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "BroadcastNestedLoopJoin BuildLeft, Inner\n",
      ":- BroadcastExchange IdentityBroadcastMode, [id=#523]\n",
      ":  +- *(1) Project [movieId#107, title#108, genres#109]\n",
      ":     +- *(1) Filter (isnotnull(movieId#107) AND (cast(movieId#107 as int) = 20))\n",
      ":        +- *(1) ColumnarToRow\n",
      ":           +- FileScan parquet default.movies1_bucket[movieId#107,title#108,genres#109] Batched: true, DataFilters: [isnotnull(movieId#107), (cast(movieId#107 as int) = 20)], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/spark-warehouse/movies1_bucket], PartitionFilters: [], PushedFilters: [IsNotNull(movieId)], ReadSchema: struct<movieId:string,title:string,genres:string>, SelectedBucketsCount: 16 out of 16\n",
      "+- *(2) ColumnarToRow\n",
      "   +- FileScan parquet default.movies2_bucket[movieId#113,title#114,genres#115] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/spark-warehouse/movies2_bucket], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<movieId:string,title:string,genres:string>, SelectedBucketsCount: 16 out of 16\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = m1.join(m2) \\\n",
    ".filter(m1['movieId'] == 20)\n",
    "m.show()\n",
    "m.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
